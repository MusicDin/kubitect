{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"examples/accessing-cluster/","title":"Accessing the cluster","text":"<p>Kubernetes clusters running on cloud providers typically support provision of a load balancer from the cloud infrastructure on demand. By simply setting a Service type to <code>LoadBalancer</code> provisions an external load balancer that has its own unique IP address and redirects all connections to the Service, as shown in the figure below.</p> <p>In on-premise environments, there is no load balancer that can be provisioned on demand. Therefore, some alternative solutions are explained in this document.</p>"},{"location":"examples/accessing-cluster/#accessing-the-cluster","title":"Accessing the cluster","text":""},{"location":"examples/accessing-cluster/#node-ports","title":"Node ports","text":"<p>Setting Service type to <code>NodePort</code> makes Kubernetes reserve a port on all its nodes. As a result, the Service becomes available on <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>, as shown in the figure below.</p> <p>When using <code>NodePort</code>, it does not matter to which node a client sends the request, since it is routed internally to the appropriate Pod. However, if all traffic is directed to a single node, its failure will make the Service unavailable.</p>"},{"location":"examples/accessing-cluster/#self-provisioned-edge","title":"Self-provisioned edge","text":"<p>With Kubitect, it is possible to configure the port forwarding of the load balancer to distribute incoming requests to multiple nodes in the cluster, as shown in the figure below.</p> <p>To set up load balancer port forwarding, at least one load balancer must be configured. The following example shows how to set up load balancer port forwarding for ports 80 (HTTP) and 443 (HTTPS).</p> <pre><code>cluster:\nnodes:\nloadBalancer:\nforwardPorts:\n- name: http\nport: 80\n- name: https\nport: 443          instances:\n- id: 1\n</code></pre> <p>Load balancer port forwarding is particularly handy when combined with a <code>NodePort</code> Service or a Service whose ports are exposed on the host. For example, for HTTP and HTTPS traffic an Ingress is most often used. To use Ingress resources in the Kubernetes cluster, an ingress controller is required. With Kubitect, a load balancer can be configured to accept connections on ports 80 and 443, and redirect them to all cluster nodes on ports 50080 and 50443 where an ingress controller is listening for incoming requests. The following code snippet shows the configuration for such a scenario.</p> <pre><code>cluster:\nnodes:\nloadBalancer:\nforwardPorts:\n- name: http\nport: 80\ntargetPort: 50080\ntarget: workers # (1)!\n- name: https\nport: 443\ntargetPort: 50443\ninstances:\n- id: 1\n\naddons:\nkubespray:\ningress_nginx_enabled: true\ningress_nginx_namespace: \"ingress-nginx\"\ningress_nginx_insecure_port: 50080 # (2)!\ningress_nginx_secure_port: 50443\n</code></pre> <ol> <li> <p>By default, each configured port instructs the load balancer to distribute traffic across all worker nodes.     The default behavior can be changed using the <code>target</code> property.</p> <p>Possible target values are:</p> <ul> <li><code>workers</code> - Distributes traffic across worker nodes. (default)</li> <li><code>masters</code> - Distributes traffic across master nodes.</li> <li><code>all</code> - Distributes traffic across master and worker nodes.</li> </ul> </li> <li> <p>When the ingress-nginx controller is set up with Kubespray, a DaemonSet is created that exposes ports on the host (<code>hostPort</code>).</p> </li> </ol>"},{"location":"examples/accessing-cluster/#metallb","title":"MetalLB","text":"<p>MetalLB is a network load balancer implementation for bare metal Kubernetes clusters. In short, it allows you to create Services of type <code>LoadBalancer</code> where actual on-demand load balancers are not an option.</p> <p>For MetalLB to work, a pool of unused IP addresses needs to be provided. In the following example, MetalLB is configured to use an IP address pool with the IP range <code>10.10.13.225/27</code>.</p> <pre><code>addons:\nkubespray:\nmetallb_enabled: true\nmetallb_speaker_enabled: true\nmetallb_ip_range:\n- \"10.10.13.225/27\"\nmetallb_pool_name: \"default\"\nmetallb_auto_assign: true\nmetallb_version: v0.12.1\nmetallb_protocol: \"layer2\"\n</code></pre> <p>When a Service of type <code>LoadBalancer</code> is created, it is assigned an IP address from the pool. For example, we could deploy an ingress-nginx controller and change its Service type to <code>LoadBalancer</code>. <pre><code># Deploy ingress-nginx controller\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/baremetal/1.23/deploy.yaml\n\n# Patch ingress controller Service type to LoadBalancer\nkubectl patch svc ingress-nginx-controller -n ingress-nginx -p '{\"spec\": {\"type\":\"LoadBalancer\"}}'\n</code></pre></p> <p>As a result, MetalLB assigns the service <code>ingress-nginx-controller</code> an external IP address from the address pool. <pre><code>kubectl get svc -n ingress-nginx ingress-nginx-controller\n\n# NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)                      AGE\n# ingress-nginx-controller   LoadBalancer   10.233.55.194   10.10.13.225   80:31497/TCP,443:30967/TCP   63s\n</code></pre></p> <p>By sending a request to the assigned IP address, it can be seen that Nginx responds to the request. <pre><code>curl -k https://10.10.13.225\n\n# &lt;html&gt;\n# &lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n# &lt;body&gt;\n# &lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n# &lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n# &lt;/body&gt;\n# &lt;/html&gt;\n</code></pre></p> <p>This example has demonstrated the functionality of MetalLB in <code>layer2</code> mode. For more MetalLB configuration options, see the official MetalLB documentation.</p>"},{"location":"examples/full-example/","title":"Full example","text":"<p>This document contains an example of Kubitect configuration. Example covers all (or most) of the Kubitect properties. This example is meant for users that learn the fastest from an example configuration.</p> <pre><code>#\n# The 'hosts' section contains data about the physical servers on which the \n# Kubernetes cluster will be installed.\n#\n# For each host, a name and connection type must be specified. Only one host can \n# have the connection type set to 'local' or 'localhost'.\n#\n# If the host is a remote machine, the path to the SSH key file must be specified.\n# Note that connections to remote hosts support only passwordless certificates.\n#\n# The host can also be marked as default, i.e. if no specific host is specified \n# for an instance (in the cluster.nodes section), it will be installed on a \n# default host. If none of the hosts are marked as default, the first host in the \n# list is used as the default host.\n#\nhosts:\n- name: localhost # (3)!\ndefault: true # (4)!\nconnection:\ntype: local # (5)!\n- name: remote-server-1\nconnection:\ntype: remote\nuser: myuser # (6)!\nip: 10.10.40.143 # (7)!\nssh:\nport: 1234  # (8)!\nverify: true # (9)!\nkeyfile: \"~/.ssh/id_rsa_server1\" # (10)!\n- name: remote-server-2\nconnection:\ntype: remote\nuser: myuser\nip: 10.10.40.144\nssh:\nkeyfile: \"~/.ssh/id_rsa_server2\"\nmainResourcePoolPath: \"/var/lib/libvirt/pools/\" # (11)!\ndataResourcePools: # (12)!\n- name: data-pool # (13)!\npath: \"/mnt/data/pool\" # (14)!\n- name: backup-pool\npath: \"/mnt/backup/pool\"\n\n#\n# The 'cluster' section of the configuration contains general data about the\n# cluster, the nodes that are part of the cluster, and the cluster's network.\n# \ncluster:\nname: my-k8s-cluster # (15)!\nnetwork:\nmode: bridge # (16)!\ncidr: 10.10.64.0/24 # (17)!\ngateway: 10.10.64.1 # (18)!\nbridge: br0 # (19)!\nnodeTemplate:\nuser: k8s\nssh:\nprivateKeyPath: \"~/.ssh/id_rsa_test\"\naddToKnownHosts: true\nos:\ndistro: ubuntu\nnetworkInterface: ens3 # (20)!\ndns: # (21)!\n- 1.1.1.1\n- 1.0.0.1\nupdateOnBoot: true\nnodes:\nloadBalancer:\nvip: 10.10.64.200 # (22)!\nvirtualRouterId: 13 # (23)!\nforwardPorts:\n- name: http\nport: 80\n- name: https\nport: 443\ntarget: all\n- name: sample\nport: 60000\ntargetPort: 35000\ndefault: # (24)!\nram: 4 # GiB\ncpu: 1 # vCPU\nmainDiskSize: 16 # GiB\ninstances:\n- id: 1\nip: 10.10.64.5 # (25)!\nmac: \"52:54:00:00:00:40\" # (26)!\nram: 8 # (27)!\ncpu: 8 # (28)!\nhost: remote-server-1 # (29)!\n- id: 2\nip: 10.10.64.6\nmac: \"52:54:00:00:00:41\"\nhost: remote-server-2\n- id: 3\nip: 10.10.64.7\nmac: \"52:54:00:00:00:42\"\n# If host is not specifed, VM will be installed on the default host.\n# If default host is not specified, VM will be installed on the first\n# host in the list.\nmaster:\ndefault:\nram: 8\ncpu: 2\nmainDiskSize: 256\ninstances:\n# IMPORTANT: There should be odd number of master nodes.\n- id: 1\nhost: remote-server-1\n- id: 2\nhost: remote-server-2\n- id: 3\nhost: localhost\nworker:\ndefault:\nram: 16\ncpu: 4\nlabels: # (30)!\ncustom-label: \"This is a custom default node label\"\nnode-role.kubernetes.io/node: # (31)!\ninstances:\n- id: 1\nip: 10.10.64.101\ncpu: 8\nram: 64\nhost: remote-server-1\n- id: 2\nip: 10.10.64.102\ndataDisks: # (32)!\n- name: rook-disk # (33)!\npool: data-pool # (34)!\nsize: 128 # GiB\n- name: test-disk\npool: data-pool\nsize: 128\n- id: 3\nip: 10.10.64.103\nram: 64\nlabels:\ncustom-label: \"Overwrite default node label\" # (35)!\ninstance-label: \"Node label, only for this instance\"\n- id: 4\nhost: remote-server-2\n- id: 5\n\n#\n# The 'kubernetes' section contains Kubernetes related properties,\n# such as version and network plugin.\n#\nkubernetes:\nversion: v1.24.7\nnetworkPlugin: calico\ndnsMode: coredns # (36)!\nother:\ncopyKubeconfig: false\n\n#\n# The 'addons' section contains the configuration of the applications that\n# will be installed on the Kubernetes cluster as part of the cluster setup.\n#\naddons:\nkubespray: # Sample Nginx ingress controller deployment\ningress_nginx_enabled: true\ningress_nginx_namespace: \"ingress-nginx\"\ningress_nginx_insecure_port: 80\ningress_nginx_secure_port: 443\n# Sample MetalLB deployment\nmetallb_enabled: true\nmetallb_speaker_enabled: true\nmetallb_ip_range:\n- \"10.10.9.201-10.10.9.254\"\nmetallb_pool_name: \"default\"\nmetallb_auto_assign: true\nmetallb_version: v0.12.1\nmetallb_protocol: \"layer2\"\n</code></pre> <ol> <li> <p>This allows you to set a custom URL that targets clone/fork of Kubitect project.</p> </li> <li> <p>Kubitect version.</p> </li> <li> <p>Custom host name.      It is used to link instances to the specific host.</p> </li> <li> <p>Makes the host a default host.      This means that if no host is specified for the node instance, the instance will be linked to the default host.</p> </li> <li> <p>Connection type can be either <code>local</code> or <code>remote</code>. </p> <p>If it is set to remote, at least the following fields must be set:</p> <ul> <li><code>user</code></li> <li><code>ip</code></li> <li><code>ssh.keyfile</code></li> </ul> </li> <li> <p>Remote host user that is used to connect to the remote hypervisor.      This user must be added in the <code>libvirt</code> group.</p> </li> <li> <p>IP address of the remote host.</p> </li> <li> <p>Overrides default SSH port (22).</p> </li> <li> <p>If true, SSH host is verified. This means that the host must be present in the known SSH hosts.</p> </li> <li> <p>Path to the passwordless SSH key used to connect to the remote host.</p> </li> <li> <p>The path to the main resource pool defines where the virtual machine disk images are stored. These disks contain the virtual machine operating system, and therefore it is recommended to install them on SSD disks.</p> </li> <li> <p>List of other data resource pools where virtual disks can be created.</p> </li> <li> <p>Custom data resource pool name. Must be unique among all data resource pools on a specific host.</p> </li> <li> <p>Path where data resource pool is created. All data disks linked to that resource pool will be created under this path.</p> </li> <li> <p>Cluster name used as a prefix for the various components.</p> </li> <li> <p>Network mode. Possible values are</p> <ul> <li><code>bridge</code> mode uses predefined bridge interface. This mode is mandatory for deployments across multiple hosts.</li> <li><code>nat</code> mode creates virtual network with IP range defined in <code>network.cidr</code></li> <li><code>route</code></li> </ul> </li> <li> <p>Network CIDR represents the network IP together with the network mask.      In <code>nat</code> mode, CIDR is used for the new network.     In <code>bridge</code> mode, CIDR represents the current local area network (LAN).</p> </li> <li> <p>The network gateway IP address.     If omitted the first client IP from network CIDR is used as a gateway.</p> </li> <li> <p>Bridge represents the bridge interface on the hosts.     This field is mandatory if the network mode is set to <code>bridge</code>.     If the network mode is set to <code>nat</code>, this field can be omitted.</p> </li> <li> <p>Set custom DNS list for all nodes.     If omitted, network gateway is also used as a DNS.</p> </li> <li> <p>Specify the network interface used by the virtual machine. In general, this option can be omitted. </p> <p>If omitted, a network interface from distro preset (<code>/terraform/defaults.yaml</code>) is used.</p> </li> <li> <p>Virtual (floating) IP shared between load balancers. </p> </li> <li> <p>Virtual router ID that is set in Keepalived configuration when virtual IP is used.     By default it is set to 51.      If multiple clusters are created it must be ensured that it is unique for each cluster.</p> </li> <li> <p>Default values apply for all virtual machines (VMs) of the same type.</p> </li> <li> <p>Static IP address of the virtual machine.      If omitted DHCP lease is requested.</p> </li> <li> <p>Static MAC address.      If omitted MAC address is generated.</p> </li> <li> <p>Overrides default RAM value for this node.</p> </li> <li> <p>Overrides default CPU value for this node.</p> </li> <li> <p>Name of the host where instance should be created.     If omitted the default host is used.</p> </li> <li> <p>Default worker node labels.</p> </li> <li> <p>Label sets worker nodes role to <code>node</code>.</p> </li> <li> <p>Overrides default data disks for this node.</p> </li> <li> <p>Custom data disk name. It must be unique among all data disks for a specific instance.</p> </li> <li> <p>Resource pool name that must be defined on the host on which the instance will be deployed.</p> </li> <li> <p>Node labels defined for specific instances take precedence over default labels with the same key, so this label overrides the default label.</p> </li> <li> <p>Currently, the only DNS mode supported is CoreDNS.</p> </li> </ol>"},{"location":"examples/full-example/#full-detailed-example","title":"Full (detailed) example","text":""},{"location":"examples/ha-cluster/","title":"Highly available (HA) cluster","text":"<p>This example shows how to use Kubitect to set up a highly available cluster that spreads over 5 hosts. Such topology provides redundancy in case any node or even host fails.</p>"},{"location":"examples/ha-cluster/#highly-available-cluster","title":"Highly available cluster","text":""},{"location":"examples/ha-cluster/#step-1-hosts-configuration","title":"Step 1: Hosts configuration","text":"<p>Important</p> <p>This example uses preconfigured bridges on each host to expose nodes on the local network.</p> <p>Network bridge example shows how to configure a bridge interface using Netplan.</p> <p>In this example, we deploy a Kubernetes cluster on 5 (remote) physical hosts. The subnet of the local network is <code>10.10.0.0/20</code> and the gateway IP address is <code>10.10.0.1</code>. Each host is connected to the same local network and has a pre-configured bridge interface <code>br0</code>.</p> <p>In addition, a user <code>kubitect</code> is configured on each host, which is accessible via SSH with the same password-less certificate stored on our local machine under the path <code>~/.ssh/id_rsa_ha</code>.</p> <p>Each host must be specified in the Kubitect configuration file. In our case, the configurations of the hosts differ only by the name and IP address of the host.</p> ha.yaml<pre><code>hosts:\n- name: host1\nconnection:\ntype: remote\nuser: kubitect\nip: 10.10.0.5\nssh:\nkeyfile: \"~/.ssh/id_rsa_ha\"\n- name: host2\nconnection:\ntype: remote\nuser: kubitect\nip: 10.10.0.6\nssh:\nkeyfile: \"~/.ssh/id_rsa_ha\"\n- name: host3\nconnection:\ntype: remote\nuser: kubitect\nip: 10.10.0.10\nssh:\nkeyfile: \"~/.ssh/id_rsa_ha\"\n- name: host4\nconnection:\ntype: remote\nuser: kubitect\nip: 10.10.0.11\nssh:\nkeyfile: \"~/.ssh/id_rsa_ha\"\n- name: host5\nconnection:\ntype: remote\nuser: kubitect\nip: 10.10.0.12\nssh:\nkeyfile: \"~/.ssh/id_rsa_ha\"\n</code></pre>"},{"location":"examples/ha-cluster/#step-2-network-configuration","title":"Step 2: Network configuration","text":"<p>In the network configuration section, we specify the bridge interface that is preconfigured on each host and CIDR of our local network.</p> <p>The following code snippet shows the network configuration for this example.</p> ha.yaml<pre><code>cluster:\nnetwork:\nmode: bridge\ncidr: 10.10.0.0/20\nbridge: br0\n</code></pre>"},{"location":"examples/ha-cluster/#step-3-load-balancer-configuration","title":"Step 3: Load balancer configuration","text":"<p>Each working node stores only a single control plane IP address.  By placing a load balancer in front of the control plane (as shown in the Multi-master cluster example), traffic can be distributed across all control plane nodes.</p> <p>By having only a single load balancer in the cluster, the control plane may become unreachable if that load balancer fails. This would cause the entire cluster to become unavailable. To avoid this single point of failure, a failover (backup) load balancer can be configured. Its main purpose is to serve incoming requests on the same virtual (shared) IP address if the primary load balancer fails, as shown in the following figure.</p> <p>Failover is achieved using a virtual router redundancy protocol (VRRP). In practice, each load balancer still has its own IP address, but the primary load balancer also serves requests on the virtual IP address, which is not bound to any network interface. The primary load balancer periodically sends heartbeats to other load balancers (backups) to let them know it is still active. If the backup load balancer does not receive a heartbeat within a certain period of time, it assumes that the primary load balancer is has failed. The new primary load balancer is elected based on the priorities of the available load balancers. Once the load balancer becomes primary, it starts serving requests on the same virtual IP address as the previous one. This ensures that the requests are served through the same virtual IP address in case of a load balancer failure.</p> <p>The following code snippet shows the configuration of two load balancers and virtual IP for their failover.</p> ha.yaml<pre><code>cluster:\nnodes:\nloadBalancer:\nvip: 10.10.13.200\ninstances:\n- id: 1\nip: 10.10.13.201\nhost: host1\n- id: 2\nip: 10.10.13.202\nhost: host2\n</code></pre> <p>Note that for each load balancer instance, the <code>host</code> property is set. Its value is a name of the host on which a particular instance is to be deployed.</p>"},{"location":"examples/ha-cluster/#step-4-nodes-configuration","title":"Step 4: Nodes configuration","text":"<p>The configuration of the nodes is very simple and similar to the configuration of the load balancer instances. Each instance is configured with an ID, an IP address, and a host affinity.</p> ha.yaml<pre><code>cluster:\nnodes:\nmaster:\ninstances:\n- id: 1\nip: 10.10.13.10\nhost: host3\n- id: 2\nip: 10.10.13.11\nhost: host4\n- id: 3\nip: 10.10.13.12\nhost: host5\nworker:\ninstances:\n- id: 1\nip: 10.10.13.20\nhost: host3\n- id: 2\nip: 10.10.13.21\nhost: host4\n- id: 3\nip: 10.10.13.22\nhost: host5\n</code></pre>"},{"location":"examples/ha-cluster/#step-41-optional-data-disks-configuration","title":"Step 4.1 (Optional): Data disks configuration","text":"<p>Kubitect creates a main (system) disk for each configured virtual machine (VM). The main disk contains the VM's operating system along with all installed Kubernetes components.</p> <p>The VM's storage can be expanded by creating additional disks, also called data disks. This can be particularly useful when using storage solutions such as Rook. For example, Rook can be configured to use empty disks on worker nodes to create reliable distributed storage. </p> <p>Data disks in Kubitect must be configured separately for each node instance. They must also be connected to a resource pool, which can be either a main resource pool or a custom data resource pool. In this example, we have defined a custom data resource pool named <code>data-pool</code> on each host running worker nodes.</p> ha.yaml<pre><code>hosts:\n- name: host3\n...\ndataResourcePools:\n- name: data-pool\npath: /mnt/libvirt/pools/\n\ncluster:\nnodes:\nworker:\n- id: 1\nip: 10.10.13.20\nhost: host3\ndataDisks:\n- name: rook\npool: data-pool\nsize: 512 # GiB\n</code></pre> Final cluster configuration  ha.yaml<pre><code>hosts:\n- name: host1\nconnection:\ntype: remote\nuser: kubitect\nip: 10.10.0.5\nssh:\nkeyfile: \"~/.ssh/id_rsa_ha\"\n- name: host2\nconnection:\ntype: remote\nuser: kubitect\nip: 10.10.0.6\nssh:\nkeyfile: \"~/.ssh/id_rsa_ha\"\n- name: host3\nconnection:\ntype: remote\nuser: kubitect\nip: 10.10.0.10\nssh:\nkeyfile: \"~/.ssh/id_rsa_ha\"\ndataResourcePools:\n- name: data-pool\npath: /mnt/libvirt/pools/\n- name: host4\nconnection:\ntype: remote\nuser: kubitect\nip: 10.10.0.11\nssh:\nkeyfile: \"~/.ssh/id_rsa_ha\"\ndataResourcePools:\n- name: data-pool\npath: /mnt/libvirt/pools/\n- name: host5\nconnection:\ntype: remote\nuser: kubitect\nip: 10.10.0.12\nssh:\nkeyfile: \"~/.ssh/id_rsa_ha\"\ndataResourcePools:\n- name: data-pool\npath: /mnt/libvirt/pools/\n\ncluster:\nname: kubitect-ha\nnetwork:\nmode: bridge\ncidr: 10.10.0.0/20\nbridge: br0\nnodeTemplate:\nuser: k8s\nupdateOnBoot: true\nssh:\naddToKnownHosts: true\nos:\ndistro: ubuntu\nnodes:\nloadBalancer:\nvip: 10.10.13.200\ninstances:\n- id: 1\nip: 10.10.13.201\nhost: host1\n- id: 2\nip: 10.10.13.202\nhost: host2\nmaster:\ninstances:\n- id: 1\nip: 10.10.13.10\nhost: host3\n- id: 2\nip: 10.10.13.11\nhost: host4\n- id: 3\nip: 10.10.13.12\nhost: host5\nworker:\ninstances:\n- id: 1\nip: 10.10.13.20\nhost: host3\ndataDisks:\n- name: rook\npool: data-pool\nsize: 512\n- id: 2\nip: 10.10.13.21\nhost: host4\ndataDisks:\n- name: rook\npool: data-pool\nsize: 512\n- id: 3\nip: 10.10.13.22\nhost: host5\ndataDisks:\n- name: rook\npool: data-pool\nsize: 512\n\nkubernetes:\nversion: v1.24.7\n</code></pre>"},{"location":"examples/ha-cluster/#step-5-applying-the-configuration","title":"Step 5: Applying the configuration","text":"<p>Apply the cluster configuration. <pre><code>kubitect apply --config ha.yaml\n</code></pre></p>"},{"location":"examples/multi-master-cluster/","title":"Multi-master cluster","text":"<p>This example shows how to use Kubitect to set up a Kubernetes cluster with 3 master and 3 worker nodes. Configuring multiple master nodes provides control plane redundancy, meaning that the control plane can continue to operate normally if a certain number of master nodes fail. Since Kubitect deploys clusters with a stacked control plane (see Kubernetes.io - Stacked etcd topology for more information), this means that there is no downtime as long as (n/2)+1 master nodes are available.</p> <p>Note</p> <p>In this example we skip the explanation of some common configurations (hosts, network, node template, ...), as they are already explained in the Getting started (step-by-step) guide.</p>"},{"location":"examples/multi-master-cluster/#multi-master-cluster","title":"Multi-master cluster","text":""},{"location":"examples/multi-master-cluster/#step-1-cluster-configuration","title":"Step 1: Cluster configuration","text":"<p>Each worker node stores only a single control plane IP address.  Therefore, when creating clusters with multiple master nodes, we need to make sure that all of them are reachable at the same IP address, otherwise all workers would send requests to only one of the master nodes. This problem can be solved by placing a load balancer in front of the control plane, and instructing it to distribute traffic to all master nodes in the cluster, as shown in the figure below.</p> <p>To create such cluster, all we need to do, is specify desired node instances and one load balancer. Control plane will be accessible on the load balancer's IP address.</p> multi-master.yaml<pre><code>cluster:\n...\nnodes:\nloadBalancer:\ninstances:\n- id: 1\nip: 192.168.113.100\nmaster:\ninstances: # (1)!\n- id: 1\nip: 192.168.113.10\n- id: 2\nip: 192.168.113.11\n- id: 3\nip: 192.168.113.12\nworker:\ninstances:\n- id: 1\nip: 192.168.113.20\n- id: 2\nip: 192.168.113.21\n- id: 3\nip: 192.168.113.22\n</code></pre> <ol> <li>Size of the control plane (number of master nodes) must be odd.</li> </ol> <p>Kubitect detects the load balancer instance in the configuration and installs the HAProxy load balancer on an additional virtual machine. By default, the load balancer is configured to distribute traffic received on port 6443 (Kubernetes API server port) to all specified master nodes.</p> Final cluster configuration  multi-master.yaml<pre><code>hosts:\n- name: localhost\nconnection:\ntype: local\n\ncluster:\nname: k8s-cluster\nnetwork:\nmode: nat\ncidr: 192.168.113.0/24\nnodeTemplate:\nuser: k8s\nupdateOnBoot: true\nssh:\naddToKnownHosts: true\nos:\ndistro: ubuntu\nnodes:\nloadBalancer:\ninstances:\n- id: 1\nip: 192.168.113.100\nmaster:\ninstances:\n- id: 1\nip: 192.168.113.10\n- id: 2\nip: 192.168.113.11\n- id: 3\nip: 192.168.113.12\nworker:\ninstances:\n- id: 1\nip: 192.168.113.20\n- id: 2\nip: 192.168.113.21\n- id: 3\nip: 192.168.113.22\n\nkubernetes:\nversion: v1.24.7\nnetworkPlugin: calico\n</code></pre>"},{"location":"examples/multi-master-cluster/#step-2-applying-the-configuration","title":"Step 2: Applying the configuration","text":"<p>Apply the cluster: <pre><code>kubitect apply --config multi-master.yaml\n</code></pre></p>"},{"location":"examples/multi-worker-cluster/","title":"Multi-worker cluster","text":"<p>This example shows how to use Kubitect to set up a Kubernetes cluster with one master and three worker nodes.</p> <p>Note</p> <p>In this example we skip the explanation of some common configurations (hosts, network, node template, ...), as they are already explained in the Getting started (step-by-step) guide.</p>"},{"location":"examples/multi-worker-cluster/#multi-worker-cluster","title":"Multi-worker cluster","text":""},{"location":"examples/multi-worker-cluster/#step-1-cluster-configuration","title":"Step 1: Cluster configuration","text":"<p>To create a cluster with multiple workers, simply specify multiple worker nodes in the configuration. In this particular case, we want to have 3 worker nodes, but there can be as many as you want.</p> multi-worker.yaml<pre><code>cluster:\n...\nnodes:\nmaster:\ninstances:\n- id: 1\nip: 192.168.113.10 # (1)!\nworker:\ninstances:\n- id: 1\nip: 192.168.113.21\n- id: 7\nip: 192.168.113.27\n- id: 99\n</code></pre> <ol> <li>Static IP address of the node.      If the <code>ip</code> property is omitted, the DHCP lease is requested when the cluster is created.</li> </ol> Final cluster configuration  multi-worker.yaml<pre><code>hosts:\n- name: localhost\nconnection:\ntype: local\n\ncluster:\nname: k8s-cluster\nnetwork:\nmode: nat\ncidr: 192.168.113.0/24\nnodeTemplate:\nuser: k8s\nupdateOnBoot: true\nssh:\naddToKnownHosts: true\nos:\ndistro: ubuntu\nnodes:\nmaster:\ninstances:\n- id: 1\nip: 192.168.113.10\nworker:\ninstances:\n- id: 1\nip: 192.168.113.21\n- id: 7\nip: 192.168.113.27\n- id: 99\n\nkubernetes:\nversion: v1.24.7\nnetworkPlugin: calico\n</code></pre>"},{"location":"examples/multi-worker-cluster/#step-2-applying-the-configuration","title":"Step 2: Applying the configuration","text":"<p>Apply the cluster: <pre><code>kubitect apply --config multi-worker.yaml\n</code></pre></p>"},{"location":"examples/network-bridge/","title":"Network bridge","text":"<p>Bridged networks allow virtual machines to connect directly to the LAN. To use Kubitect with bridged network mode, a bridge interface must be preconfigured on the host machine. This example shows how to configure a simple bridge interface using Netplan.</p>"},{"location":"examples/network-bridge/#network-bridge","title":"Network bridge","text":""},{"location":"examples/network-bridge/#step-1-preconfigure-the-bridge-on-the-host","title":"Step 1 - (Pre)configure the bridge on the host","text":"<p>Before the network bridge can be created, a name of the host's network interface is required. This interface will be used by the bridge.</p> <p>To print the available network interfaces of the host, use the following command. <pre><code>nmcli device | grep ethernet\n</code></pre></p> <p>Similarly to the previous command, network interfaces can be printed using <code>ifconfig</code> or <code>ip</code> commands. Note that these commands output all interfaces, including virtual ones. <pre><code>ifconfig -a\n# or\nip a\n</code></pre></p> <p>Once you have obtained the name of the host's network interface (in our case <code>eth0</code>), you can create a bridge interface (in our case <code>br0</code>) by creating a file with the following content: /etc/netplan/bridge0.yaml<pre><code>network:\nversion: 2\nrenderer: networkd\nethernets:\neth0: {} # (1)!\nbridges:\nbr0: # (2)!\ninterfaces:\n- eth0\ndhcp4: true\ndhcp6: false\naddresses: # (3)!\n- 10.10.0.17\n</code></pre></p> <ol> <li> <p>Existing host's ethernet interface to be enslaved.</p> </li> <li> <p>Custom name of the bridge interface.</p> </li> <li> <p>Optionally a static IP address can be set for the bridge interface.</p> </li> </ol> <p>Tip</p> <p>See the official Netplan configuration examples for more advance configurations.</p> <p>Validate if the configuration is correctly parsed by Netplan. <pre><code>sudo netplan generate\n</code></pre></p> <p>Apply the configuration. <pre><code>sudo netplan apply\n</code></pre></p>"},{"location":"examples/network-bridge/#step-2-disable-netfilter-on-the-host","title":"Step 2 - Disable netfilter on the host","text":"<p>The final step is to prevent packets traversing the bridge from being sent to iptables for processing. <pre><code> cat &gt;&gt; /etc/sysctl.conf &lt;&lt;EOF\n net.bridge.bridge-nf-call-ip6tables = 0\n net.bridge.bridge-nf-call-iptables = 0\n net.bridge.bridge-nf-call-arptables = 0\n EOF\n\nsysctl -p /etc/sysctl.conf\n</code></pre></p> <p>Tip</p> <p>For more information, see the libvirt documentation.</p>"},{"location":"examples/network-bridge/#step-3-set-up-a-cluster-over-bridged-network","title":"Step 3 - Set up a cluster over bridged network","text":"<p>In the cluster configuration file, set the following variables:</p> <ul> <li><code>cluster.network.mode</code> to <code>bridge</code>,</li> <li><code>cluster.network.cidr</code> to the network CIDR of the LAN and</li> <li><code>cluster.network.bridge</code> to the name of the bridge you have created (<code>br0</code> in our case)</li> </ul> <pre><code>cluster:\nnetwork:\nmode: bridge\ncidr: 10.10.13.0/24\nbridge: br0\n...\n</code></pre>"},{"location":"examples/rook-cluster/","title":"Rook cluster","text":"<p>Important</p> <p>Since the Rook addon is still under development, it may not work as expected. Therefore, any feedback would be greatly appreciated.</p> <p>This example shows how to use Kubitect to set up distributed storage with Rook. For distributed storage, we add an additional data disk to each virtual machine as shown on the figure below.</p>"},{"location":"examples/rook-cluster/#rook-cluster","title":"Rook cluster","text":""},{"location":"examples/rook-cluster/#basic-setup","title":"Basic setup","text":""},{"location":"examples/rook-cluster/#step-1-define-data-resource-pool","title":"Step 1: Define data resource pool","text":"<p>To configure distributed storage with Rook, the data disks must be attached to the virtual machines. By default, each data disk is created in a main resource pool. Optionally, you can configure additional resource pools and associate data disks with them later.</p> <p>In this example, we define an additional resource pool named 'rook-pool'. rook-sample.yaml<pre><code>hosts:\n- name: localhost\nconnection:\ntype: local\ndataResourcePools:\n- name: rook-pool\n</code></pre></p>"},{"location":"examples/rook-cluster/#step-2-attach-data-disks","title":"Step 2: Attach data disks","text":"<p>After the data resource pool is configured, we are ready to allocate some data disks to the virtual machines.</p> rook-sample.yaml<pre><code>cluster:\nnodes:\nworker:\ninstances:\n- id: 1\ndataDisks:\n- name: rook\npool: rook-pool # (1)!\nsize: 256\n- id: 2\ndataDisks:\n- name: rook\npool: rook-pool\nsize: 256\n- id: 3\n- id: 4\ndataDisks:\n- name: rook\npool: rook-pool\nsize: 256\n- name: test\npool: rook-pool\nsize: 32\n</code></pre> <ol> <li>To create data disks in the main resource pool, either omit the pool property or set its value to <code>main</code>.</li> </ol>"},{"location":"examples/rook-cluster/#step-3-enable-rook-addon","title":"Step 3: Enable Rook addon","text":"<p>Once the disks are configured, you only need to activate the Rook addon.</p> rook-sample.yaml<pre><code>addons:\nrook:\nenabled: true\n</code></pre> <p>By default, Rook resources are provisioned on all worker nodes (without any constraints). This behavior can be restricted with node selectors.</p> Final cluster configuration  rook-sample.yaml<pre><code>hosts:\n- name: localhost\nconnection:\ntype: local\ndataResourcePools:\n- name: rook-pool\n\ncluster:\nname: rook-cluster\nnetwork:\nmode: nat\ncidr: 192.168.113.0/24\nnodeTemplate:\nuser: k8s\nupdateOnBoot: true\nssh:\naddToKnownHosts: true\nos:\ndistro: ubuntu\nnodes:\nmaster:\ninstances:\n- id: 1\nworker:\ninstances:\n- id: 1\ndataDisks:\n- name: rook\npool: rook-pool\nsize: 256\n- id: 2\ndataDisks:\n- name: rook\npool: rook-pool\nsize: 256\n- id: 3\n- id: 4\ndataDisks:\n- name: rook\npool: rook-pool\nsize: 256\n- name: test\npool: rook-pool\nsize: 32\n\nkubernetes:\nversion: v1.24.7\n\naddons:\nrook:\nenabled: true\n</code></pre>"},{"location":"examples/rook-cluster/#step-4-apply-the-configuration","title":"Step 4: Apply the configuration","text":"<pre><code>kubitect apply --config rook-sample.yaml\n</code></pre>"},{"location":"examples/rook-cluster/#node-selector","title":"Node selector","text":"<p>The node selector is a dictionary of labels and their potential values. The node selector restricts on which nodes Rook can be deployed, by selecting only those nodes that match all the specified labels.</p>"},{"location":"examples/rook-cluster/#step-1-set-node-labels","title":"Step 1: Set node labels","text":"<p>To use the node selector effectively, you should give your nodes custom labels.</p> <p>In this example, we label all worker nodes with the label <code>rook</code>. To ensure that scaling the cluster does not subsequently affect Rook, we set label's value to false by default. Only the nodes where Rook should be deployed are labeled <code>rook: true</code>, as shown in the figure below.</p> <p>The following configuration snippet shows how to set a default label and override it for a particular instance.</p> rook-sample.yaml<pre><code>cluster:\nnodes:\nworker:\ndefault:\nlabels:\nrook: false\ninstances:\n- id: 1\nlabels:\nrook: true # (1)!\n- id: 2\nlabels:\nrook: true\n- id: 3\nlabels:\nrook: true\n- id: 4\n</code></pre> <ol> <li>By default, the label <code>rook: false</code> is set for all worker nodes.      Setting the label <code>rook: true</code> for this particular instance overrides the default label.</li> </ol>"},{"location":"examples/rook-cluster/#step-2-configure-a-node-selector","title":"Step 2: Configure a node selector","text":"<p>So far we have labeled all worker nodes, but labeling is not enough to prevent Rook from being deployed on all worker nodes. To restrict on which nodes Rook resources can be deployed, we need to configure a node selector.</p> <p>We want to deploy Rook on the nodes labeled with the label <code>rook: true</code>, as shown in the figure below.</p> <p>The following configuration snippet shows how to configure the node selector mentioned above.</p> rook-sample.yaml<pre><code>addons:\nrook:\nenabled: true\nnodeSelector:\nrook: true\n</code></pre> Final cluster configuration  rook-sample.yaml<pre><code>hosts:\n- name: localhost\nconnection:\ntype: local\ndataResourcePools:\n- name: rook-pool\n\ncluster:\nname: rook-cluster\nnetwork:\nmode: nat\ncidr: 192.168.113.0/24\nnodeTemplate:\nuser: k8s\nupdateOnBoot: true\nssh:\naddToKnownHosts: true\nos:\ndistro: ubuntu\nnodes:\nmaster:\ninstances:\n- id: 1\nworker:\ndefault:\nlabels:\nrook: false\ninstances:\n- id: 1\nlabels:\nrook: true\ndataDisks:\n- name: rook\npool: rook-pool\nsize: 256\n- id: 2\nlabels:\nrook: true\ndataDisks:\n- name: rook\npool: rook-pool\nsize: 256\n- id: 3\nlabels:\nrook: true\n- id: 4\ndataDisks:\n- name: rook\npool: rook-pool\nsize: 256\n- name: test\npool: rook-pool\nsize: 32\n\nkubernetes:\nversion: v1.24.7\n\naddons:\nrook:\nenabled: true\nnodeSelector:\nrook: true\n</code></pre>"},{"location":"examples/rook-cluster/#step-3-apply-the-configuration","title":"Step 3: Apply the configuration","text":"<pre><code>kubitect apply --config rook-sample.yaml\n</code></pre>"},{"location":"examples/single-node-cluster/","title":"Single node cluster","text":"<p>This example shows how to setup a single node Kubernetes cluster using Kubitect.</p> <p>Note</p> <p>In this example we skip the explanation of some common configurations (hosts, network, node template, ...), as they are already explained in the Getting started (step-by-step) guide.</p>"},{"location":"examples/single-node-cluster/#single-node-cluster","title":"Single node cluster","text":""},{"location":"examples/single-node-cluster/#step-1-create-the-configuration","title":"Step 1: Create the configuration","text":"<p>If you want to initialize a cluster with only one node, specify a single master node in the cluster configuration file:</p> single-node.yaml<pre><code>cluster:\n...\nnodes:\nmaster:\ninstances:\n- id: 1\nip: 192.168.113.10 # (1)!\n</code></pre> <ol> <li>Static IP address of the node.      If the <code>ip</code> property is omitted, the DHCP lease is requested when the cluster is created.</li> </ol> Final cluster configuration  single-node.yaml<pre><code>hosts:\n- name: localhost\nconnection:\ntype: local\n\ncluster:\nname: k8s-cluster\nnetwork:\nmode: nat\ncidr: 192.168.113.0/24\nnodeTemplate:\nuser: k8s\nupdateOnBoot: true\nssh:\naddToKnownHosts: true\nos:\ndistro: ubuntu\nnodes:\nmaster:\ndefault:\nram: 4\ncpu: 2\nmainDiskSize: 32\ninstances:\n- id: 1\nip: 192.168.113.10\n\nkubernetes:\nversion: v1.24.7\nnetworkPlugin: calico\n</code></pre>"},{"location":"examples/single-node-cluster/#step-2-applying-the-configuration","title":"Step 2: Applying the configuration","text":"<p>Apply the cluster: <pre><code>kubitect apply --config single-node.yaml\n</code></pre></p> <p>Your master node now also becomes a worker node.</p>"},{"location":"getting-started/getting-started/","title":"Getting started (step-by-step)","text":"<p>In the quick start guide, we learned how to create a Kubernetes cluster using a preset configuration.  Now, we will explore how to create a customized cluster topology that meets your specific requirements.</p> <p>This step-by-step guide will walk you through the process of creating a custom cluster configuration file from scratch and using it to create a functional Kubernetes cluster with one master and one worker node. By following the steps outlined in this guide, you will have a Kubernetes cluster up and running in no time.</p>"},{"location":"getting-started/getting-started/#getting-started","title":"Getting Started","text":""},{"location":"getting-started/getting-started/#step-1-ensure-all-requirements-are-met","title":"Step 1 - Ensure all requirements are met","text":"<p>Before progressing with this guide, take a minute to ensure that all of the requirements are met. Afterwards, simply create a new YAML file and open it in a text editor of your choice.</p>"},{"location":"getting-started/getting-started/#step-2-prepare-hosts-configuration","title":"Step 2 - Prepare hosts configuration","text":"<p>In the cluster configuration file, the first step is to define hosts.  Hosts represent target servers that can be either local or remote machines.</p> LocalhostRemote host <p>When setting up the cluster on your local host, where the command line tool is installed, be sure to specify a host with a connection type set to <code>local</code>.</p> kubitect.yaml<pre><code>hosts:\n- name: localhost # (1)!\nconnection:\ntype: local\n</code></pre> <ol> <li>Custom unique name of the host.</li> </ol> <p>In case the cluster is deployed on a remote host, you will be required to provide the IP address of the remote machine along with the SSH credentials.</p> kubitect.yaml<pre><code>hosts:\n- name: my-remote-host\nconnection:\ntype: remote\nuser: myuser\nip: 10.10.40.143 # (1)!\nssh:\nkeyfile: \"~/.ssh/id_rsa_server1\" # (2)!\n</code></pre> <ol> <li> <p>IP address of the remote host.</p> </li> <li> <p>Path to the password-less SSH key file required for establishing connection with the remote host. </p> </li> </ol> <p>Throughout this guide, only localhost will be used.</p>"},{"location":"getting-started/getting-started/#step-3-define-cluster-infrastructure","title":"Step 3 - Define cluster infrastructure","text":"<p>The second part of the configuration file consists of the cluster infrastructure. In this part, all cluster nodes are defined along with their properties such as operating system, CPU cores, amount of RAM and so on.</p> <p>Below is an image that visualizes the components of the final cluster.</p> <p>Let's shift our attention to the following configuration:</p> kubitect.yaml<pre><code>cluster:\nname: k8s-cluster\nnetwork:\n...\nnodeTemplate:\n...\nnodes:\n...\n</code></pre> <p>As we can see, the cluster infrastructure section consists of the cluster name and three subsections:</p> <ul> <li> <p><code>cluster.name</code> </p> <p>The cluster name is used as a prefix for each resource created by Kubitect.  It's an essential property that helps identify and manage resources created by Kubitect.</p> </li> <li> <p><code>cluster.network</code> </p> <p>The network subsection holds information about the network properties of the cluster.  It defines the IP address range, the mode of networking, and other network-specific properties that apply to the entire cluster.</p> </li> <li> <p><code>cluster.nodeTemplate</code> </p> <p>The node template subsection contains properties that apply to all nodes in the cluster, such as the operating system, SSH user, and SSH private key.</p> </li> <li> <p><code>cluster.nodes</code> </p> <p>The nodes subsection defines each node in our cluster.  This subsection includes information such as the node name, node type, and other node-specific properties.</p> </li> </ul> <p>Now that we have a general idea of the cluster infrastructure configuration, let's examine each of these subsections in more detail to understand how to define them properly and configure a Kubernetes cluster using Kubitect.</p>"},{"location":"getting-started/getting-started/#step-31-cluster-network","title":"Step 3.1 - Cluster network","text":"<p>In the network subsection of the Kubernetes configuration file, we need to define the network that our cluster will use.  Currently, there are two supported network modes - NAT or bridge.</p> <p>The <code>nat</code> network mode creates a virtual network that performs network address translation. This mode allows the use of IP address ranges that do not exist within our local area network (LAN).</p> <p>On the other hand, the <code>bridge</code> network mode uses a predefined bridge interface, allowing virtual machines to connect directly to the LAN.  This mode is mandatory when the cluster spreads over multiple hosts.</p> <p>For the sake of simplicity, this tutorial will use the NAT mode as it does not require a preconfigured bridge interface.\"</p> kubitect.yaml<pre><code>cluster:\n...\nnetwork:\nmode: nat\ncidr: 192.168.113.0/24\n</code></pre> <p>The above configuration will instruct Kubitect to create a virtual network that uses <code>192.168.113.0/24</code> IP range.</p>"},{"location":"getting-started/getting-started/#step-32-node-template","title":"Step 3.2 - Node template","text":"<p>The <code>nodeTemplate</code> subsection allows you to define general properties for all nodes in the cluster. While there are no required fields, there are several useful properties you may want to include.</p> <ul> <li> <p><code>user</code> </p> <p>This property specifies the name of the user that will be created on all virtual machines and used for SSH. (default: <code>k8s</code>) </p> </li> <li> <p><code>os.distro</code> </p> <p>This property defines the operating system for the nodes. Currently, only Ubuntu and Debian are supported, and by default, the latest Ubuntu 22.04 release is used.</p> </li> <li> <p><code>ssh.addToKnownHosts</code></p> <p>When this property is set to true, all nodes will be added to SSH known hosts.  If you later destroy the cluster, these nodes will also be removed from the known hosts.</p> </li> <li> <p><code>updateOnBoot</code></p> <p>This property determines whether virtual machines are updated at first boot</p> </li> </ul> <p>To illustrate, let's set these <code>nodeTemplate</code> properties in our configuration file:</p> kubitect.yaml<pre><code>cluster:\n...\nnodeTemplate:\nuser: k8s\nupdateOnBoot: true\nssh:\naddToKnownHosts: true\nos:\ndistro: ubuntu22\n</code></pre>"},{"location":"getting-started/getting-started/#step-33-cluster-nodes","title":"Step 3.3 - Cluster nodes","text":"<p>In the <code>nodes</code> subsection, we define all nodes that will form the cluster. Each node can be defined as one of the following three types:</p> <ul> <li> <p><code>worker</code> </p> <p>A worker node runs the applications and workloads that are deployed in the cluster.  It communicates with the master node to receive instructions on how to schedule and run the containers. </p> </li> <li> <p><code>master</code> </p> <p>Master nodes are responsible for managing and coordinating the worker nodes in the cluster. Therefore, each cluster must contain at least one master node. </p> <p>Since etcd key-value datastore is also present on these nodes, the number of master nodes must be odd.  For more information, see etcd FAQ .</p> </li> <li> <p><code>loadBalancer</code> </p> <p>These nodes server as internal load balancers that expose the Kubernetes control plane at a single endpoint. They are essential when more then one master node is configured in the cluster.</p> </li> </ul> <p>This guide is focused on deploying a Kubernetes cluster with only one master node, which eliminates the need for internal load balancers.  However, if you are interested in creating a multi-master or high-availability (HA) cluster, please refer to the corresponding examples.</p> <p>To better understand this part, let's take a look at an example configuration:</p> kubitect.yaml<pre><code>cluster:\n...\nnodes:\nmaster:\ndefault: # (1)!\nram: 4 # (2)!\ncpu: 2 # (3)!\nmainDiskSize: 32 # (4)!\ninstances: # (5)!\n- id: 1 # (6)!\nip: 192.168.113.10 # (7)!\nworker:\ndefault: ram: 8\ncpu: 2\nmainDiskSize: 32\ninstances:\n- id: 1\nip: 192.168.113.21\nram: 4 # (8)!\n</code></pre> <ol> <li> <p>Default properties are applied to all nodes of the same type, which in this case are the master nodes.     They are particularly useful to quickly configure multiple nodes of the same type.</p> </li> <li> <p>The amount of RAM allocated to the master nodes (in GiB).</p> </li> <li> <p>The number of virtual CPUs assigned to each master node.</p> </li> <li> <p>The size of the virtual disk attached to each master node (in GiB).</p> </li> <li> <p>A list of master node instances.</p> </li> <li> <p>The instance ID is the only required field that must be specified for each instance.</p> </li> <li> <p>A static IP address set for this particular instance.     If the <code>ip</code> property is omitted, the node requests a DHCP lease during creation.</p> </li> <li> <p>In this example, the amount of RAM allocated to the worker node instance is set to 4 GiB, which overwrites the default value of 8 GiB.</p> </li> </ol>"},{"location":"getting-started/getting-started/#step-34-kubernetes-properties","title":"Step 3.4 - Kubernetes properties","text":"<p>The final section of the cluster configuration contains the Kubernetes properties,  such as the version and network plugin.</p> kubitect.yaml<pre><code>kubernetes:\nversion: v1.24.7\nnetworkPlugin: calico\n</code></pre>"},{"location":"getting-started/getting-started/#step-4-create-the-cluster","title":"Step 4 - Create the cluster","text":"<p>Below is the final configuration for our Kubernetes cluster:</p> Final cluster configuration  kubitect.yaml<pre><code>hosts:\n- name: localhost\nconnection:\ntype: local\n\ncluster:\nname: k8s-cluster\nnetwork:\nmode: nat\ncidr: 192.168.113.0/24\nnodeTemplate:\nuser: k8s\nupdateOnBoot: true\nssh:\naddToKnownHosts: true\nos:\ndistro: ubuntu22\nnodes:\nmaster:\ndefault:\nram: 4\ncpu: 2\nmainDiskSize: 32\ninstances:\n- id: 1\nip: 192.168.113.10\nworker:\ndefault: ram: 8\ncpu: 2\nmainDiskSize: 32\ninstances:\n- id: 1\nip: 192.168.113.21\nram: 4\n\nkubernetes:\nversion: v1.24.7\nnetworkPlugin: calico\n</code></pre> <p>To create the cluster, apply the configuration file to Kubitect: <pre><code>kubitect apply --config kubitect.yaml\n</code></pre></p> <p>Tip</p> <p>If you encounter any issues during the installation process, please refer to the troubleshooting page first.</p> <p>After applying the configuration file to Kubitect, a directory for the created Kubernetes cluster is generated and stored in Kubitect's home directory. The default location for the home directory is <code>~/.kubitect</code> and has the following structure.</p> <pre><code>~/.kubitect\n   \u251c\u2500\u2500 clusters\n   \u2502   \u251c\u2500\u2500 k8s-cluster\n   \u2502   \u251c\u2500\u2500 my-cluster\n   \u2502   \u2514\u2500\u2500 ...\n   \u2514\u2500\u2500 share\n       \u251c\u2500\u2500 terraform\n       \u2514\u2500\u2500 venv\n</code></pre> <p>The <code>clusters</code> directory contains a subdirectory for each Kubernetes cluster that you have created using Kubitect.  Each subdirectory is named after the cluster, for example k8s-cluster.  The configuration files for each cluster are stored in these directories.</p> <p>The <code>share</code> directory contains files and directories that are shared between different cluster installations.</p> <p>All created clusters can be listed at any time using the <code>list</code> subcommand. <pre><code>kubitect list clusters\n\n# Clusters:\n#   - k8s-cluster (active)\n#   - my-cluster (active)\n</code></pre></p>"},{"location":"getting-started/getting-started/#step-5-test-the-cluster","title":"Step 5 - Test the cluster","text":"<p>Once you have successfully installed a Kubernetes cluster, the Kubeconfig file can be found in the cluster's directory.  However, you will most likely want to export the Kubeconfig to a separate file:</p> <pre><code>kubitect export kubeconfig --cluster k8s-cluster &gt; kubeconfig.yaml\n</code></pre> <p>This will create a file named <code>kubeconfig.yaml</code> in your current directory. Finally, to confirm that the cluster is ready, you can list its nodes using the <code>kubectl</code> command:</p> <pre><code>kubectl get nodes --kubeconfig kubeconfig.yaml\n</code></pre> <p> Congratulations, you have completed the getting started quide.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#installation","title":"Installation","text":""},{"location":"getting-started/installation/#install-kubitect-cli-tool","title":"Install Kubitect CLI tool","text":"Release pageGo packages <p>Download Kubitect binary file from the release page. <pre><code>curl -o kubitect.tar.gz -L https://dl.kubitect.io/linux/amd64/latest\n</code></pre></p> <p>Unpack <code>tar.gz</code> file. <pre><code>tar -xzf kubitect.tar.gz\n</code></pre></p> <p>Install Kubitect command line tool by placing the Kubitect binary file in <code>/usr/local/bin</code> directory. <pre><code>sudo mv kubitect /usr/local/bin/\n</code></pre></p> <p>Note</p> <p>The download URL is a combination of the operating system type, system architecture and version of Kubitect (<code>https://dl.kubitect.io/&lt;os&gt;/&lt;arch&gt;/&lt;version&gt;</code>).</p> <p>All releases can be found on GitHub release page.</p> <p>Install Kubitect from Go packages.</p> <pre><code>go install github.com/MusicDin/kubitect/cmd/kubitect@latest\n</code></pre> <p>Verify the installation by checking the Kubitect version. <pre><code>kubitect --version\n\n# kubitect version v3.0.0\n</code></pre></p>"},{"location":"getting-started/installation/#enable-shell-autocomplete","title":"Enable shell autocomplete","text":"<p>To load completions in your current shell session (<code>bash</code>): <pre><code>source &lt;(kubitect completion bash)\n</code></pre></p> <p>To load completions for every new session, execute once: <pre><code>kubitect completion bash &gt; /etc/bash_completion.d/kubitect\n</code></pre></p> <p>Tip</p> <p>To list all supported shells, run: <code>kubitect completion -h</code></p> <p>For shell specific instructions run: <code> kubitect completion shell -h </code></p>"},{"location":"getting-started/quick-start/","title":"Quick start","text":"<p>In this quick guide, we will show you how to use the Kubitect command line tool to quickly deploy a simple Kubernetes cluster.</p> <p>To get started, you will need to apply a cluster configuration file to the Kubitect command line tool.  You can either prepare this file manually, as explained in our Getting started guide, or use one of the available presets.</p> <p>For the purposes of this quick start guide, we will be using a <code>getting-started</code> preset, which defines a cluster with one master and one worker node. The resulting infrastructure is shown in the image below.</p>"},{"location":"getting-started/quick-start/#quick-start","title":"Quick start","text":""},{"location":"getting-started/quick-start/#step-1-create-a-kubernetes-cluster","title":"Step 1 - Create a Kubernetes cluster","text":"<p>Export the <code>gettings-started</code> preset:</p> <pre><code>kubitect export preset --name getting-started &gt; cluster.yaml </code></pre> <p>Then, apply the exported configuration file to the Kubitect:</p> <pre><code>kubitect apply --config cluster.yaml\n</code></pre> <p>That's it! The cluster, named <code>k8s-cluster</code>, should be up and running in approximately 10 minutes.</p>"},{"location":"getting-started/quick-start/#step-2-export-kubeconfig","title":"Step 2 - Export kubeconfig","text":"<p>After successfully installing the Kubernetes cluster, a Kubeconfig file will be created within the cluster's directory.  To export the Kubeconfig to a custom file, use the following command:</p> <pre><code>kubitect export kubeconfig --cluster k8s-cluster &gt; kubeconfig.yaml\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-test-the-cluster","title":"Step 3 - Test the cluster","text":"<p>To test that the cluster is up and running, display all cluster nodes using the exported Kubeconfig and the kubectl command:</p> <pre><code>kubectl get nodes --kubeconfig kubeconfig.yaml\n</code></pre> <p> Congratulations, you have successfully deployed a Kubernetes cluster using Kubitect! </p>"},{"location":"getting-started/requirements/","title":"Requirements","text":"<p>On the local host (where Kubitect command-line tool is installed), the following requirements must be met:</p> <p> Git</p> <p> Python &gt;= 3.8</p> <p> Python virtualenv</p> <p> Password-less SSH key for each remote host</p> <p></p> <p>On hosts where a Kubernetes cluster will be deployed using Kubitect, the following requirements must be met:</p> <p> A libvirt virtualization API</p> <p> A running hypervisor that is supported by libvirt (e.g. KVM)</p> How to install KVM?  <p>To install the KVM (Kernel-based Virtual Machine) hypervisor and libvirt, use apt or yum to install the following packages:</p> <ul> <li><code>qemu-kvm</code></li> <li><code>libvirt-clients</code></li> <li><code>libvirt-daemon</code></li> <li><code>libvirt-daemon-system</code></li> </ul> <p>After the installation, add your user to the <code>kvm</code> group in order to access the kvm device:</p> <pre><code>sudo usermod -aG kvm $USER\n</code></pre>"},{"location":"getting-started/requirements/#requirements","title":"Requirements","text":""},{"location":"getting-started/other/local-development/","title":"Local development","text":"<p>This document shows how to build a CLI tool manually and how to use the project without creating any files outside the project's directory.</p>"},{"location":"getting-started/other/local-development/#local-development","title":"Local development","text":""},{"location":"getting-started/other/local-development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Git</li> <li>Go 1.18 or greater</li> </ul>"},{"location":"getting-started/other/local-development/#step-1-clone-the-project","title":"Step 1: Clone the project","text":"<p>First, clone the project. <pre><code>git clone https://github.com/MusicDin/kubitect\n</code></pre></p> <p>Afterwards, move into the cloned project. <pre><code>cd kubitect\n</code></pre></p>"},{"location":"getting-started/other/local-development/#step-2-build-kubitect-cli-tool","title":"Step 2: Build Kubitect CLI tool","text":"<p>The Kubitect CLI tool can be manually built using Go.  Running the following command will produce a <code>kubitect</code> binary file. <pre><code>go build .\n</code></pre></p> <p>To make the binary file globally accessible, move it to the <code>/usr/local/bin/</code> directory. <pre><code>sudo mv kubitect /usr/local/bin/kubitect\n</code></pre></p>"},{"location":"getting-started/other/local-development/#step-3-local-development","title":"Step 3: Local development","text":"<p>By default, Kubitect creates and manages clusters in the Kubitect's home directory (<code>~/.kubitect</code>). However, for development purposes, it is often more convenient to have all resources created in the current directory.</p> <p>If you want to create a new cluster in the current directory, you can use the <code>--local</code> flag when applying the configuration.  When you create a cluster using the <code>--local</code> flag, its name will be prefixed with local.  This prefix is added to prevent any conflicts that might arise when creating new virtual resources.</p> <pre><code>kubitect apply --local\n</code></pre> <p>The resulting cluster will be created in <code>./.kubitect/clusters/local-&lt;cluster-name&gt;</code> directory.</p>"},{"location":"getting-started/other/troubleshooting/","title":"Troubleshooting","text":"<p>Is your issue not listed here?</p> <p>If the troubleshooting page is missing an error you encountered, please report it on GitHub by opening an issue. By doing so, you will help improve the project and help others find the solution to the same problem faster.</p>"},{"location":"getting-started/other/troubleshooting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/other/troubleshooting/#general-errors","title":"General errors","text":""},{"location":"getting-started/other/troubleshooting/#virtualenv-not-found","title":"Virtualenv not found","text":"Error Explanation Solution <p>Error</p> <p>Output: /bin/sh: 1: virtualenv: not found</p> <p>/bin/sh: 2: ansible-playbook: not found</p> <p>Explanation</p> <p>The error indicates that the <code>virtualenv</code> is not installed.</p> <p>Solution</p> <p>There are many ways to install <code>virtualenv</code>. For all installation options you can refere to their official documentation - Virtualenv installation.</p> <p>For example, virtualenv can be installed using <code>pip</code>.</p> <p>First install pip. <pre><code>sudo apt install python3-pip\n</code></pre></p> <p>Then install virtualenv using pip3. <pre><code>pip3 install virtualenv\n</code></pre></p>"},{"location":"getting-started/other/troubleshooting/#kvmlibvirt-errors","title":"KVM/Libvirt errors","text":""},{"location":"getting-started/other/troubleshooting/#failed-to-connect-socket-no-such-file-or-directory","title":"Failed to connect socket (No such file or directory)","text":"Error Explanation Solution <p>Error</p> <p>Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such file or directory')</p> <p>Explanation</p> <p>The problem may occur when libvirt is not started.</p> <p>Solution</p> <p>Make sure that the <code>libvirt</code> service is running: <pre><code>sudo systemctl status libvirtd\n</code></pre></p> <p>If the <code>libvirt</code> service is not running, start it: <pre><code>sudo systemctl start libvirtd\n</code></pre></p> <p>Optional: Start the <code>libvirt</code> service automatically at boot time: <pre><code>sudo systemctl enable libvirtd\n</code></pre></p>"},{"location":"getting-started/other/troubleshooting/#failed-to-connect-socket-permission-denied","title":"Failed to connect socket (Permission denied)","text":"Error Explanation Solution <p>Error</p> <p>Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied')</p> <p>Explanation</p> <p>The error indicates that either the <code>libvirtd</code> service is not running or the current user is not in the <code>libvirt</code> (or <code>kvm</code>) group.</p> <p>Solution</p> <p>If the <code>libvirtd</code> service is not running, start it: <pre><code>sudo systemctl start libvirtd\n</code></pre></p> <p>Add the current user to the <code>libvirt</code> and <code>kvm</code> groups if needed: <pre><code># Add current user to groups\nsudo adduser $USER libvirt\nsudo adduser $USER kvm\n\n# Verify groups are added\nid -nG\n\n# Reload user session\n</code></pre></p>"},{"location":"getting-started/other/troubleshooting/#error-creating-libvirt-domain","title":"Error creating libvirt domain","text":"Error Explanation Solution <p>Error</p> <p>Error: Error creating libvirt domain: \u2026 Could not open '/tmp/terraform_libvirt_provider_images/image.qcow2': Permission denied')</p> <p>Explanation</p> <p>The error indicates that the file cannot be created in the specified location due to missing permissions.</p> <ul> <li>Make sure the directory exists.</li> <li>Make sure the directory of the file that is being denied has appropriate user permissions.</li> <li>Optionally qemu security driver can be disabled. </li> </ul> <p>Solution</p> <p>Make sure the <code>security_driver</code> in <code>/etc/libvirt/qemu.conf</code> is set to <code>none</code> instead of <code>selinux</code>. This line is commented out by default, so you should uncomment it if needed: <pre><code># /etc/libvirt/qemu.conf\n\n...\nsecurity_driver = \"none\"\n...\n</code></pre></p> <p>Do not forget to restart the <code>libvirt</code> service after making the changes: <pre><code>sudo systemctl restart libvirtd\n</code></pre></p>"},{"location":"getting-started/other/troubleshooting/#libvirt-domain-already-exists","title":"Libvirt domain already exists","text":"Error Explanation Solution <p>Error</p> <p>Error: Error defining libvirt domain: virError(Code=9, Domain=20, Message='operation failed: domain 'your-domain' already exists with uuid '...')</p> <p>Explanation</p> <p>The error indicates that the libvirt domain (virtual machine) already exists.</p> <p>Solution</p> <p>The resource you are trying to create already exists.  Make sure you destroy the resource: <pre><code>virsh destroy your-domain\nvirsh undefine your-domain\n</code></pre></p> <p>You can verify that the domain was successfully removed: <pre><code>virsh dominfo --domain your-domain\n</code></pre></p> <p>If the domain was successfully removed, the output should look something like this:</p> <p><code> error: failed to get domain 'your-domain' </code></p>"},{"location":"getting-started/other/troubleshooting/#libvirt-volume-already-exists","title":"Libvirt volume already exists","text":"Error Explanation Solution <p>Error</p> <p>Error: Error creating libvirt volume: virError(Code=90, Domain=18, Message='storage volume 'your-volume.qcow2' exists already')</p> <p>and / or</p> <p>Error:Error creating libvirt volume for cloudinit device cloud-init.iso: virError(Code=90, Domain=18, Message='storage volume 'cloud-init.iso' exists already')</p> <p>Explanation</p> <p>The error indicates that the specified volume already exists.</p> <p>Solution</p> <p>Volumes created by Libvirt are still attached to the images, which prevents a new volume from being created with the same name. Therefore, these volumes must be removed:</p> <p><code> virsh vol-delete cloud-init.iso --pool your_resource_pool </code></p> <p>and / or</p> <p><code> virsh vol-delete your-volume.qcow2 --pool your_resource_pool </code></p>"},{"location":"getting-started/other/troubleshooting/#libvirt-storage-pool-already-exists","title":"Libvirt storage pool already exists","text":"Error Explanation Solution <p>Error</p> <p>Error: Error storage pool 'your-pool' already exists</p> <p>Explanation</p> <p>The error indicates that the libvirt storage pool already exists.</p> <p>Solution</p> <p>Remove the existing libvirt storage pool.</p> <p><code> virsh pool-destroy your-pool &amp;&amp; virsh pool-undefine your-pool </code></p>"},{"location":"getting-started/other/troubleshooting/#failed-to-apply-firewall-rules","title":"Failed to apply firewall rules","text":"Error Explanation Solution <p>Error</p> <p>Error: internal error: Failed to apply firewall rules /sbin/iptables -w --table filter --insert LIBVIRT_INP --in-interface virbr2 --protocol tcp --destination-port 67 --jump ACCEPT: iptables: No chain/target/match by that name.</p> <p>Explanation</p> <p>Libvirt was already running when firewall (usually FirewallD) was started/installed. Therefore, <code>libvirtd</code> service must be restarted to detect the changes.</p> <p>Solution</p> <p>Restart the <code>libvirtd</code> service: <pre><code>sudo systemctl restart libvirtd\n</code></pre></p>"},{"location":"getting-started/other/troubleshooting/#failed-to-remove-storage-pool","title":"Failed to remove storage pool","text":"Error Explanation Solution <p>Error</p> <p>Error: error deleting storage pool: failed to remove pool '/var/lib/libvirt/images/k8s-cluster-main-resource-pool': Directory not empty</p> <p>Explanation</p> <p>The pool cannot be deleted because there are still some volumes in the pool. Therefore, the volumes should be removed before the pool can be deleted.</p> <p>Solution</p> <ol> <li> <p>Make sure the pool is running. <pre><code>virsh pool-start --pool k8s-cluster-main-resource-pool\n</code></pre></p> </li> <li> <p>List volumes in the pool. <pre><code>virsh vol-list --pool k8s-cluster-main-resource-pool\n\n#  Name         Path\n# -------------------------------------------------------------------------------------\n#  base_volume  /var/lib/libvirt/images/k8s-cluster-main-resource-pool/base_volume\n</code></pre></p> </li> <li> <p>Delete listed volumes from the pool. <pre><code>virsh vol-delete --pool k8s-cluster-main-resource-pool --vol base_volume\n</code></pre></p> </li> <li> <p>Destroy and undefine the pool. <pre><code>virsh pool-destroy --pool k8s-cluster-main-resource-pool\nvirsh pool-undefine --pool k8s-cluster-main-resource-pool\n</code></pre></p> </li> </ol>"},{"location":"getting-started/other/troubleshooting/#haproxy-load-balancer-errors","title":"HAProxy load balancer errors","text":""},{"location":"getting-started/other/troubleshooting/#random-haproxy-503-bad-gateway","title":"Random HAProxy (503) bad gateway","text":"Error Explanation Solution <p>Error</p> <p>HAProxy returns a random HTTP 503 (Bad gateway) error.</p> <p>Explanation</p> <p>More than one HAProxy processes are listening on the same port.</p> <p>Solution 1</p> <p>For example, if an error is thrown when accessing port <code>80</code>, check which processes are listening on port <code>80</code> on the load balancer VM: <pre><code>netstat -lnput | grep 80\n\n# Proto Recv-Q Send-Q Local Address           Foreign Address   State       PID/Program name\n# tcp        0      0 192.168.113.200:80      0.0.0.0:*         LISTEN      1976/haproxy\n# tcp        0      0 192.168.113.200:80      0.0.0.0:*         LISTEN      1897/haproxy\n</code></pre></p> <p>If you see more than one process, kill the unnecessary process: <pre><code>kill 1976\n</code></pre></p> <p>Note: You can kill all HAProxy processes and only one will be automatically recreated.</p> <p>Solution 2</p> <p>Check the HAProxy configuration file (<code>config/haproxy/haproxy.cfg</code>) that it does not contain 2 frontends bound to the same port.</p>"},{"location":"user-guide/before-you-begin/","title":"Before you begin","text":"<p>The user guide is divided into three subsections: Configuration, Cluster Management, and Reference. The Configuration subsection contains explanations of the configurable Kubitect properties. The Cluster Management subsection introduces the operations that can be performed over the cluster. Finally, the Reference subsection contains a configuration and CLI reference.</p> <p>The following symbol conventions are used throughout the user guide:</p> <ul> <li> - Indicates the Kubitect version in which the property was either added or last modified.</li> <li> - Indicates that the property is required in every valid configuration.</li> <li> - Indicates the default value of the property.</li> <li> - Indicates that the feature or property is experimental (not yet stable). This means that its implementation may change drastically over time and that its activation may lead to unexpected behavior.</li> </ul>"},{"location":"user-guide/before-you-begin/#before-you-begin","title":"Before you begin","text":""},{"location":"user-guide/configuration/addons/","title":"Addons","text":""},{"location":"user-guide/configuration/addons/#addons","title":"Addons","text":""},{"location":"user-guide/configuration/addons/#configuration","title":"Configuration","text":""},{"location":"user-guide/configuration/addons/#kubespray-addons","title":"Kubespray addons","text":"<p> v2.1.0</p> <p>Kubespray offers many useful configurable addons, such as the Ingress-NGINX controller, MetalLB, and so on.</p> <p>Kubespray addons can be configured in Kubitect under the <code>addons.kubespray</code> property. The configuration of Kubespray addons is exactly the same as the default configuration of Kubespray addons, since Kubitect simply copies the provided configuration into Kubespray's group variables when the cluster is created.</p> <p>All available Kubespray addons can be found in the Kubespray addons sample, while most of them are documented in the official Kubespray documentation.</p> <pre><code>addons:\nkubespray:\n\n# Nginx ingress controller deployment\ningress_nginx_enabled: true\ningress_nginx_namespace: \"ingress-nginx\"\ningress_nginx_insecure_port: 80\ningress_nginx_secure_port: 443\n\n# MetalLB deployment\nmetallb_enabled: true\nmetallb_speaker_enabled: true\nmetallb_ip_range:\n- \"10.10.9.201-10.10.9.254\"\nmetallb_pool_name: \"default\"\nmetallb_auto_assign: true\nmetallb_version: v0.12.1\nmetallb_protocol: \"layer2\"\n</code></pre>"},{"location":"user-guide/configuration/addons/#rook-addon","title":"Rook addon","text":"<p> v2.2.0 Experimental</p> <p>Rook is an orchestration tool that allows Ceph, a reliable and scalable storage, to run within a Kubernetes cluster.</p> <p>In Kubitect, Rook can be enabled by simply setting <code>addons.rook.enabled</code> to true.</p> <pre><code>addons:\nrook:\nenabled: true\n</code></pre> <p>Rook is deployed only on worker nodes. When a cluster is created without worker nodes, Kubitect attempts to install Rook on the master node.</p> <p>In addition to enabling the Rook addon, at least one data disk must be attached to a node suitable for Rook deployment. If Kubitect determines that no data disks are available for Rook, it will simply skip installing Rook.</p> <p>By default, Rook uses all available data disks attached to worker nodes and converts them to distributed storage. Similarly, all worker nodes are used for Rook deployment. To restrict on which nodes Rook resources can be deployed, the node selector can be used.</p>"},{"location":"user-guide/configuration/addons/#node-selector","title":"Node selector","text":"<p>The node selector is a dictionary of node labels used to determine which nodes are eligible for Rook deployment. If a node does not match all of the specified node labels, Rook resources cannot be deployed on that node and disks attached to that node are not used for distributed storage.</p> <pre><code>addons:\nrook:\nnodeSelector:\nrook: true\n</code></pre>"},{"location":"user-guide/configuration/addons/#version","title":"Version","text":"<p>By default, the latest (<code>master</code>) Rook version is used. To use a specific version of Rook, set the <code>addons.rook.version</code> property to the desired version.</p> <pre><code>addons:\nrook:\nversion: v1.9.9\n</code></pre>"},{"location":"user-guide/configuration/cluster-name/","title":"Cluster name","text":""},{"location":"user-guide/configuration/cluster-name/#cluster-metadata","title":"Cluster metadata","text":""},{"location":"user-guide/configuration/cluster-name/#configuration","title":"Configuration","text":""},{"location":"user-guide/configuration/cluster-name/#cluster-name","title":"Cluster name","text":"<p> v2.0.0  Required</p> <p>The cluster name must be defined as part of the Kubitect configuration. It will be used as a prefix for all resources created by Kubitect as part of this cluster.</p> <pre><code>cluster:\nname: my-cluster\n</code></pre> <p>For example, the name of each virtual machine is generated as <code>&lt;cluster.name&gt;-&lt;node.type&gt;-&lt;node.instance.id&gt;</code>. This means that the virtual machine name of the master node with ID <code>1</code> would result in <code>my-cluster-master-1</code>.</p> <p>Note</p> <p>Cluster name cannot contain prefix <code>local</code>, as it is reserved for local clusters (created with <code>--local</code> flag).</p>"},{"location":"user-guide/configuration/cluster-network/","title":"Cluster network","text":"<p>This document describes how to define the cluster network in the Kubitect configuration. It defines either the properties of the network to be created or the network to which the cluster nodes are to be assigned.</p>"},{"location":"user-guide/configuration/cluster-network/#cluster-network","title":"Cluster network","text":""},{"location":"user-guide/configuration/cluster-network/#configuration","title":"Configuration","text":""},{"location":"user-guide/configuration/cluster-network/#network-mode","title":"Network mode","text":"<p> v2.0.0  Required</p> <p>Kubitect supports two network modes.  The first is the <code>nat</code> mode and the other is the <code>bridge</code> mode.</p> <pre><code>cluster:\nnetwork:\nmode: nat\n</code></pre>"},{"location":"user-guide/configuration/cluster-network/#nat-mode","title":"NAT mode","text":"<p>In NAT (Network Address Translation) mode, the libvirt virtual network is created for the cluster. This reduces the need for manual configurations, but is limited to one host (a single physical server).</p>"},{"location":"user-guide/configuration/cluster-network/#bridge-mode","title":"Bridge mode","text":"<p>In bridge mode, a real host network device is shared with the virtual machines. Therefore, each virtual machine can bind to any available IP address on the local network, just like a physical computer. This approach makes the virtual machine visible on the network, which enables the creation of clusters across multiple physical servers.</p> <p>The only requirement for using bridged networks is the preconfigured bridge interface on each target host. Preconfiguring bridge interfaces is necessary because every environment is different. For example, someone might use link aggregation (also known as link bonding or teaming), which cannot be detected automatically and therefore requires manual configuration. The Network bridge example describes how to create a bridge interface with netplan and configure Kubitect to use it.</p> <p>How to automate bridge interface creation?</p> <p>If you have an idea or suggestion on how to automate the creation of  bridge interfaces, feel free to open an issue on GitHub.</p>"},{"location":"user-guide/configuration/cluster-network/#network-cidr","title":"Network CIDR","text":"<p> v2.0.0  Required</p> <p>Network CIDR (Classless Inter-Domain Routing) defines the network in a form of <code>&lt;network_ip&gt;/&lt;network_prefix_bits&gt;</code>. All IP addresses specified in the cluster section of the configuration must be in this network range (this includes a network gateway, node instances, floating IP of the load balancer, etc.).</p> <p>When using NAT network mode, the network CIDR defines an unused private network that is created.  In bridge mode, the network CIDR should specify the network to which the cluster belongs.</p> <pre><code>cluster:\nnetwork:\ncidr: 192.168.113.0/24 # (1)!\n</code></pre> <ol> <li> <p>In <code>nat</code> mode - Any unused private network within a local network.</p> <p>In <code>bridge</code> mode - A network to which the cluster belongs.</p> </li> </ol>"},{"location":"user-guide/configuration/cluster-network/#network-gateway","title":"Network gateway","text":"<p> v2.0.0</p> <p>The network gateway (or default gateway) defines the IP address of the router. By default, it does not need to be specified because the first client IP in the network range is used as the gateway address. If the gateway IP differs from this, it must be specified manually.</p> <p>Also note that the gateway IP address must be within the specified network range.</p> <pre><code>cluster:\nnetwork:\ncidr: 10.10.0.0/20\ngateway: 10.10.0.230 # (1)!\n</code></pre> <ol> <li>If this option is omitted, <code>10.10.0.1</code> is used as the gateway IP (first client IP in the network range).</li> </ol>"},{"location":"user-guide/configuration/cluster-network/#network-bridge","title":"Network bridge","text":"<p> v2.0.0  Default: <code>virbr0</code></p> <p>The network bridge defines the bridge interface that virtual machines connect to.</p> <p>When the NAT network mode is used, a virtual network bridge interface is created on the host. Virtual bridges are usually prefixed with <code>vir</code> (example: <code>virbr44</code>). If this option is omitted, the virtual bridge name is automatically determined by libvirt. Otherwise, the specified name is used for the virtual bridge.</p> <p>In the case of bridge network mode, the network bridge should be the name of the preconfigured bridge interface (example: <code>br0</code>).</p> <pre><code>cluster:\nnetwork:\nbridge: br0\n</code></pre>"},{"location":"user-guide/configuration/cluster-network/#example-usage","title":"Example usage","text":""},{"location":"user-guide/configuration/cluster-network/#virtual-nat-network","title":"Virtual NAT network","text":"<p>If the cluster is created on a single host, the NAT network mode can be used. In this case, only the CIDR of the new network needs to be specified in addition to the network mode.</p> <pre><code>cluster:\nnetwork:\nmode: nat\ncidr: 192.168.113.0/24\n</code></pre>"},{"location":"user-guide/configuration/cluster-network/#bridged-network","title":"Bridged network","text":"<p>To make the cluster nodes visible on the local network as physical machines or to create the cluster across multiple hosts, bridge network mode must be used. Also, the network CIDR of an existing network must be specified along with the preconfigured host bridge interface.</p> <pre><code>cluster:\nnetwork:\nmode: bridge cidr: 10.10.64.0/24 bridge: br0 </code></pre>"},{"location":"user-guide/configuration/cluster-node-template/","title":"Cluster node template","text":"<p>The note template in the cluster section of the configuration defines the properties of all nodes in the cluster. This includes the properties of the operating system (OS), DNS, and virtual machine user.</p>"},{"location":"user-guide/configuration/cluster-node-template/#cluster-node-template","title":"Cluster node template","text":""},{"location":"user-guide/configuration/cluster-node-template/#configuration","title":"Configuration","text":""},{"location":"user-guide/configuration/cluster-node-template/#virtual-machine-user","title":"Virtual machine user","text":"<p> v2.0.0  Default: <code>k8s</code></p> <p>The user property defines the name of the passwordless user created on each virtual machine. It is used to access the virtual machines during cluster configuration. If the user property is omitted, a user named <code>k8s</code> is created on all virtual machines. This user can also be used later to access each virtual machine via SSH.</p> <pre><code>cluster:\nnodeTemplate:\nuser: kubitect\n</code></pre>"},{"location":"user-guide/configuration/cluster-node-template/#operating-system-os","title":"Operating system (OS)","text":""},{"location":"user-guide/configuration/cluster-node-template/#os-distribution","title":"OS distribution","text":"<p> v2.1.0  Default: <code>ubuntu</code></p> <p>The operating system for virtual machines can be specified in the node template. Currently, either Ubuntu or Debian can be configured. By default, the Ubuntu distribution is installed on all virtual machines. To use Debian instead, set <code>os.distro</code> property to Debian.</p> <pre><code>cluster:\nnodeTemplate:\nos:\ndistro: debian # (1)!\n</code></pre> <ol> <li>By default, <code>ubuntu</code> is used.</li> </ol> <p>Available OS distribution presets are the following:</p> <ul> <li><code>ubuntu</code> - Latest Ubuntu 22.04 release. (default)</li> <li><code>ubuntu22</code> - Ubuntu 22.04 release 2023-03-02.</li> <li><code>ubuntu20</code> - Ubuntu 20.04 release 2023-02-09.</li> <li><code>debian</code> - Latest Debian 11 release.</li> <li><code>debian11</code> - Debian 11 release 2023-01-24.</li> </ul> <p>Ubuntu images are downloaded from the Ubuntu cloud image repository and Debian images are downloaded from the Debian cloud image repository.</p>"},{"location":"user-guide/configuration/cluster-node-template/#custom-os-source","title":"Custom OS source","text":"<p> v2.1.0</p> <p>If the presets do not meet your needs, you can also use a custom Ubuntu or Debian image by simply specifying the image source. The source of an image can be either a local path on a system or an URL pointing to the image download.</p> <pre><code>cluster:\nnodeTemplate:\nos:\ndistro: ubuntu\nsource: https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img\n</code></pre>"},{"location":"user-guide/configuration/cluster-node-template/#primary-os-network-interface","title":"Primary OS network interface","text":"<p> v2.1.0</p> <p>When a virtual machine is created, the network interface names are evaluated deterministically. Therefore, Kubitect should use the correct network interface names for all available presets.</p> <p>However, if you want to instruct Kubitect to use a specific network interface as primary, set its name as the value of the <code>os.networkInterface</code> property.</p> <pre><code>cluster:\nnodeTemplate:\nos:\nnetworkInterface: ens3\n</code></pre>"},{"location":"user-guide/configuration/cluster-node-template/#custom-dns-list","title":"Custom DNS list","text":"<p> v2.1.0</p> <p>The list of Domain Name Servers (DNS) can be configured in the node template. These servers are used by all virtual machines for DNS resolution. By default, a DNS list contains only the network gateway.</p> <pre><code>cluster:\nnodeTemplate:\ndns: # (1)!\n- 1.1.1.1\n- 1.0.0.1\n</code></pre> <ol> <li>IP addresses <code>1.1.1.1</code> and <code>1.0.0.1</code> represent CloudFlare's primary and secondary public DNS resolvers, respectively.</li> </ol>"},{"location":"user-guide/configuration/cluster-node-template/#cpu-mode","title":"CPU mode","text":"<p> v2.2.0  Default: <code>custom</code></p> <p>The CPU mode property can be used to simplify the configuration of a guest CPU to be as close as possible to the host CPU. Consult the libvirt documentation to learn about all available CPU modes:</p> <ul> <li><code>custom</code> (default)</li> <li><code>host-model</code></li> <li><code>host-passthrough</code></li> <li><code>maximum</code></li> </ul> <pre><code>cluster:\nnodeTemplate:\ncpuMode: host-passthrough\n</code></pre>"},{"location":"user-guide/configuration/cluster-node-template/#ssh-options","title":"SSH options","text":""},{"location":"user-guide/configuration/cluster-node-template/#custom-ssh-certificate","title":"Custom SSH certificate","text":"<p> v2.0.0</p> <p>Kubitect ensures that SSH certificates are automatically generated before the cluster is deployed. The generated certificates are located in the <code>config/.ssh/</code> directory inside a cluster directory. You can use a custom SSH certificate by specifying a local path to the private key. Note that the public key must be located in the same directory with the <code>.pub</code> suffix.</p> <pre><code>cluster:\nnodeTemplate:\nssh:\nprivateKeyPath: \"~/.ssh/id_rsa_test\"\n</code></pre> <p>Warning</p> <p>SSH certificates must be passwordless, otherwise Kubespray will fail to configure the cluster.</p>"},{"location":"user-guide/configuration/cluster-node-template/#adding-nodes-to-the-known-hosts","title":"Adding nodes to the known hosts","text":"<p> v2.0.0  Default: <code>false</code></p> <p>In addition, Kubitect allows you to add all created virtual machines to SSH known hosts on the local machine. To enable this behavior, set the <code>addToKnownHosts</code> property to true.</p> <pre><code>cluster:\nnodeTemplate:\nssh:\naddToKnownHosts: true\n</code></pre>"},{"location":"user-guide/configuration/cluster-nodes/","title":"Cluster nodes","text":""},{"location":"user-guide/configuration/cluster-nodes/#cluster-nodes","title":"Cluster nodes","text":""},{"location":"user-guide/configuration/cluster-nodes/#nodes-configuration-structure","title":"Nodes configuration structure","text":"<p>Cluster's nodes configuration consists of three node types:</p> <ul> <li>Master nodes (control plane)</li> <li>Worker nodes</li> <li>Load balancers</li> </ul> <p>For any cluster deployment, at least one master node needs to be configured. Configuring only one master node produces a single-node cluster. In most cases, a multi-node cluster is desired and therefore worker nodes should be configured as well.</p> <p>If the control plane of the cluster contains multiple nodes, at least one load balancer must be configured. Such topology allows the cluster to continue operating normally if any control plane node fails. In addition, configuring multiple load balancers provides failover in case the primary load balancer fails.</p> <p>Kubitect currently supports only stacked control plane, which means that etcd key-value stores are deployed on control plane nodes. Since an etcd cluster requires a majority \"(n/2) + 1\" of nodes to agree to a change in the cluster, an odd number of nodes (1, 3, 5, ...) provides the best fault tolerance.  For example, in control planes with 3 nodes, 2 nodes represent the majority, giving a fault tolerance of 1 node.  In control planes with 4 nodes, the majority is 3 nodes, which provides the same fault tolerance. For this reason, Kubitect prevents deployment of the cluster whose control plane contains an even number of nodes.</p> <p>The nodes configuration structure is the following: <pre><code>cluster:\nnodes:\nmasters:\n...\nworkers:\n...\nloadBalancers:\n...\n</code></pre></p> <p>Each node type has two subsections, <code>default</code> and <code>instances</code>. Instances represent an array of actual nodes, while defaults provide the configuration that is applied to all instances of a certain node type. Each default value can also be overwritten by setting the same property for a specific instance.</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;:\ndefault:\n...\ninstances:\n...\n</code></pre>"},{"location":"user-guide/configuration/cluster-nodes/#configuration","title":"Configuration","text":""},{"location":"user-guide/configuration/cluster-nodes/#common-node-properties","title":"Common node properties","text":"<p>For each instance there is a set of predefined properties that can be set. Some properties apply for all node types, while some properties are specific for a certain node type. Properties that apply for all node types, are referred to as common properties.</p>"},{"location":"user-guide/configuration/cluster-nodes/#instance-id","title":"Instance ID","text":"<p> v2.3.0  Required</p> <p>Each instance in the cluster must have an ID that must be unique among all instances of the same node type. The instance ID is used as a suffix for the name of each node.</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;:\ninstances:\n- id: 1\n- id: compute-1\n- id: 77\n</code></pre>"},{"location":"user-guide/configuration/cluster-nodes/#cpu","title":"CPU","text":"<p> v2.0.0  Default: <code>2</code> vCPU</p> <p>The <code>cpu</code> property defines an amount of vCPU cores assigned to the virtual machine. It can be set for a specific instance or as a default value for all instances.</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;:\ndefault:\ncpu: 2\ninstances:\n- id: 1 # (1)!\n- id: 2\ncpu: 4 # (2)!\n</code></pre> <ol> <li> <p>Since the <code>cpu</code> property is not set for this instance, the default value is used (2).</p> </li> <li> <p>This instance has the <code>cpu</code> property set, and therefore the set value (4) overrides the default value (2).</p> </li> </ol> <p>If the property is not set at the instance level or as a default value, Kubitect uses its own default value (2 vCPU).</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;:\ninstances:\n- id: 1 # (1)!\n</code></pre> <ol> <li>Since the 'cpu' property is not set at instance level or as a default value, Kubitect sets the value of the 'cpu' property to 2 vCPU.</li> </ol>"},{"location":"user-guide/configuration/cluster-nodes/#ram","title":"RAM","text":"<p> v2.0.0  Default: <code>4</code> GiB</p> <p>The <code>ram</code> property defines an amount of RAM assigned to the virtual machine (in GiB). It can be set for a specific instance or as a default value for all instances.</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;:\ndefault:\nram: 8\ninstances:\n- id: 1 # (1)!\n- id: 2\nram: 16 # (2)!\n</code></pre> <ol> <li> <p>Since the <code>ram</code> property is not set for this instance, the default value is used (8 GiB).</p> </li> <li> <p>This instance has the <code>ram</code> property set, and therefore the set value (16 GiB) overrides the default value (8 GiB).</p> </li> </ol> <p>If the property is not set at the instance level or as a default value, Kubitect uses its own default value (4 GiB).</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;:\ninstances:\n- id: 1 # (1)!\n</code></pre> <ol> <li>Since the <code>ram</code> property is not set at instance level or as a default value, Kubitect sets the value of the <code>ram</code> property to 4 GiB.</li> </ol>"},{"location":"user-guide/configuration/cluster-nodes/#main-disk-size","title":"Main disk size","text":"<p> v2.0.0  Default: <code>32</code> GiB</p> <p>The <code>mainDiskSize</code> property defines an amount of space assigned to the virtual machine (in GiB). It can be set for a specific instance or as a default value for all instances.</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;:\ndefault:\nmainDiskSize: 128\ninstances:\n- id: 1 # (1)!\n- id: 2\nmainDiskSize: 256 # (2)!\n</code></pre> <ol> <li> <p>Since the <code>mainDiskSize</code> property is not set for this instance, the default value is used (128 GiB).</p> </li> <li> <p>This instance has the <code>mainDiskSize</code> property set, so therefore the set value (256 GiB) overrides the default value (128 GiB).</p> </li> </ol> <p>If the property is not set at the instance level or as a default value, Kubitect uses its own default value (32 GiB).</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;:\ninstances:\n- id: 1 # (1)!\n</code></pre> <ol> <li>Since the <code>mainDiskSize</code> property is not set at instance level or as a default value, Kubitect sets the value of the <code>mainDiskSize</code> property to 32 GiB.</li> </ol>"},{"location":"user-guide/configuration/cluster-nodes/#ip-address","title":"IP address","text":"<p> v2.0.0</p> <p>For each node a static IP address can be set. f no IP address is set for a particular node, a DHCP lease is requested. Kubitect also checks whether all set IP addresses are within the defined network range (see Network CIDR).</p> <pre><code>cluster:\nnetwork:\nmode: nat\ncidr: 192.168.113.0/24\nnodes:\n&lt;node-type&gt;:\ninstances:\n- id: 1\nip: 192.168.113.5 # (1)!\n- id: 2 # (2)!\n</code></pre> <ol> <li> <p>A static IP (<code>192.168.113.5</code>) is set for this instance.</p> </li> <li> <p>Since no IP address is defined for this instance, a DHCP lease is requested.</p> </li> </ol>"},{"location":"user-guide/configuration/cluster-nodes/#mac-address","title":"MAC address","text":"<p> v2.0.0</p> <p>By default, MAC addresses are generated for each virtual machine created, but a custom MAC address can also be set.</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;:\ninstances:\n- id: 1\nmac: \"52:54:00:00:13:10\" # (1)!\n- id: 2 # (2)!\n</code></pre> <ol> <li> <p>A custom MAC address (<code>52:54:00:00:13:10</code>) is set for this instance.</p> </li> <li> <p>Since no MAC address is defined for this instance, the MAC address is generated during cluster creation.</p> </li> </ol>"},{"location":"user-guide/configuration/cluster-nodes/#host-affinity","title":"Host affinity","text":"<p> v2.0.0</p> <p>By default, all instances are deployed on the default host. Kubitect can be instructed to deploy the instance on a specific host by specifying the name of the host in the instance configuration.</p> <pre><code>hosts:\n- name: host1\n...\n- name: host2\ndefault: true\n...\n\ncluster:\nnodes:\n&lt;node-type&gt;:\ninstances:\n- id: 1\nhost: host1 # (1)!\n- id: 2 # (2)!\n</code></pre> <ol> <li> <p>The instance is deployed on <code>host1</code>.</p> </li> <li> <p>Since no host is specified, the instance is deployed on the default host (<code>host2</code>).</p> </li> </ol>"},{"location":"user-guide/configuration/cluster-nodes/#control-plane-and-worker-node-properties","title":"Control plane and worker node properties","text":"<p>The following properties can be configured only for control plane or worker nodes.</p>"},{"location":"user-guide/configuration/cluster-nodes/#data-disks","title":"Data disks","text":"<p> v2.2.0</p> <p>By default, only a main disk (volume) is attached to each provisioned virtual machine. Since the main disk already contains an operating system, so it may not be suitable for storing data. Therefore, additional disks might be required. For example, a Rook can be easily configured to use all the empty disks attached to the virtual machine to form a storage cluster.</p> <p>A name and size (in GiB) must be configured for each data disk. By default, data disks are created in the main resource pool. To create a data disk in a custom data resource pool, the pool property can be set to the name of the desired data resource pool. Also note that the data disk name must be unique among all data disks for a given instance.</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;:\ninstances:\n- id: 1\ndataDisks:\n- name: data-volume\npool: main # (1)!\nsize: 256\n- name: rook-volume\npool: rook-pool # (2)!\nsize: 512\n</code></pre> <ol> <li> <p>When <code>pool</code> property is omitted or set to <code>main</code>, the data disk is created in the main resource pool.</p> </li> <li> <p>Custom data resource pool must be configured in the hosts section.</p> </li> </ol> <p>Note</p> <p>Default data disks are currently not supported.</p>"},{"location":"user-guide/configuration/cluster-nodes/#node-labels","title":"Node labels","text":"<p> v2.1.0</p> <p>Node labels are configured as a dictionary of key-value pairs. They are used to label actual Kubernetes nodes, and therefore can only be applied to control plane (master) and worker nodes.</p> <p>They can be set for a specific instance or as a default value for all instances. Labels set for a specific instance are merged with the default labels. However, labels set at the instance level take precedence over default labels.</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;: # (1)!\ndefault:\nlabels:\nlabel-key-1: def-label-value-1\nlabel-key-2: def-label-value-2\ninstances:\n- id: 1\nlabels: # (2)!\nlabel-key-3: instance-label-value-3\n- id: 2\nlabels: # (3)!\nlabel-key-1: new-label-value-1\n</code></pre> <ol> <li> <p>Node labels can only be applied to control plane (master) and worker nodes.</p> </li> <li> <p>Labels defined at the instance level are merged with default labels.     As a result, the following labels are applied to this instance:</p> <ul> <li><code>label-key-1: def-label-value-1</code></li> <li><code>label-key-2: def-label-value-2</code></li> <li><code>label-key-3: instance-label-value-3</code></li> </ul> </li> <li> <p>Labels defined at the instance level take precedence over default labels.     As a result, the following labels are applied to this instance:</p> <ul> <li><code>label-key-1: new-label-value-1</code></li> <li><code>label-key-2: def-label-value-2</code></li> </ul> </li> </ol>"},{"location":"user-guide/configuration/cluster-nodes/#node-taints","title":"Node taints","text":"<p> v2.2.0</p> <p>Node taints are configured as a list of strings in the format <code>key=value:effect</code>. Similar to node labels, taints can only be applied to control plane (master) and worker nodes.</p> <p>Taints can be set for a specific instance or as a default value for all instances. Taints set for a particular instance are merged with the default taints and duplicate entries are removed.</p> <pre><code>cluster:\nnodes:\n&lt;node-type&gt;: # (1)!\ndefault:\ntaints:\n- \"key1=value1:NoSchedule\"\ninstances:\n- id: 1\ntaints:\n- \"key2=value2:NoExecute\"\n</code></pre> <ol> <li>Node taints can only be applied to control plane (master) and worker nodes.</li> </ol>"},{"location":"user-guide/configuration/cluster-nodes/#load-balancer-properties","title":"Load balancer properties","text":"<p>The following properties can be configured only for load balancers.</p>"},{"location":"user-guide/configuration/cluster-nodes/#virtual-ip-address-vip","title":"Virtual IP address (VIP)","text":"<p> v2.0.0</p> <p>Load balancers distribute traffic directed to the control plane across all master nodes. Nevertheless, a load balancer can fail and make the control plane unreachable. To avoid such a situation, multiple load balancers can be configured. They work on the failover principle, i.e. one of them is primary and actively serves incoming traffic, while others are secondary and take over the primary position only if the primary load balancer fails. If one of the secondary load balancers becomes primary, it should still be reachable via the same IP. This IP is usually referred to as a virtual or floating IP (VIP).</p> <p>VIP must be specified if multiple load balancers are configured. It must also be an unused host IP address within the configured network.</p> <pre><code>cluster:\nnodes:\nloadBalancer:\nvip: 168.192.113.200\n</code></pre>"},{"location":"user-guide/configuration/cluster-nodes/#virtual-router-id","title":"Virtual router ID","text":"<p> v2.1.0  Default: <code>51</code></p> <p>When a cluster is created with a virtual IP (VIP) set, Kubitect configures the virtual router redundancy protocol (VRRP), which provides failover for load balancers. A virtual router ID (VRID) identifies the group of VRRP routers. Each group has its own ID. Since there can be only one master in each group, two groups cannot have the same ID.</p> <p>The virtual router ID can be any number between 0 and 255. By default, Kubitect sets the virtual router ID to <code>51</code>. If you set up multiple clusters that use VIP, you must ensure that the virtual router ID is different for each cluster.</p> <pre><code>cluster:\nnodes:\nloadBalancer:\nvip: 168.192.113.200\nvirtualRouterId: 30 # (1)!\n</code></pre> <ol> <li>If the virtual IP (VIP) is not set, the virtual router ID is ignored.</li> </ol>"},{"location":"user-guide/configuration/cluster-nodes/#priority","title":"Priority","text":"<p> v2.1.0  Default: <code>10</code></p> <p>Each load balancer has a priority that is used to select a primary load balancer. The one with the highest priority becomes the primary and all others become secondary. If the primary load balancer fails, the next one with the highest priority takes over. If two load balancers have the same priority, the one with the higher sum of IP address digits is selected.</p> <p>The priority can be any number between 0 and 255. The default priority is 10.</p> <pre><code>cluster:\nnodes:\nloadBalancer:\ninstances:\n- id: 1 # (1)!\n- id: 2 priority: 200 # (2)!\n</code></pre> <ol> <li> <p>Since the load balancer priority for this instance is not specified, it is set to 10.</p> </li> <li> <p>Since this load balancer instance has the highest priority (200 &gt; 10), it becomes the primary load balancer.</p> </li> </ol>"},{"location":"user-guide/configuration/cluster-nodes/#port-forwarding","title":"Port forwarding","text":"<p> v2.1.0</p> <p>By default, all configured load balancers distribute incoming traffic on port 6443 across all control plane nodes. Kubitect allows additional user-defined ports to be configured.</p> <p>The following properties can be configured for each port:</p> <ul> <li><code>name</code> - Name is a unique port identifier.</li> <li><code>port</code> - Incoming port is a port on which the load balancer listens for incoming traffic.</li> <li><code>targetPort</code> - Target port is a port where traffic is forwarded by the load balancer.</li> <li><code>target</code> - Target is a group of nodes to which traffic is forwarded. Possible targets are:<ul> <li><code>masters</code> - control plane nodes</li> <li><code>workers</code> - worker nodes </li> <li><code>all</code> - control plane and worker nodes.</li> </ul> </li> </ul> <p>A unique name and a unique incoming port must be configured for each port. The configuration of target and target port is optional. If target port is not configured, it is set to the same value as the incoming port. If target is not configured, incoming traffic is distributed across worker nodes by default.</p> <pre><code>cluster:\nnodes:\nloadBalancer:\nforwardPorts:\n- name: https\nport: 443 # (1)!\ntargetPort: 31200 # (2)!\ntarget: all # (3)!\n</code></pre> <ol> <li> <p>Incoming port is the port on which a load balancer listens for incoming traffic.     It can be any number between 1 and 65353, excluding ports 6443 (Kubernetes API server) and 22 (SSH).</p> </li> <li> <p>Target port is the port on which the traffic is forwarded.      By default, it is set to the same value as the incoming port.</p> </li> <li> <p>Target represents a group of nodes to which incoming traffic is forwarded.     Possible values are:</p> <ul> <li><code>masters</code></li> <li><code>workers</code></li> <li><code>all</code></li> </ul> <p>If the target is not configured, it defaults to the <code>workers</code>.</p> </li> </ol>"},{"location":"user-guide/configuration/cluster-nodes/#example-usage","title":"Example usage","text":""},{"location":"user-guide/configuration/cluster-nodes/#set-a-role-to-all-worker-nodes","title":"Set a role to all worker nodes","text":"<p>By default worker nodes have no roles (<code>&lt;none&gt;</code>). For example, to set <code>node</code> role to all worker nodes in the cluster, set default label with key <code>node-role.kubernetes.io/node</code>.</p> <pre><code>cluster:\nnodes:\nworker:\ndefault:\nlabels:\nnode-role.kubernetes.io/node: # (1)!\ninstances:\n...\n</code></pre> <ol> <li>If the label value is omitted, <code>null</code> is set as the label value.</li> </ol> <p>Node roles can be seen by listing cluster nodes with <code>kubectl</code>.</p> <pre><code>NAME                   STATUS   ROLES                  AGE   VERSION\nk8s-cluster-master-1   Ready    control-plane,master   19m   v1.22.6\nk8s-cluster-worker-1   Ready    node                   19m   v1.22.6\nk8s-cluster-worker-2   Ready    node                   19m   v1.22.6\n</code></pre>"},{"location":"user-guide/configuration/cluster-nodes/#load-balance-http-requests","title":"Load balance HTTP requests","text":"<p>Kubitect allows users to define custom port forwarding on load balancers. For example, to distribute HTTP and HTTPS requests across all worker nodes, at least one load balancer has to be specified and port forwarding must be configured, as shown in the sample configuration below.</p> <pre><code>cluster:\nnodes:\nloadBalancer:\nforwardPorts:\n- name: http\nport: 80\n- name: https\nport: 443\ntarget: all # (1)!\ninstances:\n- id: 1\n</code></pre> <ol> <li>When the target is set to <code>all</code>, load balancers distribute traffic across all nodes (master and worker nodes).</li> </ol>"},{"location":"user-guide/configuration/hosts/","title":"Hosts","text":"<p>Defining Kubitect hosts is esential.  Hosts represent the target servers where the cluster will be deployed. Every valid configuration must contain at least one host, but there can be as many hosts as needed. The host can be either a local or remote server. </p>"},{"location":"user-guide/configuration/hosts/#hosts-configuration","title":"Hosts configuration","text":""},{"location":"user-guide/configuration/hosts/#configuration","title":"Configuration","text":""},{"location":"user-guide/configuration/hosts/#localhost","title":"Localhost","text":"<p> v2.0.0</p> <p>When cluster is deployed on the server where the Kubitect command line tool is installed, a host whose connection type is set to local needs to be specified.  Such host is also refered to as localhost.</p> <pre><code>hosts:\n- name: localhost # (1)!\nconnection:\ntype: local\n</code></pre> <ol> <li>Custom unique name of the host.</li> </ol>"},{"location":"user-guide/configuration/hosts/#remote-hosts","title":"Remote hosts","text":"<p> v2.0.0</p> <p>When cluster is deployed on the remote host, the IP address of the remote host along with the SSH credentails needs to be specified for the host.</p> <pre><code>hosts:\n- name: my-remote-host\nconnection:\ntype: remote\nuser: myuser\nip: 10.10.40.143 # (1)!\nssh:\nkeyfile: \"~/.ssh/id_rsa_server1\" # (2)!\n</code></pre> <ol> <li> <p>IP address of the remote host.</p> </li> <li> <p>Path to the password-less SSH key file required for establishing connection with the remote host. Default is <code>~/.ssh/id_rsa</code>.</p> </li> </ol>"},{"location":"user-guide/configuration/hosts/#hosts-ssh-port","title":"Host's SSH port","text":"<p> v2.0.0  Default: <code>22</code></p> <p>By default, port <code>22</code> is used for SSH. If host is running SSH client on a different port, it is possible to change it for each host separately.</p> <pre><code>hosts:\n- name: remote-host\nconnection:\ntype: remote\nssh:\nport: 1234\n</code></pre>"},{"location":"user-guide/configuration/hosts/#host-verification-known-ssh-hosts","title":"Host verification (known SSH hosts)","text":"<p> v2.0.0  Default: <code>false</code></p> <p>By default remote hosts are not verified in the known SSH hosts. If for any reason host verification is desired, you can enable it for each host separately.</p> <pre><code>hosts:\n- name: remote-host\nconnection:\ntype: remote\nssh:\nverify: true\n</code></pre>"},{"location":"user-guide/configuration/hosts/#default-host","title":"Default host","text":"<p> v2.0.0</p> <p>If the host is specified as the default, all instances that do not point to a specific host are deployed to the default host.  If the default host is not specified, these instances are deployed on the first host in the list.</p> <pre><code>hosts:\n- name: localhost\nconnection:\ntype: local\n- name: default-host\ndefault: true\n...\n</code></pre>"},{"location":"user-guide/configuration/hosts/#main-resource-pool","title":"Main resource pool","text":"<p> v2.0.0  Default: <code>/var/lib/libvirt/images/</code></p> <p>The main resource pool path defines the location on the host where main disks (volumes) are created for each node provisioned on that particular host. Because the main resource pool contains volumes on which the node's operating system and all required packages are installed, it is recommended that the main resource pool is created on fast storage devices such as SSD disks. By default, main disk pool path is set to <code>/var/lib/libvirt/images/</code>.</p> <pre><code>hosts:\n- name: host1 # (1)!\n- name: host2 mainResourcePoolPath: /mnt/ssd/kubitect/ # (2)!\n</code></pre> <ol> <li> <p>Because the main resource pool path for this host is not set, the default path (<code>/var/lib/libvirt/images/</code>) is used.</p> </li> <li> <p>The main resource pool path is set for this host, so the node's main disks are created in this location.</p> </li> </ol>"},{"location":"user-guide/configuration/hosts/#data-resource-pools","title":"Data resource pools","text":"<p> v2.0.0</p> <p>Data resource pools define additional resource pools (besides the required main resource pool). For example, main disks contain the OS image and should be created on fast storage devices, while data resource pools can be used to attach additional virtual disks that can be created on slower storage devices such as HDDs.</p> <p>Multiple data resource pools can be defined on each host. Each configured pool must have a unique name on a particular host. The data resource pool name is used to associate the virtual disks defined in the node configuration with the actual data resource pool. The path of the data resources is set to <code>/var/lib/libvirt/images</code> by default, but can be easily configured with the <code>path</code> property.</p> <pre><code>hosts:\n- name: host1\ndataResourcePools:\n- name: rook-pool\npath: /mnt/hdd/kubitect/pools/\n- name: data-pool # (1)!\n</code></pre> <ol> <li>If the path of the resource pool is not specified, it is created under the path <code>/var/lib/libvirt/images/</code>.</li> </ol>"},{"location":"user-guide/configuration/hosts/#example-usage","title":"Example usage","text":""},{"location":"user-guide/configuration/hosts/#multiple-hosts","title":"Multiple hosts","text":"<p>With Kubitect the cluster can be deployed on multiple hosts. All hosts need to be specified in the configuration file.</p> <pre><code>hosts:\n- name: localhost\nconnection:\ntype: local\n- name: remote-host-1\nconnection:\ntype: remote\nuser: myuser\nip: 10.10.40.143\nssh:\nport: 123\nkeyfile: \"~/.ssh/id_rsa_server1\"\n- name: remote-host-2\ndefault: true\nconnection:\ntype: remote\nuser: myuser\nip: 10.10.40.145\nssh:\nkeyfile: \"~/.ssh/id_rsa_server2\"\n...\n</code></pre>"},{"location":"user-guide/configuration/kubernetes/","title":"Kubernetes","text":"<p>The Kubernetes section of the configuration file contains properties that are closely related to Kubernetes, such as Kubernetes version and network plugin.</p>"},{"location":"user-guide/configuration/kubernetes/#kubernetes-configuration","title":"Kubernetes configuration","text":""},{"location":"user-guide/configuration/kubernetes/#configuration","title":"Configuration","text":""},{"location":"user-guide/configuration/kubernetes/#kubernetes-version","title":"Kubernetes version","text":"<p> v3.0.0  Default: <code>v1.25.6</code></p> <p>Kubernetes version to be deployed.</p> <pre><code>kubernetes:\nversion: v1.24.7\n</code></pre>"},{"location":"user-guide/configuration/kubernetes/#kubernetes-network-plugin","title":"Kubernetes network plugin","text":"<p> v2.0.0  Default: <code>calico</code></p> <p>Kubitect supports multiple Kubernetes network plugins. Currently, the following network plugins are supported:</p> <ul> <li><code>calico</code></li> <li><code>canal</code></li> <li><code>cilium</code></li> <li><code>flannel</code></li> <li><code>kube-router</code></li> <li><code>weave</code></li> </ul> <p>If the network plugin is not set in the Kubitect configuration file, <code>calico</code> is used by default.</p> <pre><code>kubernetes:\nnetworkPlugin: calico\n</code></pre> <p>The following table shows the compatibility matrix of supported network plugins and Kubernetes versions:</p> Kubernetes Version Calico Canal Cilium Flannel KubeRouter Weave 1.23 1.24 1.25"},{"location":"user-guide/configuration/kubernetes/#kubernetes-dns-mode","title":"Kubernetes DNS mode","text":"<p> v2.0.0  Default: <code>coredns</code></p> <p>Currently, the only DNS mode supported by Kubitect is <code>coredns</code>. Therefore, it is safe to omit this property.</p> <pre><code>kubernetes:\ndnsMode: coredns\n</code></pre>"},{"location":"user-guide/configuration/kubernetes/#copy-kubeconfig","title":"Copy kubeconfig","text":"<p> v2.0.0  Default: <code>false</code></p> <p>Kubitect provides option to automatically copy the Kubeconfig file to <code>~/.kube/config</code> path.  By default, this option is disabled, as it can overwrite an existing file.</p> <pre><code>kubernetes:\nother:\ncopyKubeconfig: true\n</code></pre>"},{"location":"user-guide/configuration/kubernetes/#auto-renew-control-plane-certificates","title":"Auto renew control plane certificates","text":"<p> v2.2.0  Default: <code>false</code></p> <p>Control plane certificates are valid for 1 year and are renewed each time the cluster is upgraded. In some rare cases, this can cause clusters that are not upgraded frequently to stop working properly. Therefore, the control plane certificates can be renewed automatically on the first Monday of each month by setting the <code>autoRenewCertificates</code> property to true.</p> <pre><code>kubernetes:\nother:\nautoRenewCertificates: true\n</code></pre>"},{"location":"user-guide/management/creating/","title":"Creating the cluster","text":"<p>With Kubitect, clusters are created by applying the cluster configuration to the Kubitect CLI tool. If no cluster configuration is not specified, the default configuration is applied, as described in the Quick start guide.</p>"},{"location":"user-guide/management/creating/#creating-the-cluster","title":"Creating the cluster","text":""},{"location":"user-guide/management/creating/#applying-default-configuration","title":"Applying default configuration","text":"<p>To create a cluster with the default configuration, simply run the following command. <pre><code>kubitect apply\n</code></pre></p>"},{"location":"user-guide/management/creating/#applying-custom-configuration","title":"Applying custom configuration","text":"<p>To create a cluster with the custom configuration file, run the apply command with the <code>--config</code> flag. <pre><code>kubitect apply --flag &lt;PathToClusterConfig&gt;\n</code></pre></p>"},{"location":"user-guide/management/creating/#specifying-cluster-directory-name","title":"Specifying cluster directory name","text":"<p>The configuration files for each cluster created with Kubitect are generated under the path <code>~/.kubitect/clusters/&lt;ClusterName&gt;</code>. If no cluster name is specified, cluster name <code>default</code> is used.</p> <p>The name of the cluster can be specified with the flag <code>--cluster</code>. <pre><code>kubitect apply --cluster &lt;ClusterName&gt;\n</code></pre></p>"},{"location":"user-guide/management/destroying/","title":"Destroying the cluster","text":""},{"location":"user-guide/management/destroying/#destroying-the-cluster","title":"Destroying the cluster","text":""},{"location":"user-guide/management/destroying/#destroy-the-cluster","title":"Destroy the cluster","text":"<p>Important</p> <p>This action is irreversible and any data stored within the cluster will be lost. </p> <p>To destroy a specific cluster, simply run the destroy command, specifying the name of the cluster to be destroyed.</p> <pre><code>kubitect destroy --cluster my-cluster\n</code></pre> <p>Keep in mind that this action will permanently remove all resources associated with the cluster, including virtual machines, resource pools and configuration files. </p>"},{"location":"user-guide/management/scaling/","title":"Scaling the cluster","text":"<p>Any cluster created with Kubitect can be subsequently scaled. To do so, simply change the configuration and reapply it using the <code>scale</code> action.</p>"},{"location":"user-guide/management/scaling/#scaling-the-cluster","title":"Scaling the cluster","text":""},{"location":"user-guide/management/scaling/#export-the-cluster-configuration","title":"Export the cluster configuration","text":"<p>Exporting the current cluster configuration is optional, but strongly recommended to ensure that changes are made to the latest version of the configuration. The cluster configuration file can be exported using the <code>export</code> command.</p> <pre><code>kubitect export config --cluster my-cluster &gt; my-cluster.yaml\n</code></pre>"},{"location":"user-guide/management/scaling/#scale-the-cluster","title":"Scale the cluster","text":"<p>Warning</p> <p>Currently, only worker nodes can scale seamlessly.  The ultimate goal is to be able to scale every node type in the cluster with a single command. It is planned to address this issue in one of the following releases.</p> <p>Kubitect supports the simultaneous addition and removal of multiple worker nodes. In the configuration file, add new workers or remove/comment workers from the <code>cluster.nodes.worker.instances</code> list.</p> my-cluster.yaml<pre><code>cluster:\n...\nnodes:\n...\nworker:\ninstances:\n- id: 1\n#- id: 2 # Worker node to be removed\n- id: 3 # New worker node\n- id: 4 # New worker node\n</code></pre> <p>Apply the modified configuration with action set to <code>scale</code>: <pre><code>kubitect apply --config my-cluster.yaml --cluster my-cluster --action scale\n</code></pre></p> <p>As a result, the worker node with ID 2 is removed and the worker nodes with IDs 3 and 4 are added to the cluster.</p>"},{"location":"user-guide/management/upgrading/","title":"Upgrading the cluster","text":"<p>A running Kubernetes cluster can be upgraded to a higher version by increasing the Kubernetes version in the cluster's configuration file and reapplying it using the <code>upgrade</code> action.</p>"},{"location":"user-guide/management/upgrading/#upgrading-the-cluster","title":"Upgrading the cluster","text":""},{"location":"user-guide/management/upgrading/#export-the-cluster-configuration","title":"Export the cluster configuration","text":"<p>Exporting the current cluster configuration is optional, but strongly recommended to ensure that changes are made to the latest version of the configuration. The cluster configuration file can be exported using the <code>export</code> command.</p> <pre><code>kubitect export config --cluster my-cluster &gt; my-cluster.yaml\n</code></pre>"},{"location":"user-guide/management/upgrading/#upgrade-the-cluster","title":"Upgrade the cluster","text":"<p>Important</p> <p>Do not skip Kubitect's minor releases when upgrading the cluster.</p> <p>In the cluster configuration file, change the Kubernetes version.</p> <p>Example: cluster.yaml<pre><code>kubernetes:\nversion: v1.22.5 # Old value: v1.21.6\n...\n</code></pre></p> <p>Apply the modified configuration using <code>upgrade</code> action. <pre><code>kubitect apply --config cluster.yaml --action upgrade\n</code></pre></p> <p>The cluster is upgraded using the in-place strategy, i.e., the nodes are upgraded one after the other, making each node unavailable for the duration of its upgrade.</p>"},{"location":"user-guide/reference/cli/","title":"CLI tool reference","text":"<p>This document contains a reference of the Kubitect CLI tool. It documents each command along with its flags.</p> <p>Tip</p> <p>All available commands can be displayed by running <code>kubitect --help</code> or simply <code>kubitect -h</code>.</p> <p>To see the help for a particular command, run <code>kubitect command -h</code>.</p>"},{"location":"user-guide/reference/cli/#cli-reference","title":"CLI reference","text":""},{"location":"user-guide/reference/cli/#kubitect-commands","title":"Kubitect commands","text":""},{"location":"user-guide/reference/cli/#kubitect-apply","title":"kubitect apply","text":"<p>Apply the cluster configuration.</p> <p>Usage</p> <pre><code>kubitect apply [flags]\n</code></pre> <p>Flags</p> <ul> <li> <code>-a</code>, <code>--action &lt;string&gt;</code> \u2003     cluster action: create | scale | upgrade (default: create)   </li> <li> <code>--auto-approve</code> \u2003     automatically approve any user permission requests   </li> <li> <code>-c</code>, <code>--config &lt;string&gt;</code> \u2003     path to the cluster config file   </li> <li> <code>-l</code>, <code>--local</code> \u2003     use a current directory as the cluster path   </li> </ul>"},{"location":"user-guide/reference/cli/#kubitect-destroy","title":"kubitect destroy","text":"<p>Destroy the cluster with a given name. Executing the following command will permanently delete all resources associated with the cluster, including virtual machines and configuration files. </p> <p>Important</p> <p>Please be aware that this action is irreversible and any data stored within the cluster will be lost. </p> <p>Usage</p> <pre><code>kubitect destroy [flags]\n</code></pre> <p>Flags</p> <ul> <li> <code>--auto-approve</code> \u2003     automatically approve any user permission requests   </li> <li> <code>--cluster &lt;string&gt;</code> \u2003     name of the cluster to be used (default: default)   </li> </ul>"},{"location":"user-guide/reference/cli/#kubitect-export-config","title":"kubitect export config","text":"<p>Print cluster's configuration file to the standard output.</p> <p>Usage</p> <pre><code>kubitect export config [flags]\n</code></pre> <p>Flags</p> <ul> <li> <code>--cluster &lt;string&gt;</code> \u2003     name of the cluster to be used (default: default)   </li> </ul>"},{"location":"user-guide/reference/cli/#kubitect-export-kubeconfig","title":"kubitect export kubeconfig","text":"<p>Print cluster's kubeconfig to the standard output.</p> <p>Usage</p> <pre><code>kubitect export kubeconfig [flags]\n</code></pre> <p>Flags</p> <ul> <li> <code>--cluster &lt;string&gt;</code> \u2003     name of the cluster to be used (default: default)   </li> </ul>"},{"location":"user-guide/reference/cli/#kubitect-export-preset","title":"kubitect export preset","text":"<p>Print cluster configuration preset to the standard output.</p> <p>Usage</p> <pre><code>kubitect export preset [flags]\n</code></pre> <p>Flags</p> <ul> <li> <code>--name &lt;string&gt;</code> \u2003     preset name   </li> </ul>"},{"location":"user-guide/reference/cli/#kubitect-list-clusters","title":"kubitect list clusters","text":"<p>List clusters.</p> <p>Usage</p> <pre><code>kubitect list clusters\n</code></pre>"},{"location":"user-guide/reference/cli/#kubitect-list-presets","title":"kubitect list presets","text":"<p>List available cluster configuration presets.</p> <p>Usage</p> <pre><code>kubitect list presets\n</code></pre>"},{"location":"user-guide/reference/cli/#autogenerated-commands","title":"Autogenerated commands","text":""},{"location":"user-guide/reference/cli/#kubitect-completion","title":"kubitect completion","text":"<p>Generate the autocompletion script for Kubitect for the specified shell.</p> <p>Usage</p> <pre><code>kubitect completion [command]\n</code></pre> <p>Commands</p> <ul> <li> <code>bash</code> \u2003     Generate the autocompletion script for bash.   </li> <li> <code>fish</code> \u2003     Generate the autocompletion script for fish.   </li> <li> <code>zsh</code> \u2003     Generate the autocompletion script for zsh.   </li> </ul> <p>Tip</p> <p>Run <code>kubitect completion shell -h</code> for instructions how to add autocompletion for a specific shell.</p>"},{"location":"user-guide/reference/cli/#kubitect-help","title":"kubitect help","text":"<p>Help provides help for any command in the application. Simply type kubitect help [path to command] for full details.</p> <p>Usage</p> <pre><code>kubitect help [command]\n</code></pre> <p>or</p> <pre><code>kubitect [command] -h\n</code></pre>"},{"location":"user-guide/reference/cli/#other","title":"Other","text":""},{"location":"user-guide/reference/cli/#version-flag","title":"Version flag","text":"<p>Print Kubitect CLI tool version.</p> <p>Usage</p> <pre><code>kubitect --version\n</code></pre> <p>or</p> <pre><code>kubitect -v\n</code></pre>"},{"location":"user-guide/reference/cli/#debug-flag","title":"Debug flag","text":"<p>Enable debug messages. This can be especially handy with the <code>apply</code> command.</p> <p>Usage</p> <pre><code>kubitect [command] --debug\n</code></pre>"},{"location":"user-guide/reference/configuration/","title":"Configuration reference","text":"<p>This document contains a reference of the Kubitect configuration file and documents all possible configuration properties.</p> <p>The configuration sections are as follows:</p> <ul> <li><code>hosts</code> - A list of physical hosts (local or remote).</li> <li><code>cluster</code> - Configuration of the cluster infrastructure. Virtual machine properties, node types to install, and the host on which to install the nodes.</li> <li><code>kubernetes</code> - Kubernetes configuration.</li> <li><code>addons</code> - Configurable addons and applications.</li> </ul> <p>Each configuration property is documented with 5 columns: Property name, description, type, default value and is the property required.</p> <p>Note</p> <p><code>[*]</code> annotates an array.</p>"},{"location":"user-guide/reference/configuration/#configuration-reference","title":"Configuration reference","text":""},{"location":"user-guide/reference/configuration/#hosts-section","title":"Hosts section","text":"Name Type Default value Required? Description <code>hosts[*].connection.ip</code> string Yes, if <code>connection.type</code> is set to <code>remote</code> IP address is used to SSH into the remote machine. <code>hosts[*].connection.ssh.keyfile</code> string ~/.ssh/id_rsa Path to the keyfile that is used to SSH into the remote machine <code>hosts[*].connection.ssh.port</code> number 22 The port number of SSH protocol for remote machine. <code>hosts[*].connection.ssh.verify</code> boolean false          If true, the SSH host is verified, which means that the host must be present in the known SSH hosts.        <code>hosts[*].connection.type</code> string Yes Possible values are:         <ul> <li><code>local</code> or <code>localhost</code></li> <li><code>remote</code></li> </ul> <code>hosts[*].connection.user</code> string Yes, if <code>connection.type</code> is set to <code>remote</code> Username is used to SSH into the remote machine. <code>hosts[*].dataResourcePools[*].name</code> string          Name of the data resource pool. Must be unique within the same host.         It is used to link virtual machine volumes to the specific resource pool.        <code>hosts[*].dataResourcePools[*].path</code> string /var/lib/libvirt/images/ Host path to the location where data resource pool is created. <code>hosts[*].default</code> boolean false          Nodes where host is not specified will be installed on default host.          The first host in the list is used as a default host if none is marked as a default.        <code>hosts[*].name</code> string Yes Custom server name used to link nodes with physical hosts. <code>hosts[*].mainResourcePoolPath</code> string /var/lib/libvirt/images/ Path to the resource pool used for main virtual machine volumes."},{"location":"user-guide/reference/configuration/#cluster-section","title":"Cluster section","text":"Name Type Default value Required? Description <code>cluster.name</code> string Yes          Custom cluster name that is used as a prefix for various cluster components.          Note: cluster name cannot contain prefix <code>local</code>. <code>cluster.network.bridge</code> string virbr0          By default virbr0 is set as a name of virtual bridge.         In case network mode is set to bridge, name of the preconfigured bridge needs to be set here.        <code>cluster.network.cidr</code> string Yes Network cidr that contains network IP with network mask bits (IPv4/mask_bits). <code>cluster.network.gateway</code> string First client IP in network.          By default first client IP is taken as a gateway.         If network cidr is set to 10.0.0.0/24 then gateway would be 10.0.0.1.         Set gateway if it differs from default value.        <code>cluster.network.mode</code> string Yes          Network mode. Possible values are:         <ul> <li><code>nat</code> - Creates virtual local network.</li> <li><code>bridge</code> - Uses preconfigured bridge interface on the machine (Only bridge mode supports multiple hosts).</li> <li><code>route</code> - Creates virtual local network, but does not apply NAT.</li> </ul> <code>cluster.nodes.loadBalancer.default.cpu</code> number 2 Default number of vCPU allocated to a load balancer instance. <code>cluster.nodes.loadBalancer.default.mainDiskSize</code> number 32 Size of the main disk (in GiB) that is attached to a load balancer instance. <code>cluster.nodes.loadBalancer.default.ram</code> number 4 Default amount of RAM (in GiB) allocated to a load balancer instance. <code>cluster.nodes.loadBalancer.forwardPorts[*].name</code> string Yes, if port is configured Unique name of the forwarded port. <code>cluster.nodes.loadBalancer.forwardPorts[*].port</code> number Yes, if port is configured Incoming port is the port on which a load balancer listens for the incoming traffic. <code>cluster.nodes.loadBalancer.forwardPorts[*].targetPort</code> number Incoming port value Target port is the port on which a load balancer forwards traffic. <code>cluster.nodes.loadBalancer.forwardPorts[*].target</code> string workers          Target is a group of nodes on which a load balancer forwards traffic.         Possible targets are:         <ul> <li><code>masters</code></li> <li><code>workers</code></li> <li><code>all</code></li> </ul> <code>cluster.nodes.loadBalancer.instances[*].cpu</code> number Overrides a default value for that specific instance. <code>cluster.nodes.loadBalancer.instances[*].host</code> string          Name of the host on which the instance is deployed.          If the name is not specified, the instance is deployed on the default host.        <code>cluster.nodes.loadBalancer.instances[*].id</code> string Yes          Unique identifier of a load balancer instance.        <code>cluster.nodes.loadBalancer.instances[*].ip</code> string          If an IP is set for an instance then the instance will use it as a static IP.         Otherwise it will try to request an IP from a DHCP server.        <code>cluster.nodes.loadBalancer.instances[*].mac</code> string MAC used by the instance. If it is not set, it will be generated. <code>cluster.nodes.loadBalancer.instances[*].mainDiskSize</code> number Overrides a default value for that specific instance. <code>cluster.nodes.loadBalancer.instances[*].priority</code> number 10          Keepalived priority of the load balancer.         A load balancer with the highest priority becomes the leader (active).          The priority can be set to any number between 0 and 255.        <code>cluster.nodes.loadBalancer.instances[*].ram</code> number Overrides a default value for the RAM for that instance. <code>cluster.nodes.loadBalancer.vip</code> string Yes, if more then one instance of load balancer is specified.          Virtual IP (floating IP) is the static IP used by load balancers to provide a fail-over.         Each load balancer still has its own IP beside the shared one.        <code>cluster.nodes.loadBalancer.virtualRouterId</code> number 51          Virtual router ID identifies the group of VRRP routers.         It can be any number between 0 and 255 and should be unique among different clusters.        <code>cluster.nodes.master.default.cpu</code> number 2 Default number of vCPU allocated to a master node. <code>cluster.nodes.master.default.labels</code> dictionary          Array of default node labels that are applied to all master nodes.        <code>cluster.nodes.master.default.mainDiskSize</code> number 32 Size of the main disk (in GiB) that is attached to a master node. <code>cluster.nodes.master.default.ram</code> number 4 Default amount of RAM (in GiB) allocated to a master node. <code>cluster.nodes.master.default.taints</code> list          List of default node taints that are applied to all master nodes.        <code>cluster.nodes.master.instances[*].cpu</code> number Overrides a default value for that specific instance. <code>cluster.nodes.master.instances[*].dataDisks[*].name</code> string Name of the additional data disk that is attached to the master node. <code>cluster.nodes.master.instances[*].dataDisks[*].pool</code> string main          Name of the data resource pool where the additional data disk is created.         Referenced resource pool must be configure on the same host.        <code>cluster.nodes.master.instances[*].dataDisks[*].size</code> string          Size of the additional data disk (in GiB) that is attached to the master node.        <code>cluster.nodes.master.instances[*].host</code> string          Name of the host on which the instance is deployed.          If the name is not specified, the instance is deployed on the default host.        <code>cluster.nodes.master.instances[*].id</code> string Yes Unique identifier of a master node. <code>cluster.nodes.master.instances[*].ip</code> string          If an IP is set for an instance then the instance will use it as a static IP.         Otherwise it will try to request an IP from a DHCP server.        <code>cluster.nodes.master.instances[*].labels</code> dictionary          Array of node labels that are applied to this specific master node.        <code>cluster.nodes.master.instances[*].mac</code> string MAC used by the instance. If it is not set, it will be generated. <code>cluster.nodes.master.instances[*].mainDiskSize</code> number Overrides a default value for that specific instance. <code>cluster.nodes.master.instances[*].ram</code> number Overrides a default value for the RAM for that instance. <code>cluster.nodes.master.instances[*].taints</code> list          List of node taints that are applied to this specific master node.        <code>cluster.nodes.worker.default.cpu</code> number 2 Default number of vCPU allocated to a worker node. <code>cluster.nodes.worker.default.labels</code> dictionary Array of default node labels that are applied to all worker nodes. <code>cluster.nodes.worker.default.mainDiskSize</code> number 32 Size of the main disk (in GiB) that is attached to a worker node. <code>cluster.nodes.worker.default.ram</code> number 4 Default amount of RAM (in GiB) allocated to a worker node. <code>cluster.nodes.worker.default.taints</code> list          List of default node taints that are applied to all worker nodes.        <code>cluster.nodes.worker.instances[*].cpu</code> number Overrides a default value for that specific instance. <code>cluster.nodes.worker.instances[*].dataDisks[*].name</code> string Name of the additional data disk that is attached to the worker node. <code>cluster.nodes.worker.instances[*].dataDisks[*].pool</code> string main          Name of the data resource pool where the additional data disk is created.         Referenced resource pool must be configure on the same host.        <code>cluster.nodes.worker.instances[*].dataDisks[*].size</code> string          Size of the additional data disk (in GiB) that is attached to the worker node.        <code>cluster.nodes.worker.instances[*].host</code> string          Name of the host on which the instance is deployed.          If the name is not specified, the instance is deployed on the default host.        <code>cluster.nodes.worker.instances[*].id</code> string Yes Unique identifier of a worker node. <code>cluster.nodes.worker.instances[*].ip</code> string          If an IP is set for an instance then the instance will use it as a static IP.         Otherwise it will try to request an IP from a DHCP server.        <code>cluster.nodes.worker.instances[*].labels</code> dictionary          Array of node labels that are applied to this specific worker node.        <code>cluster.nodes.worker.instances[*].mac</code> string MAC used by the instance. If it is not set, it will be generated. <code>cluster.nodes.worker.instances[*].mainDiskSize</code> number Overrides a default value for that specific instance. <code>cluster.nodes.worker.instances[*].ram</code> number Overrides a default value for the RAM for that instance. <code>cluster.nodes.worker.instances[*].taints</code> list          List of node taints that are applied to this specific worker node.        <code>cluster.nodeTemplate.cpuMode</code> string custom          Guest virtual machine CPU mode.        <code>cluster.nodeTemplate.dns</code> list Value of <code>network.gateway</code>          Custom DNS list used by all created virtual machines.         If none is provided, network gateway is used.        <code>cluster.nodeTemplate.os.distro</code> string ubuntu          Set OS distribution. Possible values are:         <ul> <li><code>ubuntu</code></li> <li><code>debian</code></li> <li> <code>custom</code> - For all other distros             (for development only) </li> </ul> <code>cluster.nodeTemplate.os.networkInterface</code> string Depends on <code>os.distro</code>          Network interface used by virtual machines to connect to the network.         Network interface is preconfigured for each OS image (usually ens3 or eth0).         By default, the value from distro preset (/terraform/defaults.yaml) is set, but can be overwritten if needed.        <code>cluster.nodeTemplate.os.source</code> string Depends on <code>os.distro</code>          Source of an OS image.          It can be either path on a local file system or an URL of the image.         By default, the value from distro preset (/terraform/defaults.yaml)isset, but can be overwritten if needed.        <code>cluster.nodeTemplate.ssh.addToKnownHosts</code> boolean true          If set to true, each virtual machine will be added to the known hosts on the machine where the project is being run.         Note that all machines will also be removed from known hosts when destroying the cluster.        <code>cluster.nodeTemplate.ssh.privateKeyPath</code> string          Path to private key that is later used to SSH into each virtual machine.         On the same path with <code>.pub</code> prefix needs to be present public key.         If this value is not set, SSH key will be generated in <code>./config/.ssh/</code> directory.        <code>cluster.nodeTemplate.updateOnBoot</code> boolean true If set to true, the operating system will be updated when it boots. <code>cluster.nodeTemplate.user</code> string k8s User created on each virtual machine."},{"location":"user-guide/reference/configuration/#kubernetes-section","title":"Kubernetes section","text":"Name Type Default value Required? Description <code>kubernetes.dnsMode</code> string coredns          DNS server used within a Kubernetes cluster. Possible values are:          <ul> <li><code>coredns</code></li> </ul> <code>kubernetes.networkPlugin</code> string calico          Network plugin used within a Kubernetes cluster. Possible values are:          <ul> <li><code>flannel</code></li> <li><code>weave</code></li> <li><code>calico</code></li> <li><code>cilium</code></li> <li><code>canal</code></li> <li><code>kube-router</code></li> </ul> <code>kubernetes.other.autoRenewCertificates</code> boolean false          When this property is set to true, control plane certificates are renewed first Monday of each month.        <code>kubernetes.other.copyKubeconfig</code> boolean false          When this property is set to true, the kubeconfig of a new cluster is copied to the <code>~/.kube/config</code>.         Please note that setting this property to true may cause the existing file at the destination to be overwritten.        <code>kubernetes.version</code> string v1.25.6 Kubernetes version that will be installed."},{"location":"user-guide/reference/configuration/#addons-section","title":"Addons section","text":"Name Type Default value Required? Description <code>addons.kubespray</code> dictionary          Kubespray addons configuration.        <code>addons.rook.enabled</code> boolean false          Enable Rook addon.        <code>addons.rook.nodeSelector</code> dictionary          Dictionary containing node labels (\"key: value\").         Rook is deployed on the nodes that match all the given labels.        <code>addons.rook.version</code> string          Rook version.         By default, the latest release version is used."}]}