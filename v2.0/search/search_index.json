{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"examples/bridged-network/","title":"Bridged network","text":"<p>This example shows how to configure a simple bridge interface using netplan.</p>"},{"location":"examples/bridged-network/#step-1-preconfigure-the-bridge-on-the-host","title":"Step 1 - (Pre)configure the bridge on the host","text":"<p>In order to use the bridged network, bridge interface needs to be preconfigured on the host machine.</p> <p>Create the bridge interface (<code>br0</code> in our case) by creating a file with the following content: /etc/netplan/bridge0.yaml<pre><code>network:\nversion: 2\nrenderer: networkd\nethernets:\neth0: {} # (1)!\nbridges:\nbr0: # (2)!\ninterfaces:\n- eth0\ndhcp4: true\ndhcp6: false\naddresses: # (3)!\n- 10.10.0.17\n</code></pre></p> <ol> <li> <p>Existing ethernet interface to be enslaved.</p> </li> <li> <p>Custom name of the bridge interface.</p> </li> <li> <p>Optionally a static IP address can be set for the bridge interface.</p> </li> </ol> <p>Tip</p> <p>See the official netplan configuration examples for more complex configurations.</p> <p>Validate if the configuration is correctly parsed by netplan. <pre><code>sudo netplan generate\n</code></pre></p> <p>Apply the configuration. <pre><code>sudo netplan apply\n</code></pre></p>"},{"location":"examples/bridged-network/#step-2-disable-netfilter-on-the-host","title":"Step 2 - Disable netfilter on the host","text":"<p>The final step is to prevent packets traversing the bridge from being sent to iptables for processing. <pre><code> cat &gt;&gt; /etc/sysctl.conf &lt;&lt;EOF\n net.bridge.bridge-nf-call-ip6tables = 0\n net.bridge.bridge-nf-call-iptables = 0\n net.bridge.bridge-nf-call-arptables = 0\n EOF\n\nsysctl -p /etc/sysctl.conf\n</code></pre></p> <p>Tip</p> <p>For more information, see the libvirt documentation.</p>"},{"location":"examples/bridged-network/#step-3-set-up-a-cluster-over-bridged-network","title":"Step 3 - Set up a cluster over bridged network","text":"<p>In the cluster configuration file, set the following variables:</p> <ul> <li><code>cluster.network.mode</code> to <code>bridge</code>,</li> <li><code>cluster.network.bridge</code> to the name of the bridge you have created (<code>br0</code> in our case) and</li> <li><code>cluster.network.gateway</code> if the first host in <code>network_cidr</code> is not a gateway.</li> </ul> <pre><code>cluster:\nnetwork:\nmode: \"bridge\"\ncidr: \"10.10.13.0/24\"\ngateway: \"10.10.13.1\"\nbridge: \"br0\"\n...\n</code></pre>"},{"location":"examples/full-example/","title":"Full example","text":"<pre><code>#\n# In the 'kubitect' section, you can specify the target git project and version.\n# This can be handy if you want to use a specific project version or if you\n# want to point to your forked/cloned project.\n#\n# [!] Note that this is ignored if you use the --local option with the\n#     actions of the CLI tool, since in this case you should be in the \n#     Git repository.\n#\nkubitect:\nurl: \"https://github.com/MusicDin/kubitect\" # (1)!\nversion: \"v2.0.7\"\n\n#\n# The \"hosts\" section contains data about the physical servers on which \n# the Kubernetes cluster will be installed.\n#\n# For each host, a name and connection type must be specified. Only one\n# host can have the connection type set to 'local' or 'localhost'.\n#\n# If the host is a remote machine, SSH key file must be specified.\n# [!] Note that the connection to the remote hosts supports only \n#     passwordless login (using only SSH keyfile).  \n#\n# The host can also be marked as default, i.e. if no specific host is\n# specified for an instance (in the cluster.nodes section), it will be\n# installed on a default host. If none of the hosts is marked as default, \n# the first one in the list is used as default host.\n#\nhosts:\n- name: localhost # (3)!\ndefault: true # (4)!\nconnection:\ntype: local # (5)!\n- name: remote-server-1\nconnection:\ntype: remote\nuser: myuser # (6)!\nip: 10.10.40.143 # (7)!\nssh:\nport: 1234  # (8)!\nverify: false # (9)!\nkeyfile: \"~/.ssh/id_rsa_server1\" # (10)!\n- name: remote-server-2\nconnection:\ntype: remote\nuser: myuser\nip: 10.10.40.144\nssh:\nkeyfile: \"~/.ssh/id_rsa_server2\"\nmainResourcePoolPath: \"/var/lib/libvirt/pools/\" # (11)!\ndataResourcePools: # (12)!\n- name: data-pool\npath: \"/mnt/data/pool\"\n- name: backup-pool\npath: \"/mnt/backup/pool\"\n\n#\n# The \"cluster\" section of configuration contains general data about the cluster,\n# nodes that are part of the cluster and cluster's network.\n# \ncluster:\nname: \"my-k8s-cluster\" # (13)!\nnetwork:\nmode: bridge # (14)!\ncidr: \"10.10.64.0/24\" # (15)!\ngateway: 10.10.64.1 # (16)!\nbridge: br0 # (17)!\ndns: # (18)!\n- 1.1.1.1\n- 1.0.0.1\nnodeTemplate:\nnetworkInterface: \"ens3\" # (19)!\nuser: \"k8s\"\nssh:\nprivateKeyPath: \"~/.ssh/id_rsa_test\"\naddToKnownHosts: true\nimage:\ndistro: \"ubuntu\"\nsource: \"https://cloud-images.ubuntu.com/releases/focal/release-20220111/ubuntu-20.04-server-cloudimg-amd64.img\"\nupdateOnBoot: true\nnodes:\nloadBalancer:\nvip: \"10.10.64.200\" # (20)!\ndefault: # (21)!\nram: 4 # GiB\ncpu: 1 # vCPU\nmainDiskSize: 16 # GiB\ninstances:\n- id: 1\nip: 10.10.64.5 # (22)!\nmac: \"52:54:00:00:00:40\" # (23)!\nram: 8 # (24)!\ncpu: 8 # (25)!\nhost: remote-server-1 # (26)!\n- id: 2\nip: 10.10.64.6\nmac: \"52:54:00:00:00:41\"\nhost: remote-server-2\n- id: 3\nip: 10.10.64.7\nmac: \"52:54:00:00:00:42\"\n# If host is not specifed, VM will be installed on the default host.\n# If default host is not specified, VM will be installed on the first\n# host in the list.\nmaster:\ndefault:\nram: 8\ncpu: 2\nmainDiskSize: 256\ninstances:\n# IMPORTANT: There should be odd number of master nodes.\n- id: 1 # Node with generated MAC address, IP retrieved as an DHCP lease and default RAM and CPU.\nhost: remote-server-1\n- id: 2\nhost: remote-server-2\n- id: 3\nhost: localhost\nworker:\ndefault:\nram: 16\ncpu: 4\nlabel: node # (27)!\ninstances:\n- id: 1\nip: 10.10.64.101\ncpu: 8\nram: 64\nhost: remote-server-1\n- id: 2\nip: 10.10.64.102\ndataDisks: # (33)!\n- name: rook-disk\npool: data-pool\nsize: 128\n- name: test-disk\npool: data-pool\nsize: 128\n- id: 3\nip: 10.10.64.103\nram: 64\n- id: 4\nhost: remote-server-2\n- id: 5\n\n#\n# The \"kubernetes\" section specifies which version of Kubernetes and\n# Kubespray to use and which network plugin and DNS server to install.\n#\nkubernetes:\nversion: \"v1.22.6\"\nnetworkPlugin: calico\ndnsMode: coredns\nkubespray:\nurl: \"https://github.com/kubernetes-sigs/kubespray.git\"\nversion: \"v2.18.1\"\nother:\ncopyKubeconfig: false\n</code></pre> <ol> <li> <p>This allows you to set a custom URL that targets clone/fork of Kubitect project.</p> </li> <li> <p>Kubitect version.</p> </li> <li> <p>Custom host name.      It is used to link instances to the specific host.</p> </li> <li> <p>Makes the host a default host.      This means that if no host is specified for the node instance, the instance will be linked to the default host.</p> </li> <li> <p>Connection type can be either <code>local</code> or <code>remote</code>. </p> <p>If it is set to remote, at least the following fields must be set:</p> <ul> <li><code>user</code></li> <li><code>ip</code></li> <li><code>ssh.keyfile</code></li> </ul> </li> <li> <p>Remote host user that is used to connect to the remote hypervisor.      This user must be added in the <code>libvirt</code> group.</p> </li> <li> <p>IP address of the remote host.</p> </li> <li> <p>Overrides default SSH port (22).</p> </li> <li> <p>If set to false, host verification is skipped.</p> </li> <li> <p>Path to the passwordless SSH key used to connect to the remote host.</p> </li> <li> <p>The path to the main resource pool defines where the virtual machine disk images are stored. These disks contain the virtual machine operating system, and therefore it is recommended to install them on SSD disks.</p> </li> <li> <p>List of other storage pools where virtual disks can be created.</p> </li> <li> <p>Cluster name used as a prefix for the various components.</p> </li> <li> <p>Network mode. Possible values are</p> <ul> <li><code>bridge</code> mode uses predefined bridge interface. This mode is mandatory for deployments across multiple hosts.</li> <li><code>nat</code> mode creates virtual network with IP range defined in <code>network.cidr</code></li> <li><code>route</code></li> </ul> </li> <li> <p>Network CIDR represents the network IP together with the network mask.      In <code>nat</code> mode, CIDR is used for the new network.     In <code>bridge</code> mode, CIDR represents the current local area network (LAN).</p> </li> <li> <p>The network gateway IP address.     If omitted the first client IP from network CIDR is used as a gateway.</p> </li> <li> <p>Bridge represents the bridge interface on the hosts.     This field is mandatory if the network mode is set to <code>bridge</code>.     If the network mode is set to <code>nat</code>, this field can be omitted.</p> </li> <li> <p>Set custom DNS for nodes.      If omitted, gateway is also used as DNS.</p> </li> <li> <p>Specify the network interface used by the virtual machine. In general, this option can be omitted. </p> <p>If you omit it, <code>ens3</code> is used for Ubuntu images and <code>eth0</code> for all other distributions.</p> </li> <li> <p>Virtual (floating) IP shared between load balancers. </p> </li> <li> <p>Default values apply for all virtual machines (VMs) of the same type.</p> </li> <li> <p>Static IP address of the virtual machine.      If omitted DHCP lease is requested.</p> </li> <li> <p>Static MAC address.      If omitted MAC address is generated.</p> </li> <li> <p>Overrides default RAM value for this node.</p> </li> <li> <p>Overrides default CPU value for this node.</p> </li> <li> <p>Name of the host where instance should be created.     If omitted the default host is used.</p> </li> <li> <p>Worker nodes label.</p> </li> <li> <p>Overrides default data disks for this node.</p> </li> <li> <p>Default data disks (attached to each worker node).</p> </li> <li> <p>Unique data disk name.</p> </li> <li> <p>Reference to the data resource pool that must exist on the same host as this node.</p> </li> <li> <p>Size of the data disk in GiB.      Note that each node receives a data disk of a specific size.</p> </li> <li> <p>Overrides default data disks for this node.</p> </li> </ol>"},{"location":"examples/single-node-cluster/","title":"Single node cluster","text":"<p>If you want to initialize a cluster with only one node, specify single master node in cluster configuration file:</p> single-node.yaml<pre><code>cluster:\n...\nnodes:\nmaster:\ninstances:\n- id: 1\nip: 10.10.64.5 # (1)!\nmac: \"52:54:00:00:00:40\" # (2)!\n</code></pre> <ol> <li>Static IP address. If omitted, the DHCP lease is requested.</li> <li>Static MAC address. If omitted, the MAC address is generated.</li> </ol> <p>Do not forget to remove (or comment out) the worker and load balancer nodes.</p> <p>Apply the cluster: <pre><code>kubitect apply --config single-node.yaml\n</code></pre></p> <p>Your master node now also becomes a worker node.</p> <p>Note</p> <p>If you do not specify worker nodes, all master nodes also become worker nodes.</p>"},{"location":"reference/reference/","title":"Reference","text":"<p>The cluster configuration consists of four parts:</p> <ul> <li><code>kubitect</code> - project metadata.</li> <li><code>hosts</code> - a list of physical hosts (local or remote).</li> <li><code>cluster</code> - cluster infrastructure configuration. Virtual machine properties, node types to install, and the host on which to install the nodes.</li> <li><code>kubernetes</code> - Kubernetes and Kubespray configuration. Versions, addons and other Kubernetes related settings.</li> </ul> <p>Note</p> <p><code>[*]</code> annotates an array.</p>"},{"location":"reference/reference/#kubitect-section","title":"Kubitect section","text":"Name Type Default value Required? Description <code>kubitect.url</code> string https://github.com/MusicDin/kubitect No URL of the project's git repository. <code>kubitect.version</code> string master No Version of the git repository. Can be a branch or a tag."},{"location":"reference/reference/#hosts-section","title":"Hosts section","text":"Name Type Default value Required? Description <code>hosts[*].name</code> string Yes Custom server name used to link nodes with physical hosts. <code>hosts[*].default</code> string false          Nodes where host is not specified will be installed on default host.          The first host in the list is used as a default host if none is marked as a default.        <code>hosts[*].connection.type</code> string Yes Possible values are:         <ul> <li><code>local</code> or <code>localhost</code></li> <li><code>remote</code></li> </ul> <code>hosts[*].connection.user</code> string Yes, if <code>connection.type</code> is set to <code>remote</code> Username is used to SSH into the remote machine. <code>hosts[*].connection.ip</code> string Yes, if <code>connection.type</code> is set to <code>remote</code> IP address is used to SSH into the remote machine. <code>hosts[*].connection.ssh.port</code> number 22 The port number of SSH protocol for remote machine. <code>hosts[*].connection.ssh.keyfile</code> string ~/.ssh/id_rsa Path to the keyfile that is used to SSH into the remote machine <code>hosts[*].connection.ssh.verify</code> boolean true If set to true, SSH host is verified. <code>hosts[*].mainResourcePoolPath</code> string /var/lib/libvirt/pools/ Path to the resource pool used for main virtual machine volumes. <code>hosts[*].dataResourcePools[*].name</code> string          Name of the data resource pool. Must be unique within the same host.         It is used to link virtual machine volumes to the specific resource pool.        <code>hosts[*].dataResourcePools[*].path</code> string Host path to the location where data resource pool is created."},{"location":"reference/reference/#cluster-section","title":"Cluster section","text":"Name Type Default value Required? Description <code>cluster.name</code> string Yes Custom cluster name that is used as a prefix for various cluster components. <code>cluster.network.mode</code> string nat Yes          Network mode. Possible values are:         <ul> <li><code>nat</code> - Creates virtual local network.</li> <li><code>bridge</code> - Uses preconfigured bridge interface on the machine (Only bridge mode supports multiple hosts).</li> <li><code>route</code> - Creates virtual local network, but does not apply NAT.</li> </ul> <code>cluster.network.cidr</code> string Yes Network cidr that contains network IP with network mask bits (IPv4/mask_bits). <code>cluster.network.gateway</code> string First client IP in network.          By default first client IP is taken as a gateway.         If network cidr is set to 10.0.0.0/24 then gateway would be 10.0.0.1.         Set gateway if it differs from default value.        <code>cluster.network.bridge</code> string virbr0          By default virbr0 is set as a name of virtual bridge.         In case network mode is set to bridge, name of the preconfigured bridge needs to be set here.        <code>cluster.network.dns</code> list [Network gateway]          DNS used by all created virtual machines.         If none is provided, gateway is used as a DNS server.        <code>cluster.nodeTemplate.images.networkInterface</code> string ens3, if <code>distro</code> is set to ubuntu, otherwise eth0          Network interface used by virtual machines to connect to the network.         Usually for Ubuntu images is ens3 and for most other distros is eth0.         <code>cluster.nodeTemplate.user</code> string user User created on each virtual machine. <code>cluster.nodeTemplate.ssh.privateKeyPath</code> string          Path to private key that is later used to SSH into each virtual machine.         On the same path with <code>.pub</code> prefix needs to be present public key.         If this value is not set, SSH key will be generated in <code>./config/.ssh/</code> directory.        <code>cluster.nodeTemplate.ssh.addToKnownHosts</code> boolean true          If set to true, each virtual machine will be added to the known hosts on the machine where the project is being run.         Note that all machines will also be removed from known hosts when destroying the cluster.        <code>cluster.nodeTemplate.image.distro</code> string N/A          Set OS image distribution. Possible values are:         <ul> <li><code>ubuntu</code></li> <li><code>debian</code></li> <li><code>centos</code></li> <li><code>N/A</code> - For all other distros</li> </ul> <code>cluster.nodeTemplate.image.source</code> string Yes          Source of an OS image.          It can be either path on a local file system or a URL to an image.        <code>cluster.nodeTemplate.updateOnBoot</code> boolean true If set to true, the operating system will be updated when it boots. <code>cluster.nodes.loadBalancer.vip</code> string Yes, if more then one instance of load balancer is specified.          Virtual IP (floating IP) is the static IP used by load balancers to provide a fail-over.         Each load balancer still has its own IP beside the shared one.        <code>cluster.nodes.loadBalancer.default.ram</code> number 4 Yes Default amount of RAM (in GiB) allocated to a load balancer instance. <code>cluster.nodes.loadBalancer.default.cpu</code> number 1 Default number of vCPU allocated to a load balancer instance. <code>cluster.nodes.loadBalancer.default.mainDiskSize</code> number 16 Size of the main disk (in GiB) that is attached to a load balancer instance. <code>cluster.nodes.loadBalancer.instances[*].id</code> number Yes          Unique numeric identifier of a load balancer instance.         It has to be a number between 0 and 200, because the priority of a load balancer is calculated out of its ID.        <code>cluster.nodes.loadBalancer.instances[*].ip</code> string          If an IP is set for an instance then the instance will use it as a static IP.         Otherwise it will try to request an IP from a DHCP server.        <code>cluster.nodes.loadBalancer.instances[*].mac</code> string MAC used by the instance. If it is not set, it will be generated. <code>cluster.nodes.loadBalancer.instances[*].ram</code> number Overrides a default value for the RAM for that instance. <code>cluster.nodes.loadBalancer.instances[*].cpu</code> number Overrides a default value for that specific instance. <code>cluster.nodes.loadBalancer.instances[*].mainDiskSize</code> number Overrides a default value for that specific instance. <code>cluster.nodes.master.default.ram</code> number 4 Default amount of RAM (in GiB) allocated to a master node. <code>cluster.nodes.master.default.cpu</code> number 1 Default number of vCPU allocated to a master node. <code>cluster.nodes.master.default.mainDiskSize</code> number 16 Size of the main disk (in GiB) that is attached to a master node. <code>cluster.nodes.master.instances[*].id</code> number Yes Unique numeric identifier of a master node. <code>cluster.nodes.master.instances[*].ip</code> string          If an IP is set for an instance then the instance will use it as a static IP.         Otherwise it will try to request an IP from a DHCP server.        <code>cluster.nodes.master.instances[*].mac</code> string MAC used by the instance. If it is not set, it will be generated. <code>cluster.nodes.master.instances[*].ram</code> number Overrides a default value for the RAM for that instance. <code>cluster.nodes.master.instances[*].cpu</code> number Overrides a default value for that specific instance. <code>cluster.nodes.master.instances[*].mainDiskSize</code> number Overrides a default value for that specific instance. <code>cluster.nodes.master.instances[*].dataDisks[*].name</code> string Name of the additional data disk that is attached to the master node. <code>cluster.nodes.master.instances[*].dataDisks[*].pool</code> string          Name of the data resource pool where the additional data disk is created.          Referenced resource pool must be specified on the same host.        <code>cluster.nodes.master.instances[*].dataDisks[*].size</code> string          Size of the additional data disk (in GiB) that is attached to the master node.        <code>cluster.nodes.worker.default.ram</code> number 8 Default amount of RAM (in GiB) allocated to a worker node. <code>cluster.nodes.worker.default.cpu</code> number 2 Default number of vCPU allocated to a worker node. <code>cluster.nodes.worker.default.mainDiskSize</code> number 32 Size of the main disk (in GiB) that is attached to a worker node. <code>cluster.nodes.worker.default.label</code> string Default label for worker nodes. <code>cluster.nodes.worker.instances[*].id</code> number Yes Unique numeric identifier of a worker node. <code>cluster.nodes.worker.instances[*].ip</code> string          If an IP is set for an instance then the instance will use it as a static IP.         Otherwise it will try to request an IP from a DHCP server.        <code>cluster.nodes.worker.instances[*].mac</code> string MAC used by the instance. If it is not set, it will be generated. <code>cluster.nodes.worker.instances[*].ram</code> number Overrides a default value for the RAM for that instance. <code>cluster.nodes.worker.instances[*].cpu</code> number Overrides a default value for that specific instance. <code>cluster.nodes.worker.instances[*].mainDiskSize</code> number Overrides a default value for that specific instance. <code>cluster.nodes.worker.instances[*].dataDisks[*].name</code> string Name of the additional data disk that is attached to the worker node. <code>cluster.nodes.worker.instances[*].dataDisks[*].pool</code> string          Name of the data resource pool where the additional data disk is created.          Referenced resource pool must be specified on the same host.        <code>cluster.nodes.worker.instances[*].dataDisks[*].size</code> string          Size of the additional data disk (in GiB) that is attached to the worker node."},{"location":"reference/reference/#kubernetes-section","title":"Kubernetes section","text":"Name Type Default value Required? Description <code>kubernetes.version</code> string Yes Kubernetes version that will be installed. <code>kubernetes.networkPlugin</code> string calico          Network plugin used within a Kubernetes cluster. Possible values are:          <ul> <li><code>flannel</code></li> <li><code>weave</code></li> <li><code>calico</code></li> <li><code>cilium</code></li> <li><code>canal</code></li> <li><code>kube-router</code></li> </ul> <code>kubernetes.dnsMode</code> string coredns          DNS server used within a Kubernetes cluster. Possible values are:          <ul> <li><code>coredns</code></li> <li><code>kubedns</code></li> </ul> <code>kubernetes.kubespray.url</code> string https://github.com/kubernetes-sigs/kubespray.git          URL to the Kubespray project.         For example, it can be changed so that it targets your fork of a project.        <code>kubernetes.kubespray.version</code> string Yes Kubespray version. Version is used to checkout into appropriate branch. <code>kubernetes.kubespray.other.copyKubeconfig</code> boolean false          If set to true, Kubeconfig of a new cluster will be copied to the `~/.kube/admin.conf`.          Please note that setting this to true can result in overwriting file on target location."},{"location":"user-guide/cluster-management/","title":"Cluster management","text":"<p>Project currently supports the following actions that can be executed on the running Kubernetes cluster:</p> <ul> <li>scaling the cluster<ul> <li>adding worker nodes,</li> <li>removing worker nodes,</li> </ul> </li> <li>upgrading the cluster,</li> <li>destroying the cluster.</li> </ul> <p>Note</p> <p>Each action supports the <code>--cluster &lt;cluster_name&gt;</code> option, which allows you to execute the action on a specific cluster.  By default, all actions are executed on the <code>default</code> cluster, which corresponds to using the <code>--cluster default</code> option.</p>"},{"location":"user-guide/cluster-management/#export-cluster-configuration-file","title":"Export cluster configuration file","text":"<p>Each action requires the cluster configuration file to be modified. Cluster configuration file can be exported using <code>export</code> command of the <code>kubitect</code> tool.</p> <pre><code>kubitect export config &gt; cluster.yaml\n</code></pre>"},{"location":"user-guide/cluster-management/#scale-the-cluster","title":"Scale the cluster","text":""},{"location":"user-guide/cluster-management/#add-worker-nodes-to-the-cluster","title":"Add worker nodes to the cluster","text":"<p>In the configuration file add new worker nodes to <code>cluster.nodes.worker.instances</code> list. cluster.yaml<pre><code>cluster:\n...\nnodes:\n...\nworker:\ninstances:\n- id: 1\n- id: 2 # New worker node\n- id: 3 # New worker node\n</code></pre></p> <p>Apply the modified configuration using <code>kubitect</code> tool to add new worker nodes: <pre><code>kubitect apply --config cluster.yaml --action scale\n</code></pre></p>"},{"location":"user-guide/cluster-management/#remove-worker-nodes-from-the-cluster","title":"Remove worker nodes from the cluster","text":"<p>In the configuration file remove worker nodes from <code>cluster.nodes.worker.instances</code> list. cluster.yaml<pre><code>cluster:\n...\nnodes:\n...\nworker:\ninstances:\n- id: 1\n#- id: 2\n#- id: 3\n</code></pre></p> <p>Apply the modified configuration using <code>kubitect</code> tool to remove worker nodes: <pre><code>kubitect apply --config cluster.yaml --action scale\n</code></pre></p>"},{"location":"user-guide/cluster-management/#upgrade-the-cluster","title":"Upgrade the cluster","text":"<p>Important</p> <p>Do not skip releases when upgrading--upgrade by one tag at a time.</p> <p>For more information read Kubespray upgrades.</p> <p>In the cluster configuration file set the following variables:   + <code>kubernetes.version</code> and   + <code>kubernetes.kubespray.version</code>.</p> <p>Note</p> <p>Before upgrading the cluster, make sure that Kubespray supports a specific Kubernetes version.</p> <p>Example: cluster.yaml<pre><code>kubernetes:\nversion: \"v1.22.5\" # Old value: \"v1.21.6\"\n...\nkubespray:\nversion: \"v2.18.0\" # Old value: \"v2.17.1\"\n...\n</code></pre></p> <p>Apply the modified configuration using <code>kubitect</code> tool: <pre><code>kubitect apply --config cluster.yaml --action upgrade\n</code></pre></p>"},{"location":"user-guide/cluster-management/#destroy-the-cluster","title":"Destroy the cluster","text":"<p>To destroy the cluster, simply run: <pre><code>kubitect destroy\n</code></pre></p>"},{"location":"user-guide/getting-started/","title":"Getting started","text":"<p>In this step-by-step guide, you will learn how to prepare a custom cluster configuration file and use it to create a functional Kubernetes cluster consisting of a single master node and three worker nodes.</p> <p>Note</p> <p>See reference documentation for explanations of each possible configuration property.</p>"},{"location":"user-guide/getting-started/#step-1-make-sure-all-requirements-are-satisfied","title":"Step 1 - Make sure all requirements are satisfied","text":"<p>For the successful installation of the Kubernetes cluster, some requirements must be met.</p>"},{"location":"user-guide/getting-started/#step-2-create-cluster-configuration-file","title":"Step 2 - Create cluster configuration file","text":"<p>In the quick start you have created a very basic Kubernetes cluster from predefined cluster configuration file. If configuration is not explicitly provided to the command-line tool using <code>--config</code> option, default cluster configuration file is used (/examples/default-cluster.yaml).</p> <p>Now it's time to create your own cluster topology.</p> <p>Before you begin create a new yaml file. <pre><code>touch kubitect.yaml\n</code></pre></p>"},{"location":"user-guide/getting-started/#step-3-prepare-hosts-configuration","title":"Step 3 - Prepare hosts configuration","text":"<p>In the cluster configuration file, we will first define hosts. Hosts represent target servers. A host can be either a local or a remote machine.</p> LocalhostRemote host <p>If the cluster is set up on the same machine where the command line tool is installed, we specify a host whose connection type is set to <code>local</code>. <pre><code>hosts:\n- name: localhost # Can be anything\nconnection:\ntype: local\n</code></pre></p> <p>When cluster is deployed on the remote machine, the IP address of the remote machine along with the SSH credentails needs to be specified for the host. <pre><code>hosts:\n- name: my-remote-host\nconnection:\ntype: remote\nuser: myuser\nip: 10.10.40.143 # IP address of the remote host\nssh:\nkeyfile: \"~/.ssh/id_rsa_server1\" # Password-less SSH key file\n</code></pre></p> <p>In this tutorial we will use only localhost.</p>"},{"location":"user-guide/getting-started/#step-4-define-cluster-infrastructure","title":"Step 4 - Define cluster infrastructure","text":"<p>The second part of the configuration file consists of the cluster infrastructure. In this part, all virtual machines are defined along with their properties such as operating system, CPU cores, amount of RAM and so on.</p> <p>Let's take a look at the following configuration: <pre><code>cluster:\nname: \"my-k8s-cluster\"\nnetwork:\n...\nnodeTemplate:\n...\nnodes:\n...\n</code></pre></p> <p>We can see that the infrastructure configuration consists of the cluster name and 3 subsections:</p> <ul> <li><code>cluster.name</code> is a cluster name that is used as a prefix for each resource created by kubitect.</li> <li><code>cluster.network</code> holds information about the network properties of the cluster.</li> <li><code>cluster.nodeTemplate</code> contains properties that apply to all our nodes. For example, properties like operating system, SSH user, and SSH private key are the same for all our nodes.</li> <li><code>cluster.nodes</code> subsection defines each node in our cluster.</li> </ul> <p>Now that we have a general idea about the infrastructure configuration, we can look at each of these subsections in more detail.</p>"},{"location":"user-guide/getting-started/#step-41-cluster-network","title":"Step 4.1 - Cluster network","text":"<p>The cluster network subsection defines the network that our cluster will use. Currently, two network modes are supported - NAT and bridge.</p> <p>The <code>nat</code> network mode instructs kubitect to create a virtual network that does network address translation. This mode allows us to use IP address ranges that do not exist within our local area network (LAN).</p> <p>The <code>bridge</code> network mode instructs kubitect to use a predefined bridge interface. In this mode, virtual machines can connect directly to LAN. Using this mode is mandatory when you set up a cluster that spreads over multipe hosts.</p> <p>To keep this tutorial as simple as possible, we will use the NAT mode, as it does not require a preconfigured bridge interface.</p> <pre><code>cluster:\n...\nnetwork:\nmode: \"nat\"\ncidr: \"192.168.113.0/24\"\n</code></pre> <p>The above configuration will instruct kubitect to create a virtual network that uses <code>192.168.113.0/24</code> IP range.</p>"},{"location":"user-guide/getting-started/#step-42-node-template","title":"Step 4.2 - Node template","text":"<p>As mentioned earlier, the <code>nodeTemplate</code> subsection is used to define general properties of our nodes.</p> <p>Required properties are:</p> <ul> <li><code>user</code> is the name of the user that will be created on all virtual machines and will also be used for SSH.</li> <li><code>image.distro</code> defines the type of the used operating system (ubuntu, debian, ...).</li> <li><code>image.source</code> defines the location of the OS image. It can be either a local file system path or an URL.</li> </ul> <p>Besides the required properties, there are two potentially useful properties:</p> <ul> <li><code>ssh.addToKnownHosts</code> - if set to true, all virtual machines will be added to SSH known hosts. If you later destroy the cluster, these virtual machines will also be removed from the known hosts.</li> <li><code>updateOnBoot</code> - if set to true, all virtual machines are updated at the first boot.</li> </ul> <p>Our <code>noteTemplate</code> subsection now looks like this: <pre><code>cluster:\n...\nnodeTemplate:\nuser: \"k8s\"\nssh:\naddToKnownHosts: true\nimage:\ndistro: \"ubuntu\"\nsource: \"https://cloud-images.ubuntu.com/releases/focal/release-20220111/ubuntu-20.04-server-cloudimg-amd64.img\"\nupdateOnBoot: true\n</code></pre></p>"},{"location":"user-guide/getting-started/#step-43-cluster-nodes","title":"Step 4.3 - Cluster nodes","text":"<p>In the <code>nodes</code> subsection, we can define three types of nodes:</p> <ul> <li><code>loadBalancer</code> nodes are internal load balancers used to expose the Kubernetes control plane at a single endpoint.</li> <li><code>master</code> nodes are Kubernetes master nodes that also contain an etcd key-value store. Since etcd is present on these nodes, the number of master nodes must be odd. For more information, see etcd FAQ.</li> <li><code>worker</code> nodes are the nodes on which your actual workload runs.</li> </ul> <p>In this tutorial, we will use only one master node, so internal load balancers are not required. </p> <p>The easiest way to explain this part is to look at the actual configuration: <pre><code>cluster:\n...\nnodes:\nmaster:\ndefault: # Default properties of all master node instances\nram: 4\ncpu: 2\nmainDiskSize: 32\ninstances: # Master node instances\n- id: 1\nip: 192.168.113.10\nworker:\ndefault:\nram: 4\ncpu: 2\nmainDiskSize: 32\ninstances:\n- id: 1\nip: 192.168.113.21\ncpu: 4  # Override default vCPU value for this node\nram: 8  # Override default amount of RAM for this node\n- id: 7\nip: 192.168.113.27\nmac: \"52:54:00:00:00:42\" # Specify MAC address for this node\n- id: 99\n# If ip property is omitted, node will request an IP address \n# from the DHCP server. If mac property is omitted, MAC address\n# will be auto generated.\n</code></pre></p>"},{"location":"user-guide/getting-started/#step-44-kubernetes-properties","title":"Step 4.4 - Kubernetes properties","text":"<p>The last part of the cluster configuration consists of the Kubernetes properties. In this section we define the Kubernetes version, the DNS plugin and so on. It is also important to check if Kubespray supports a specific Kubernetes version.</p> <p>If you are using a custom Kubespray, you can also specify the URL to a custom Git repository.</p> <pre><code>kubernetes:\nversion: \"v1.22.6\"\nnetworkPlugin: \"calico\"\ndnsMode: \"coredns\"\nkubespray:\nversion: \"v2.18.1\"\n# url: URL to custom Kubespray git repository.\n#      (default is: https://github.com/kubernetes-sigs/kubespray.git)\n</code></pre>"},{"location":"user-guide/getting-started/#step-5-create-the-cluster","title":"Step 5 - Create the cluster","text":"<p>Our final cluster configuration looks like this: kubitect.yaml<pre><code>hosts:\n- name: localhost\nconnection:\ntype: local\n\ncluster:\nname: \"my-k8s-cluster\"\nnetwork:\nmode: \"nat\"\ncidr: \"192.168.113.0/24\"\nnodeTemplate:\nuser: \"k8s\"\nssh:\naddToKnownHosts: true\nimage:\ndistro: \"ubuntu\"\nsource: \"https://cloud-images.ubuntu.com/releases/focal/release-20220111/ubuntu-20.04-server-cloudimg-amd64.img\"\nupdateOnBoot: true\nnodes:\nmaster:\ndefault:\nram: 4\ncpu: 2\nmainDiskSize: 32\ninstances:\n- id: 1\nip: 192.168.113.10\nworker:\ndefault:\nram: 4\ncpu: 2\nmainDiskSize: 32\ninstances:\n- id: 1\nip: 192.168.113.21\ncpu: 4\nram: 8\n- id: 7\nip: 192.168.113.27\nmac: \"52:54:00:00:00:42\"\n- id: 99\n\nkubernetes:\nversion: \"v1.22.6\"\nnetworkPlugin: \"calico\"\ndnsMode: \"coredns\"\nkubespray:\nversion: \"v2.18.1\"\n</code></pre></p> <p>Now create the cluster by applying your custom configuration using the kubitect command line tool. Also, let's name our cluster <code>my-first-cluster</code>. <pre><code>kubitect apply --cluster my-first-cluster --config kubitect.yaml\n</code></pre></p> <p>Tip</p> <p>If you encounter any issues during the installation, please refer to the troubleshooting page first.</p> <p>When the cluster is applied, it is created in kubitect home directory, which has the following structure. <pre><code>~/.kubitect\n   \u251c\u2500\u2500 clusters\n   \u2502   \u251c\u2500\u2500 default\n   \u2502   \u251c\u2500\u2500 my-first-cluster\n   \u2502   \u2514\u2500\u2500 ...\n   \u2514\u2500\u2500 bin\n       \u2514\u2500\u2500 ...\n</code></pre></p> <p>All created clusters can be listed at any time using kubitect command line tool. <pre><code>kubitect list clusters\n</code></pre></p>"},{"location":"user-guide/getting-started/#step-6-test-the-cluster","title":"Step 6 - Test the cluster","text":"<p>After successful installation of the Kubernetes cluster, Kubeconfig is created in the cluster's directory.</p> <p>To export the Kubeconfig to a separate file, run the following command. <pre><code>kubitect export kubeconfig --cluster my-first-cluster &gt; kubeconfig.yaml\n</code></pre></p> <p>Use the exported Kubeconfig to list all cluster nodes. <pre><code>kubectl get nodes --kubeconfig kubeconfig.yaml\n</code></pre></p> <p> Congratulations, you have completed the getting started tutorial.</p>"},{"location":"user-guide/installation/","title":"Installation","text":"<p>Before starting with installation, make sure you meet all the requirements.</p>"},{"location":"user-guide/installation/#install-kubitect-cli-tool","title":"Install Kubitect CLI tool","text":"<p>After all requirements are met, download the Kubitect command line tool. <pre><code>curl -o kubitect.tar.gz -L https://github.com/MusicDin/kubitect/releases/download/v2.0.7/kubitect-v2.0.7-linux-amd64.tar.gz\n</code></pre></p> <p>Unpack <code>tar.gz</code> file. <pre><code>tar -xzf kubitect.tar.gz\n</code></pre></p> <p>Install Kubitect command line tool by placing the Kubitect binary file in <code>/usr/local/bin</code> directory. <pre><code>sudo mv kubitect /usr/local/bin/\n</code></pre></p> <p>Verify the installation by checking the Kubitect version. <pre><code>kubitect --version\n\n# kubitect version v2.0.7\n</code></pre></p> <p>Tip</p> <p>If you are using Kubitect for the first time, we strongly recommend you to take a look at the getting started tutorial.</p>"},{"location":"user-guide/installation/#enable-shell-autocomplete","title":"Enable shell autocomplete","text":"<p>For example, to enable automplete for <code>bash</code>, run the following command. <pre><code>echo 'source &lt;(kubitect completion bash)' &gt;&gt; ~/.bashrc\n</code></pre></p> <p>Then reload your shell.</p>"},{"location":"user-guide/load-balancer/","title":"Load balancer","text":"<p>Multiple master nodes ensure that services remain available if one or even more master nodes fail.  Cluster has to be set up with an odd number of master nodes so that the quorum (the majority of master nodes) can be maintained if one or more masters fail. In the high-availability (HA) scenario, Kubernetes maintains a copy of the <code>etcd</code> databases on each master node, but holds elections for the <code>kube-controller</code> and <code>kube-scheduler</code> managers to avoid conflicts. This allows worker nodes to communicate with any master node through a single endpoint provided by load balancers.</p>"},{"location":"user-guide/load-balancer/#configure-haproxy-load-balancers","title":"Configure HAProxy load balancers","text":"<p>Specify load balancer instances in the cluster configuration file. <pre><code>cluster:\n...\nnodes:\n...\nloadBalancer:\nvip: 10.10.64.200 # Floating IP that should not be taken by any other device\ninstances:\n- id: 1\n- id: 40\n</code></pre></p> <p> Note: Load balancers <code>id</code> must be a number between 0 and 200, because their fail-over priority is calculated from the <code>id</code>.</p>"},{"location":"user-guide/load-balancer/#cluster-without-internal-load-balancers","title":"Cluster without internal load balancers","text":"<p>If you have only one master node, the internal load balancers are redundant. In this case, remove (or comment out) all load balancer instances from the cluster configuration file: <pre><code>cluster:\n...\nnodes:\n...\nloadBalancer:\n...\n#instances:\n#- id: 1\n#- id: 40\n</code></pre></p> <p> Note: If multiple master nodes are specified, the IP of the first one is used as the cluster IP.</p>"},{"location":"user-guide/local-development/","title":"Local development","text":"<p>This document shows how to build a CLI tool manually and how to use the project without creating any files outside the project's directory.</p>"},{"location":"user-guide/local-development/#clone-the-project","title":"Clone the project","text":"<p>First, you have to clone the project. <pre><code>git clone https://github.com/MusicDin/kubitect\n</code></pre></p> <p>Afterwards, move into the cloned project. <pre><code>cd kubitect\n</code></pre></p>"},{"location":"user-guide/local-development/#install-kubitect-cli-tool","title":"Install Kubitect CLI tool","text":"<p>Kubitect CLI tool is implemented in Go using cobra library. The tool can either be installed from already built versions available on GitHub or you can build it manually.</p> <p>To manually build the CLI tool, first change to the <code>cli</code> directory. <pre><code>cd cli\n</code></pre></p> <p>Now, using build the tool using go. <pre><code>go build .\n</code></pre></p> <p>This will create a <code>cli</code> binary file, which can be moved into <code>/usr/local/bin/</code> directory to use it globaly. <pre><code>sudo mv cli /usr/bin/local/kubitect\n</code></pre></p>"},{"location":"user-guide/local-development/#local-development","title":"Local development","text":"<p>By default, Kubitect creates and manages clusters located in the Kubitect home directory (<code>~/.kubitect</code>).</p> <p>Although this pattern is very useful for everyday use, it can be somewhat inconvenient if you are actively making changes to the project, as each change must be committed to the Git repository. </p> <p>For this very reason, the Kubitect CLI tool has the <code>--local</code> option, which replaces the project's home directory with the path of the current directory. This way, the source code from the current directory is used to create a cluster and all cluster-related files are created in the current directory. This option can be used with most actions, such as <code>apply</code> or <code>destroy</code>.</p> <pre><code>kubitect apply --local\n</code></pre>"},{"location":"user-guide/quick-start/","title":"Quick start","text":""},{"location":"user-guide/quick-start/#step-1-create-the-cluster","title":"Step 1 - Create the cluster","text":"<p>Run the following command to create the default cluster. Cluster will be created in <code>~/.kubitect/clusters/default/</code> directory.</p> <pre><code>kubitect apply\n</code></pre> <p>Note</p> <p>Using a <code>--cluster</code> option, you can provide custom cluster name. This way multiple clusters can be created.</p>"},{"location":"user-guide/quick-start/#step-2-export-kubeconfig","title":"Step 2 - Export kubeconfig","text":"<p>After successful installation of the Kubernetes cluster, Kubeconfig will be created within cluster's directory. To export the Kubeconfig into custom file run the following command.</p> <pre><code>kubitect export kubeconfig &gt; kubeconfig.yaml\n</code></pre>"},{"location":"user-guide/quick-start/#step-3-test-the-cluster","title":"Step 3 - Test the cluster","text":"<p>Test if the cluster works by displaying all cluster nodes.</p> <pre><code>kubectl get nodes --kubeconfig kubeconfig.yaml\n</code></pre>"},{"location":"user-guide/requirements/","title":"Requirements","text":""},{"location":"user-guide/requirements/#local-machine","title":"Local machine","text":"<p>On the machine where the command-line tool is installed, the following requirements must be met:</p> <ul> <li>Git</li> <li>Python &gt;= 3.0</li> <li>Python virtualenv</li> </ul>"},{"location":"user-guide/requirements/#hosts","title":"Hosts","text":"<p>A host is a physical server that can be either a local or remote machine. Each host must have:</p> <ul> <li>installed hypervisor and</li> <li>installed libvirt virtualization API</li> </ul> <p>If the host is a remote machine, a local machine must have:</p> <ul> <li>appropriate pasword-less SSH keys to sucessfully connect to the remote hypervisor.</li> </ul>"},{"location":"user-guide/requirements/#example-install-kvm","title":"Example - Install KVM","text":"<p>For example, to install the KVM (Kernel Virtual Machine) hypervisor and libvirt, use yum or apt to install the following packages:</p> <ul> <li><code>qemu</code></li> <li><code>qemu-kvm</code></li> <li><code>libvirt-clients</code></li> <li><code>libvirt-daemon</code></li> <li><code>libvirt-daemon-system</code></li> </ul> <p>After installation, also add user to the <code>kvm</code> and <code>libvirt</code> groups.</p>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":"Is your problem not listed here? <p>If the troubleshooting page is missing an error you encountered, please report it on GitHub by opening an issue. By doing so, you will help improve the project and help others find the solution to the same problem faster.</p>"},{"location":"user-guide/troubleshooting/#general-errors","title":"General errors","text":""},{"location":"user-guide/troubleshooting/#virtualenv-not-found","title":"Virtualenv not found","text":"Error Explanation Solution <p>Error</p> <p>Output: /bin/sh: 1: virtualenv: not found</p> <p>/bin/sh: 2: ansible-playbook: not found</p> <p>Explanation</p> <p>The error indicates that the <code>virtualenv</code> is not installed.</p> <p>Solution</p> <p>There are many ways to install <code>virtualenv</code>. For all installation options you can refere to their official documentation - Virtualenv installation.</p> <p>For example, virtualenv can be installed using <code>pip</code>.</p> <p>First install pip. <pre><code>sudo apt install python3-pip\n</code></pre></p> <p>Then install virtualenv using pip3. <pre><code>pip3 install virtualenv\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#kvmlibvirt-errors","title":"KVM/Libvirt errors","text":""},{"location":"user-guide/troubleshooting/#failed-to-connect-socket-no-such-file-or-directory","title":"Failed to connect socket (No such file or directory)","text":"Error Explanation Solution <p>Error</p> <p>Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such file or directory')</p> <p>Explanation</p> <p>The problem may occur when libvirt is not started.</p> <p>Solution</p> <p>Make sure that the <code>libvirt</code> service is running: <pre><code>sudo systemctl status libvirtd\n</code></pre></p> <p>If the <code>libvirt</code> service is not running, start it: <pre><code>sudo systemctl start libvirtd\n</code></pre></p> <p>Optional: Start the <code>libvirt</code> service automatically at boot time: <pre><code>sudo systemctl enable libvirtd\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#failed-to-connect-socket-permission-denied","title":"Failed to connect socket (Permission denied)","text":"Error Explanation Solution <p>Error</p> <p>Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied')</p> <p>Explanation</p> <p>The error indicates that either the <code>libvirtd</code> service is not running or the current user is not in the <code>libvirt</code> (or <code>kvm</code>) group.</p> <p>Solution</p> <p>If the <code>libvirtd</code> service is not running, start it: <pre><code>sudo systemctl start libvirtd\n</code></pre></p> <p>Add the current user to the <code>libvirt</code> and <code>kvm</code> groups if needed: <pre><code># Add current user to groups\nsudo usermod -aG libvirt,kvm `id -un`\n\n# Verify groups are added\nid -nG\n\n# Reload user session\nsu - `id -un`\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#error-creating-libvirt-domain","title":"Error creating libvirt domain","text":"Error Explanation Solution <p>Error</p> <p>Error: Error creating libvirt domain: \u2026 Could not open '/tmp/terraform_libvirt_provider_images/image.qcow2': Permission denied')</p> <p>Explanation</p> <p>The error indicates that the file cannot be created in the specified location due to missing permissions.</p> <ul> <li>Make sure the directory exists.</li> <li>Make sure the directory of the file that is being denied has appropriate user permissions.</li> <li>Optionally qemu security driver can be disabled. </li> </ul> <p>Solution</p> <p>Make sure the <code>security_driver</code> in <code>/etc/libvirt/qemu.conf</code> is set to <code>none</code> instead of <code>selinux</code>. This line is commented out by default, so you should uncomment it if needed: <pre><code># /etc/libvirt/qemu.conf\n\n...\nsecurity_driver = \"none\"\n...\n</code></pre></p> <p>Do not forget to restart the <code>libvirt</code> service after making the changes: <pre><code>sudo systemctl restart libvirtd\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#libvirt-domain-already-exists","title":"Libvirt domain already exists","text":"Error Explanation Solution <p>Error</p> <p>Error: Error defining libvirt domain: virError(Code=9, Domain=20, Message='operation failed: domain 'your-domain' already exists with uuid '...')</p> <p>Explanation</p> <p>The error indicates that the libvirt domain (virtual machine) already exists.</p> <p>Solution</p> <p>The resource you are trying to create already exists.  Make sure you destroy the resource: <pre><code>virsh destroy your-domain\nvirsh undefine your-domain\n</code></pre></p> <p>You can verify that the domain was successfully removed: <pre><code>virsh dominfo --domain your-domain\n</code></pre></p> <p>If the domain was successfully removed, the output should look something like this:</p> <p><code> error: failed to get domain 'your-domain' </code></p>"},{"location":"user-guide/troubleshooting/#libvirt-volume-already-exists","title":"Libvirt volume already exists","text":"Error Explanation Solution <p>Error</p> <p>Error: Error creating libvirt volume: virError(Code=90, Domain=18, Message='storage volume 'your-volume.qcow2' exists already')</p> <p>and / or</p> <p>Error:Error creating libvirt volume for cloudinit device cloud-init.iso: virError(Code=90, Domain=18, Message='storage volume 'cloud-init.iso' exists already')</p> <p>Explanation</p> <p>The error indicates that the specified volume already exists.</p> <p>Solution</p> <p>Volumes created by Libvirt are still attached to the images, which prevents a new volume from being created with the same name. Therefore, these volumes must be removed:</p> <p><code> virsh vol-delete cloud-init.iso --pool your_resource_pool </code></p> <p>and / or</p> <p><code> virsh vol-delete your-volume.qcow2 --pool your_resource_pool </code></p>"},{"location":"user-guide/troubleshooting/#libvirt-storage-pool-already-exists","title":"Libvirt storage pool already exists","text":"Error Explanation Solution <p>Error</p> <p>Error: Error storage pool 'your-pool' already exists</p> <p>Explanation</p> <p>The error indicates that the libvirt storage pool already exists.</p> <p>Solution</p> <p>Remove the existing libvirt storage pool.</p> <p><code> virsh pool-destroy your-pool &amp;&amp; virsh pool-undefine your-pool </code></p>"},{"location":"user-guide/troubleshooting/#failed-to-apply-firewall-rules","title":"Failed to apply firewall rules","text":"Error Explanation Solution <p>Error</p> <p>Error: internal error: Failed to apply firewall rules /sbin/iptables -w --table filter --insert LIBVIRT_INP --in-interface virbr2 --protocol tcp --destination-port 67 --jump ACCEPT: iptables: No chain/target/match by that name.</p> <p>Explanation</p> <p>Libvirt was already running when firewall (usually FirewallD) was started/installed. Therefore, <code>libvirtd</code> service must be restarted to detect the changes.</p> <p>Solution</p> <p>Restart the <code>libvirtd</code> service: <pre><code>sudo systemctl restart libvirtd\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#failed-to-remove-storage-pool","title":"Failed to remove storage pool","text":"Error Explanation Solution <p>Error</p> <p>Error: error deleting storage pool: failed to remove pool '/var/lib/libvirt/pools/local-k8s-cluster-main-resource-pool': Directory not empty</p> <p>Explanation</p> <p>The pool cannot be deleted because there are still some volumes in the pool. Therefore, the volumes should be removed before the pool can be deleted.</p> <p>Solution</p> <ol> <li> <p>Make sure the pool is running. <pre><code>virsh pool-start --pool local-k8s-cluster-main-resource-pool\n</code></pre></p> </li> <li> <p>List volumes in the pool. <pre><code>virsh vol-list --pool local-k8s-cluster-main-resource-pool\n\n#  Name         Path\n# -------------------------------------------------------------------------------------\n#  base_volume  /var/lib/libvirt/pools/local-k8s-cluster-main-resource-pool/base_volume\n</code></pre></p> </li> <li> <p>Delete listed volumes from the pool. <pre><code>virsh vol-delete --pool local-k8s-cluster-main-resource-pool --vol base_volume\n</code></pre></p> </li> <li> <p>Destroy and undefine the pool. <pre><code>virsh pool-destroy --pool local-k8s-cluster-main-resource-pool\nvirsh pool-undefine --pool local-k8s-cluster-main-resource-pool\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#haproxy-load-balancer-errors","title":"HAProxy load balancer errors","text":""},{"location":"user-guide/troubleshooting/#random-haproxy-503-bad-gateway","title":"Random HAProxy (503) bad gateway","text":"Error Explanation Solution <p>Error</p> <p>HAProxy returns a random HTTP 503 (Bad gateway) error.</p> <p>Explanation</p> <p>More than one HAProxy processes are listening on the same port.</p> <p>Solution 1</p> <p>For example, if an error is thrown when accessing port <code>80</code>, check which processes are listening on port <code>80</code> on the load balancer VM: <pre><code>netstat -lnput | grep 80\n\n# Proto Recv-Q Send-Q Local Address           Foreign Address   State       PID/Program name\n# tcp        0      0 192.168.113.200:80      0.0.0.0:*         LISTEN      1976/haproxy\n# tcp        0      0 192.168.113.200:80      0.0.0.0:*         LISTEN      1897/haproxy\n</code></pre></p> <p>If you see more than one process, kill the unnecessary process: <pre><code>kill 1976\n</code></pre></p> <p>Note: You can kill all HAProxy processes and only one will be automatically recreated.</p> <p>Solution 2</p> <p>Check the HAProxy configuration file (<code>config/haproxy/haproxy.cfg</code>) that it does not contain 2 frontends bound to the same port.</p>"}]}