{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"examples/accessing-cluster/","text":"Accessing the cluster \ud83d\udd17\ufe0e Kubernetes clusters running on cloud providers typically support provision of a load balancer from the cloud infrastructure on demand. By simply setting a Service type to LoadBalancer provisions an external load balancer that has its own unique IP address and redirects all connections to the Service, as shown in the figure below. In on-premise environments, there is no load balancer that can be provisioned on demand. Therefore, some alternative solutions are explained in this document. Node ports \ud83d\udd17\ufe0e Setting Service type to NodePort makes Kubernetes reserve a port on all its nodes. As a result, the Service becomes available on <NodeIP>:<NodePort> , as shown in the figure below. When using NodePort , it does not matter to which node a client sends the request, since it is routed internally to the appropriate Pod. However, if all traffic is directed to a single node, its failure will make the Service unavailable. Self-provisioned edge \ud83d\udd17\ufe0e With Kubitect, it is possible to configure the port forwarding of the load balancer to distribute incoming requests to multiple nodes in the cluster, as shown in the figure below. To set up load balancer port forwarding, at least one load balancer must be configured. The following example shows how to set up load balancer port forwarding for ports 80 (HTTP) and 443 (HTTPS). cluster : nodes : loadBalancer : forwardPorts : - name : http port : 80 - name : https port : 443 instances : - id : 1 Load balancer port forwarding is particularly handy when combined with a NodePort Service or a Service whose ports are exposed on the host. For example, for HTTP and HTTPS traffic an Ingress is most often used. To use Ingress resources in the Kubernetes cluster, an ingress controller is required. With Kubitect, a load balancer can be configured to accept connections on ports 80 and 443, and redirect them to all cluster nodes on ports 50080 and 50443 where an ingress controller is listening for incoming requests. The following code snippet shows the configuration for such a scenario. cluster : nodes : loadBalancer : forwardPorts : - name : http port : 80 targetPort : 50080 target : workers # (1)! - name : https port : 443 targetPort : 50443 instances : - id : 1 addons : kubespray : ingress_nginx_enabled : true ingress_nginx_namespace : \"ingress-nginx\" ingress_nginx_insecure_port : 50080 # (2)! ingress_nginx_secure_port : 50443 By default, each configured port instructs the load balancer to distribute traffic across all worker nodes. The default behavior can be changed using the target property. Possible target values are: workers - Distributes traffic across worker nodes. (default) masters - Distributes traffic across master nodes. all - Distributes traffic across master and worker nodes. When the ingress-nginx controller is set up with Kubespray, a DaemonSet is created that exposes ports on the host ( hostPort ). MetalLB \ud83d\udd17\ufe0e MetalLB is a network load balancer implementation for bare metal Kubernetes clusters. In short, it allows you to create Services of type LoadBalancer where actual on-demand load balancers are not an option. For MetalLB to work, a pool of unused IP addresses needs to be provided. In the following example, MetalLB is configured to use an IP address pool with the IP range 10.10.13.225/27 . addons : kubespray : metallb_enabled : true metallb_speaker_enabled : true metallb_ip_range : - \"10.10.13.225/27\" metallb_pool_name : \"default\" metallb_auto_assign : true metallb_version : v0.12.1 metallb_protocol : \"layer2\" When a Service of type LoadBalancer is created, it is assigned an IP address from the pool. For example, we could deploy an ingress-nginx controller and change its Service type to LoadBalancer . # Deploy ingress-nginx controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/baremetal/1.23/deploy.yaml # Patch ingress controller Service type to LoadBalancer kubectl patch svc ingress-nginx-controller -n ingress-nginx -p '{\"spec\": {\"type\":\"LoadBalancer\"}}' As a result, MetalLB assigns the service ingress-nginx-controller an external IP address from the address pool. kubectl get svc -n ingress-nginx ingress-nginx-controller # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # ingress-nginx-controller LoadBalancer 10.233.55.194 10.10.13.225 80:31497/TCP,443:30967/TCP 63s By sending a request to the assigned IP address, it can be seen that Nginx responds to the request. curl -k https://10.10.13.225 # <html> # <head><title>404 Not Found</title></head> # <body> # <center><h1>404 Not Found</h1></center> # <hr><center>nginx</center> # </body> # </html> This example has demonstrated the functionality of MetalLB in layer2 mode. For more MetalLB configuration options, see the official MetalLB documentation .","title":"Accessing the cluster"},{"location":"examples/accessing-cluster/#accessing-the-cluster","text":"Kubernetes clusters running on cloud providers typically support provision of a load balancer from the cloud infrastructure on demand. By simply setting a Service type to LoadBalancer provisions an external load balancer that has its own unique IP address and redirects all connections to the Service, as shown in the figure below. In on-premise environments, there is no load balancer that can be provisioned on demand. Therefore, some alternative solutions are explained in this document.","title":"Accessing the cluster"},{"location":"examples/accessing-cluster/#node-ports","text":"Setting Service type to NodePort makes Kubernetes reserve a port on all its nodes. As a result, the Service becomes available on <NodeIP>:<NodePort> , as shown in the figure below. When using NodePort , it does not matter to which node a client sends the request, since it is routed internally to the appropriate Pod. However, if all traffic is directed to a single node, its failure will make the Service unavailable.","title":"Node ports"},{"location":"examples/accessing-cluster/#self-provisioned-edge","text":"With Kubitect, it is possible to configure the port forwarding of the load balancer to distribute incoming requests to multiple nodes in the cluster, as shown in the figure below. To set up load balancer port forwarding, at least one load balancer must be configured. The following example shows how to set up load balancer port forwarding for ports 80 (HTTP) and 443 (HTTPS). cluster : nodes : loadBalancer : forwardPorts : - name : http port : 80 - name : https port : 443 instances : - id : 1 Load balancer port forwarding is particularly handy when combined with a NodePort Service or a Service whose ports are exposed on the host. For example, for HTTP and HTTPS traffic an Ingress is most often used. To use Ingress resources in the Kubernetes cluster, an ingress controller is required. With Kubitect, a load balancer can be configured to accept connections on ports 80 and 443, and redirect them to all cluster nodes on ports 50080 and 50443 where an ingress controller is listening for incoming requests. The following code snippet shows the configuration for such a scenario. cluster : nodes : loadBalancer : forwardPorts : - name : http port : 80 targetPort : 50080 target : workers # (1)! - name : https port : 443 targetPort : 50443 instances : - id : 1 addons : kubespray : ingress_nginx_enabled : true ingress_nginx_namespace : \"ingress-nginx\" ingress_nginx_insecure_port : 50080 # (2)! ingress_nginx_secure_port : 50443 By default, each configured port instructs the load balancer to distribute traffic across all worker nodes. The default behavior can be changed using the target property. Possible target values are: workers - Distributes traffic across worker nodes. (default) masters - Distributes traffic across master nodes. all - Distributes traffic across master and worker nodes. When the ingress-nginx controller is set up with Kubespray, a DaemonSet is created that exposes ports on the host ( hostPort ).","title":"Self-provisioned edge"},{"location":"examples/accessing-cluster/#metallb","text":"MetalLB is a network load balancer implementation for bare metal Kubernetes clusters. In short, it allows you to create Services of type LoadBalancer where actual on-demand load balancers are not an option. For MetalLB to work, a pool of unused IP addresses needs to be provided. In the following example, MetalLB is configured to use an IP address pool with the IP range 10.10.13.225/27 . addons : kubespray : metallb_enabled : true metallb_speaker_enabled : true metallb_ip_range : - \"10.10.13.225/27\" metallb_pool_name : \"default\" metallb_auto_assign : true metallb_version : v0.12.1 metallb_protocol : \"layer2\" When a Service of type LoadBalancer is created, it is assigned an IP address from the pool. For example, we could deploy an ingress-nginx controller and change its Service type to LoadBalancer . # Deploy ingress-nginx controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/baremetal/1.23/deploy.yaml # Patch ingress controller Service type to LoadBalancer kubectl patch svc ingress-nginx-controller -n ingress-nginx -p '{\"spec\": {\"type\":\"LoadBalancer\"}}' As a result, MetalLB assigns the service ingress-nginx-controller an external IP address from the address pool. kubectl get svc -n ingress-nginx ingress-nginx-controller # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # ingress-nginx-controller LoadBalancer 10.233.55.194 10.10.13.225 80:31497/TCP,443:30967/TCP 63s By sending a request to the assigned IP address, it can be seen that Nginx responds to the request. curl -k https://10.10.13.225 # <html> # <head><title>404 Not Found</title></head> # <body> # <center><h1>404 Not Found</h1></center> # <hr><center>nginx</center> # </body> # </html> This example has demonstrated the functionality of MetalLB in layer2 mode. For more MetalLB configuration options, see the official MetalLB documentation .","title":"MetalLB"},{"location":"examples/full-example/","text":"Full (detailed) example \ud83d\udd17\ufe0e This document contains an example of Kubitect configuration. Example covers all ( or most ) of the Kubitect properties. This example is meant for users that learn the fastest from an example configuration. # # In the 'kubitect' section, you can specify the target git project and version. # This can be handy if you want to use a specific project version or refer to # your forked/cloned project. # # [!] Note that this will be ignored if you use the '--local' flag with the CLI # tool actions, as you should already be in the Git repository. # kubitect : url : \"https://github.com/MusicDin/kubitect\" # (1)! version : \"v2.1.0\" # # The 'hosts' section contains data about the physical servers on which the # Kubernetes cluster will be installed. # # For each host, a name and connection type must be specified. Only one host can # have the connection type set to 'local' or 'localhost'. # # If the host is a remote machine, the path to the SSH key file must be specified. # Note that connections to remote hosts support only passwordless certificates. # # The host can also be marked as default, i.e. if no specific host is specified # for an instance (in the cluster.nodes section), it will be installed on a # default host. If none of the hosts are marked as default, the first host in the # list is used as the default host. # hosts : - name : localhost # (3)! default : true # (4)! connection : type : local # (5)! - name : remote-server-1 connection : type : remote user : myuser # (6)! ip : 10.10.40.143 # (7)! ssh : port : 1234 # (8)! verify : true # (9)! keyfile : \"~/.ssh/id_rsa_server1\" # (10)! - name : remote-server-2 connection : type : remote user : myuser ip : 10.10.40.144 ssh : keyfile : \"~/.ssh/id_rsa_server2\" mainResourcePoolPath : \"/var/lib/libvirt/pools/\" # (11)! dataResourcePools : # (12)! - name : data-pool # (13)! path : \"/mnt/data/pool\" # (14)! - name : backup-pool path : \"/mnt/backup/pool\" # # The 'cluster' section of the configuration contains general data about the # cluster, the nodes that are part of the cluster, and the cluster's network. # cluster : name : my-k8s-cluster # (15)! network : mode : bridge # (16)! cidr : 10.10.64.0/24 # (17)! gateway : 10.10.64.1 # (18)! bridge : br0 # (19)! nodeTemplate : user : k8s ssh : privateKeyPath : \"~/.ssh/id_rsa_test\" addToKnownHosts : true os : distro : ubuntu networkInterface : ens3 # (20)! dns : # (21)! - 1.1.1.1 - 1.0.0.1 updateOnBoot : true nodes : loadBalancer : vip : 10.10.64.200 # (22)! virtualRouterId : 13 # (23)! forwardPorts : - name : http port : 80 - name : https port : 443 target : all - name : sample port : 60000 targetPort : 35000 default : # (24)! ram : 4 # GiB cpu : 1 # vCPU mainDiskSize : 16 # GiB instances : - id : 1 ip : 10.10.64.5 # (25)! mac : \"52:54:00:00:00:40\" # (26)! ram : 8 # (27)! cpu : 8 # (28)! host : remote-server-1 # (29)! - id : 2 ip : 10.10.64.6 mac : \"52:54:00:00:00:41\" host : remote-server-2 - id : 3 ip : 10.10.64.7 mac : \"52:54:00:00:00:42\" # If host is not specifed, VM will be installed on the default host. # If default host is not specified, VM will be installed on the first # host in the list. master : default : ram : 8 cpu : 2 mainDiskSize : 256 instances : # IMPORTANT: There should be odd number of master nodes. - id : 1 # Node with generated MAC address, IP retrieved as an DHCP lease and default RAM and CPU. host : remote-server-1 - id : 2 host : remote-server-2 - id : 3 host : localhost worker : default : ram : 16 cpu : 4 labels : # (30)! custom-label : \"This is a custom default node label\" node-role.kubernetes.io/node : # (31)! instances : - id : 1 ip : 10.10.64.101 cpu : 8 ram : 64 host : remote-server-1 - id : 2 ip : 10.10.64.102 dataDisks : # (32)! - name : rook-disk # (33)! pool : data-pool # (34)! size : 128 # GiB - name : test-disk pool : data-pool size : 128 - id : 3 ip : 10.10.64.103 ram : 64 labels : custom-label : \"Overwrite default node label\" # (35)! instance-label : \"Node label, only for this instance\" - id : 4 host : remote-server-2 - id : 5 # # The 'kubernetes' section specifies which version of Kubernetes and # Kubespray to use and which network plugin and DNS server to install. # kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : url : \"https://github.com/kubernetes-sigs/kubespray.git\" version : v2.20.0 other : copyKubeconfig : false # # The 'addons' section contains the configuration of the applications that # will be installed on the Kubernetes cluster as part of the cluster setup. # addons : kubespray : # Sample Nginx ingress controller deployment ingress_nginx_enabled : true ingress_nginx_namespace : \"ingress-nginx\" ingress_nginx_insecure_port : 80 ingress_nginx_secure_port : 443 # Sample MetalLB deployment metallb_enabled : true metallb_speaker_enabled : true metallb_ip_range : - \"10.10.9.201-10.10.9.254\" metallb_pool_name : \"default\" metallb_auto_assign : true metallb_version : v0.12.1 metallb_protocol : \"layer2\" This allows you to set a custom URL that targets clone/fork of Kubitect project. Kubitect version. Custom host name. It is used to link instances to the specific host. Makes the host a default host. This means that if no host is specified for the node instance, the instance will be linked to the default host. Connection type can be either local or remote . If it is set to remote , at least the following fields must be set: user ip ssh.keyfile Remote host user that is used to connect to the remote hypervisor. This user must be added in the libvirt group. IP address of the remote host. Overrides default SSH port (22). If true, SSH host is verified. This means that the host must be present in the known SSH hosts. Path to the passwordless SSH key used to connect to the remote host. The path to the main resource pool defines where the virtual machine disk images are stored. These disks contain the virtual machine operating system, and therefore it is recommended to install them on SSD disks. List of other data resource pools where virtual disks can be created. Custom data resource pool name. Must be unique among all data resource pools on a specific host. Path where data resource pool is created. All data disks linked to that resource pool will be created under this path. Cluster name used as a prefix for the various components. Network mode. Possible values are bridge mode uses predefined bridge interface. This mode is mandatory for deployments across multiple hosts. nat mode creates virtual network with IP range defined in network.cidr route Network CIDR represents the network IP together with the network mask. In nat mode, CIDR is used for the new network. In bridge mode, CIDR represents the current local area network (LAN). The network gateway IP address. If omitted the first client IP from network CIDR is used as a gateway. Bridge represents the bridge interface on the hosts. This field is mandatory if the network mode is set to bridge . If the network mode is set to nat , this field can be omitted. Set custom DNS list for all nodes. If omitted, network gateway is also used as a DNS. Specify the network interface used by the virtual machine. In general, this option can be omitted. If omitted, a network interface from distro preset ( /terraform/defaults.yaml ) is used. Virtual (floating) IP shared between load balancers. Virtual router ID that is set in Keepalived configuration when virtual IP is used. By default it is set to 51. If multiple clusters are created it must be ensured that it is unique for each cluster. Default values apply for all virtual machines (VMs) of the same type. Static IP address of the virtual machine. If omitted DHCP lease is requested. Static MAC address. If omitted MAC address is generated. Overrides default RAM value for this node. Overrides default CPU value for this node. Name of the host where instance should be created. If omitted the default host is used. Default worker node labels. Label sets worker nodes role to node . Overrides default data disks for this node. Custom data disk name. It must be unique among all data disks for a specific instance. Resource pool name that must be defined on the host on which the instance will be deployed. Node labels defined for specific instances take precedence over default labels with the same key, so this label overrides the default label.","title":"Full example"},{"location":"examples/full-example/#full-detailed-example","text":"This document contains an example of Kubitect configuration. Example covers all ( or most ) of the Kubitect properties. This example is meant for users that learn the fastest from an example configuration. # # In the 'kubitect' section, you can specify the target git project and version. # This can be handy if you want to use a specific project version or refer to # your forked/cloned project. # # [!] Note that this will be ignored if you use the '--local' flag with the CLI # tool actions, as you should already be in the Git repository. # kubitect : url : \"https://github.com/MusicDin/kubitect\" # (1)! version : \"v2.1.0\" # # The 'hosts' section contains data about the physical servers on which the # Kubernetes cluster will be installed. # # For each host, a name and connection type must be specified. Only one host can # have the connection type set to 'local' or 'localhost'. # # If the host is a remote machine, the path to the SSH key file must be specified. # Note that connections to remote hosts support only passwordless certificates. # # The host can also be marked as default, i.e. if no specific host is specified # for an instance (in the cluster.nodes section), it will be installed on a # default host. If none of the hosts are marked as default, the first host in the # list is used as the default host. # hosts : - name : localhost # (3)! default : true # (4)! connection : type : local # (5)! - name : remote-server-1 connection : type : remote user : myuser # (6)! ip : 10.10.40.143 # (7)! ssh : port : 1234 # (8)! verify : true # (9)! keyfile : \"~/.ssh/id_rsa_server1\" # (10)! - name : remote-server-2 connection : type : remote user : myuser ip : 10.10.40.144 ssh : keyfile : \"~/.ssh/id_rsa_server2\" mainResourcePoolPath : \"/var/lib/libvirt/pools/\" # (11)! dataResourcePools : # (12)! - name : data-pool # (13)! path : \"/mnt/data/pool\" # (14)! - name : backup-pool path : \"/mnt/backup/pool\" # # The 'cluster' section of the configuration contains general data about the # cluster, the nodes that are part of the cluster, and the cluster's network. # cluster : name : my-k8s-cluster # (15)! network : mode : bridge # (16)! cidr : 10.10.64.0/24 # (17)! gateway : 10.10.64.1 # (18)! bridge : br0 # (19)! nodeTemplate : user : k8s ssh : privateKeyPath : \"~/.ssh/id_rsa_test\" addToKnownHosts : true os : distro : ubuntu networkInterface : ens3 # (20)! dns : # (21)! - 1.1.1.1 - 1.0.0.1 updateOnBoot : true nodes : loadBalancer : vip : 10.10.64.200 # (22)! virtualRouterId : 13 # (23)! forwardPorts : - name : http port : 80 - name : https port : 443 target : all - name : sample port : 60000 targetPort : 35000 default : # (24)! ram : 4 # GiB cpu : 1 # vCPU mainDiskSize : 16 # GiB instances : - id : 1 ip : 10.10.64.5 # (25)! mac : \"52:54:00:00:00:40\" # (26)! ram : 8 # (27)! cpu : 8 # (28)! host : remote-server-1 # (29)! - id : 2 ip : 10.10.64.6 mac : \"52:54:00:00:00:41\" host : remote-server-2 - id : 3 ip : 10.10.64.7 mac : \"52:54:00:00:00:42\" # If host is not specifed, VM will be installed on the default host. # If default host is not specified, VM will be installed on the first # host in the list. master : default : ram : 8 cpu : 2 mainDiskSize : 256 instances : # IMPORTANT: There should be odd number of master nodes. - id : 1 # Node with generated MAC address, IP retrieved as an DHCP lease and default RAM and CPU. host : remote-server-1 - id : 2 host : remote-server-2 - id : 3 host : localhost worker : default : ram : 16 cpu : 4 labels : # (30)! custom-label : \"This is a custom default node label\" node-role.kubernetes.io/node : # (31)! instances : - id : 1 ip : 10.10.64.101 cpu : 8 ram : 64 host : remote-server-1 - id : 2 ip : 10.10.64.102 dataDisks : # (32)! - name : rook-disk # (33)! pool : data-pool # (34)! size : 128 # GiB - name : test-disk pool : data-pool size : 128 - id : 3 ip : 10.10.64.103 ram : 64 labels : custom-label : \"Overwrite default node label\" # (35)! instance-label : \"Node label, only for this instance\" - id : 4 host : remote-server-2 - id : 5 # # The 'kubernetes' section specifies which version of Kubernetes and # Kubespray to use and which network plugin and DNS server to install. # kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : url : \"https://github.com/kubernetes-sigs/kubespray.git\" version : v2.20.0 other : copyKubeconfig : false # # The 'addons' section contains the configuration of the applications that # will be installed on the Kubernetes cluster as part of the cluster setup. # addons : kubespray : # Sample Nginx ingress controller deployment ingress_nginx_enabled : true ingress_nginx_namespace : \"ingress-nginx\" ingress_nginx_insecure_port : 80 ingress_nginx_secure_port : 443 # Sample MetalLB deployment metallb_enabled : true metallb_speaker_enabled : true metallb_ip_range : - \"10.10.9.201-10.10.9.254\" metallb_pool_name : \"default\" metallb_auto_assign : true metallb_version : v0.12.1 metallb_protocol : \"layer2\" This allows you to set a custom URL that targets clone/fork of Kubitect project. Kubitect version. Custom host name. It is used to link instances to the specific host. Makes the host a default host. This means that if no host is specified for the node instance, the instance will be linked to the default host. Connection type can be either local or remote . If it is set to remote , at least the following fields must be set: user ip ssh.keyfile Remote host user that is used to connect to the remote hypervisor. This user must be added in the libvirt group. IP address of the remote host. Overrides default SSH port (22). If true, SSH host is verified. This means that the host must be present in the known SSH hosts. Path to the passwordless SSH key used to connect to the remote host. The path to the main resource pool defines where the virtual machine disk images are stored. These disks contain the virtual machine operating system, and therefore it is recommended to install them on SSD disks. List of other data resource pools where virtual disks can be created. Custom data resource pool name. Must be unique among all data resource pools on a specific host. Path where data resource pool is created. All data disks linked to that resource pool will be created under this path. Cluster name used as a prefix for the various components. Network mode. Possible values are bridge mode uses predefined bridge interface. This mode is mandatory for deployments across multiple hosts. nat mode creates virtual network with IP range defined in network.cidr route Network CIDR represents the network IP together with the network mask. In nat mode, CIDR is used for the new network. In bridge mode, CIDR represents the current local area network (LAN). The network gateway IP address. If omitted the first client IP from network CIDR is used as a gateway. Bridge represents the bridge interface on the hosts. This field is mandatory if the network mode is set to bridge . If the network mode is set to nat , this field can be omitted. Set custom DNS list for all nodes. If omitted, network gateway is also used as a DNS. Specify the network interface used by the virtual machine. In general, this option can be omitted. If omitted, a network interface from distro preset ( /terraform/defaults.yaml ) is used. Virtual (floating) IP shared between load balancers. Virtual router ID that is set in Keepalived configuration when virtual IP is used. By default it is set to 51. If multiple clusters are created it must be ensured that it is unique for each cluster. Default values apply for all virtual machines (VMs) of the same type. Static IP address of the virtual machine. If omitted DHCP lease is requested. Static MAC address. If omitted MAC address is generated. Overrides default RAM value for this node. Overrides default CPU value for this node. Name of the host where instance should be created. If omitted the default host is used. Default worker node labels. Label sets worker nodes role to node . Overrides default data disks for this node. Custom data disk name. It must be unique among all data disks for a specific instance. Resource pool name that must be defined on the host on which the instance will be deployed. Node labels defined for specific instances take precedence over default labels with the same key, so this label overrides the default label.","title":"Full (detailed) example"},{"location":"examples/ha-cluster/","text":"Highly available cluster \ud83d\udd17\ufe0e This example shows how to use Kubitect to set up a highly available cluster that spreads over 5 hosts. Such topology provides redundancy in case any node or even host fails. Step 1: Hosts configuration \ud83d\udd17\ufe0e Important This example uses preconfigured bridges on each host to expose nodes on the local network. Network bridge example shows how to configure a bridge interface using Netplan. In this example, we deploy a Kubernetes cluster on 5 (remote) physical hosts. The subnet of the local network is 10.10.0.0/20 and the gateway IP address is 10.10.0.1 . Each host is connected to the same local network and has a pre-configured bridge interface br0 . In addition, a user kubitect is configured on each host, which is accessible via SSH with the same password-less certificate stored on our local machine under the path ~/.ssh/id_rsa_ha . Each host must be specified in the Kubitect configuration file. In our case, the configurations of the hosts differ only by the name and IP address of the host. ha.yaml hosts : - name : host1 connection : type : remote user : kubitect ip : 10.10.0.5 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host2 connection : type : remote user : kubitect ip : 10.10.0.6 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host3 connection : type : remote user : kubitect ip : 10.10.0.10 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host4 connection : type : remote user : kubitect ip : 10.10.0.11 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host5 connection : type : remote user : kubitect ip : 10.10.0.12 ssh : keyfile : \"~/.ssh/id_rsa_ha\" Step 2: Network configuration \ud83d\udd17\ufe0e In the network configuration section, we specify the bridge interface that is preconfigured on each host and CIDR of our local network. The following code snippet shows the network configuration for this example. ha.yaml cluster : network : mode : bridge cidr : 10.10.0.0/20 bridge : br0 Step 3: Load balancer configuration \ud83d\udd17\ufe0e Each working node stores only a single control plane IP address. By placing a load balancer in front of the control plane (as shown in the Multi-master cluster example), traffic can be distributed across all control plane nodes. By having only a single load balancer in the cluster, the control plane may become unreachable if that load balancer fails. This would cause the entire cluster to become unavailable. To avoid this single point of failure, a failover (backup) load balancer can be configured. Its main purpose is to serve incoming requests on the same virtual (shared) IP address if the primary load balancer fails, as shown in the following figure. Failover is achieved using a virtual router redundancy protocol (VRRP). In practice, each load balancer still has its own IP address, but the primary load balancer also serves requests on the virtual IP address, which is not bound to any network interface. The primary load balancer periodically sends heartbeats to other load balancers (backups) to let them know it is still active. If the backup load balancer does not receive a heartbeat within a certain period of time, it assumes that the primary load balancer is has failed. The new primary load balancer is elected based on the priorities of the available load balancers. Once the load balancer becomes primary, it starts serving requests on the same virtual IP address as the previous one. This ensures that the requests are served through the same virtual IP address in case of a load balancer failure. The following code snippet shows the configuration of two load balancers and virtual IP for their failover. ha.yaml cluster : nodes : loadBalancer : vip : 10.10.13.200 instances : - id : 1 ip : 10.10.13.201 host : host1 - id : 2 ip : 10.10.13.202 host : host2 Note that for each load balancer instance, the host property is set. Its value is a name of the host on which a particular instance is to be deployed. Step 4: Nodes configuration \ud83d\udd17\ufe0e The configuration of the nodes is very simple and similar to the configuration of the load balancer instances. Each instance is configured with an ID, an IP address, and a host affinity. ha.yaml cluster : nodes : master : instances : - id : 1 ip : 10.10.13.10 host : host3 - id : 2 ip : 10.10.13.11 host : host4 - id : 3 ip : 10.10.13.12 host : host5 worker : instances : - id : 1 ip : 10.10.13.20 host : host3 - id : 2 ip : 10.10.13.21 host : host4 - id : 3 ip : 10.10.13.22 host : host5 Step 4.1 (Optional): Data disks configuration \ud83d\udd17\ufe0e Kubitect creates a main (system) disk for each configured virtual machine (VM). The main disk contains the VM's operating system along with all installed Kubernetes components. The VM's storage can be expanded by creating additional disks, also called data disks. This can be particularly useful when using storage solutions such as Rook. For example, Rook can be configured to use empty disks on worker nodes to create reliable distributed storage. Data disks in Kubitect must be configured separately for each node instance. They must also be connected to a resource pool, which can be either a main resource pool or a custom data resource pool. In this example, we have defined a custom data resource pool named data-pool on each host running worker nodes. ha.yaml hosts : - name : host3 ... dataResourcePools : - name : data-pool path : /mnt/libvirt/pools/ cluster : nodes : worker : - id : 1 ip : 10.10.13.20 host : host3 dataDisks : - name : rook pool : data-pool size : 512 # GiB Final cluster configuration ha.yaml hosts : - name : host1 connection : type : remote user : kubitect ip : 10.10.0.5 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host2 connection : type : remote user : kubitect ip : 10.10.0.6 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host3 connection : type : remote user : kubitect ip : 10.10.0.10 ssh : keyfile : \"~/.ssh/id_rsa_ha\" dataResourcePools : - name : data-pool path : /mnt/libvirt/pools/ - name : host4 connection : type : remote user : kubitect ip : 10.10.0.11 ssh : keyfile : \"~/.ssh/id_rsa_ha\" dataResourcePools : - name : data-pool path : /mnt/libvirt/pools/ - name : host5 connection : type : remote user : kubitect ip : 10.10.0.12 ssh : keyfile : \"~/.ssh/id_rsa_ha\" dataResourcePools : - name : data-pool path : /mnt/libvirt/pools/ cluster : name : kubitect-ha network : mode : bridge cidr : 10.10.0.0/20 bridge : br0 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : loadBalancer : vip : 10.10.13.200 instances : - id : 1 ip : 10.10.13.201 host : host1 - id : 2 ip : 10.10.13.202 host : host2 master : instances : - id : 1 ip : 10.10.13.10 host : host3 - id : 2 ip : 10.10.13.11 host : host4 - id : 3 ip : 10.10.13.12 host : host5 worker : instances : - id : 1 ip : 10.10.13.20 host : host3 dataDisks : - name : rook pool : data-pool size : 512 - id : 2 ip : 10.10.13.21 host : host4 dataDisks : - name : rook pool : data-pool size : 512 - id : 3 ip : 10.10.13.22 host : host5 dataDisks : - name : rook pool : data-pool size : 512 kubernetes : version : v1.23.7 kubespray : version : v2.20.0 Step 5: Applying the configuration \ud83d\udd17\ufe0e Apply the cluster configuration. kubitect apply --config ha.yaml","title":"Highly available (HA) cluster"},{"location":"examples/ha-cluster/#highly-available-cluster","text":"This example shows how to use Kubitect to set up a highly available cluster that spreads over 5 hosts. Such topology provides redundancy in case any node or even host fails.","title":"Highly available cluster"},{"location":"examples/ha-cluster/#step-1-hosts-configuration","text":"Important This example uses preconfigured bridges on each host to expose nodes on the local network. Network bridge example shows how to configure a bridge interface using Netplan. In this example, we deploy a Kubernetes cluster on 5 (remote) physical hosts. The subnet of the local network is 10.10.0.0/20 and the gateway IP address is 10.10.0.1 . Each host is connected to the same local network and has a pre-configured bridge interface br0 . In addition, a user kubitect is configured on each host, which is accessible via SSH with the same password-less certificate stored on our local machine under the path ~/.ssh/id_rsa_ha . Each host must be specified in the Kubitect configuration file. In our case, the configurations of the hosts differ only by the name and IP address of the host. ha.yaml hosts : - name : host1 connection : type : remote user : kubitect ip : 10.10.0.5 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host2 connection : type : remote user : kubitect ip : 10.10.0.6 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host3 connection : type : remote user : kubitect ip : 10.10.0.10 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host4 connection : type : remote user : kubitect ip : 10.10.0.11 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host5 connection : type : remote user : kubitect ip : 10.10.0.12 ssh : keyfile : \"~/.ssh/id_rsa_ha\"","title":"Step 1: Hosts configuration"},{"location":"examples/ha-cluster/#step-2-network-configuration","text":"In the network configuration section, we specify the bridge interface that is preconfigured on each host and CIDR of our local network. The following code snippet shows the network configuration for this example. ha.yaml cluster : network : mode : bridge cidr : 10.10.0.0/20 bridge : br0","title":"Step 2: Network configuration"},{"location":"examples/ha-cluster/#step-3-load-balancer-configuration","text":"Each working node stores only a single control plane IP address. By placing a load balancer in front of the control plane (as shown in the Multi-master cluster example), traffic can be distributed across all control plane nodes. By having only a single load balancer in the cluster, the control plane may become unreachable if that load balancer fails. This would cause the entire cluster to become unavailable. To avoid this single point of failure, a failover (backup) load balancer can be configured. Its main purpose is to serve incoming requests on the same virtual (shared) IP address if the primary load balancer fails, as shown in the following figure. Failover is achieved using a virtual router redundancy protocol (VRRP). In practice, each load balancer still has its own IP address, but the primary load balancer also serves requests on the virtual IP address, which is not bound to any network interface. The primary load balancer periodically sends heartbeats to other load balancers (backups) to let them know it is still active. If the backup load balancer does not receive a heartbeat within a certain period of time, it assumes that the primary load balancer is has failed. The new primary load balancer is elected based on the priorities of the available load balancers. Once the load balancer becomes primary, it starts serving requests on the same virtual IP address as the previous one. This ensures that the requests are served through the same virtual IP address in case of a load balancer failure. The following code snippet shows the configuration of two load balancers and virtual IP for their failover. ha.yaml cluster : nodes : loadBalancer : vip : 10.10.13.200 instances : - id : 1 ip : 10.10.13.201 host : host1 - id : 2 ip : 10.10.13.202 host : host2 Note that for each load balancer instance, the host property is set. Its value is a name of the host on which a particular instance is to be deployed.","title":"Step 3: Load balancer configuration"},{"location":"examples/ha-cluster/#step-4-nodes-configuration","text":"The configuration of the nodes is very simple and similar to the configuration of the load balancer instances. Each instance is configured with an ID, an IP address, and a host affinity. ha.yaml cluster : nodes : master : instances : - id : 1 ip : 10.10.13.10 host : host3 - id : 2 ip : 10.10.13.11 host : host4 - id : 3 ip : 10.10.13.12 host : host5 worker : instances : - id : 1 ip : 10.10.13.20 host : host3 - id : 2 ip : 10.10.13.21 host : host4 - id : 3 ip : 10.10.13.22 host : host5","title":"Step 4: Nodes configuration"},{"location":"examples/ha-cluster/#step-41-optional-data-disks-configuration","text":"Kubitect creates a main (system) disk for each configured virtual machine (VM). The main disk contains the VM's operating system along with all installed Kubernetes components. The VM's storage can be expanded by creating additional disks, also called data disks. This can be particularly useful when using storage solutions such as Rook. For example, Rook can be configured to use empty disks on worker nodes to create reliable distributed storage. Data disks in Kubitect must be configured separately for each node instance. They must also be connected to a resource pool, which can be either a main resource pool or a custom data resource pool. In this example, we have defined a custom data resource pool named data-pool on each host running worker nodes. ha.yaml hosts : - name : host3 ... dataResourcePools : - name : data-pool path : /mnt/libvirt/pools/ cluster : nodes : worker : - id : 1 ip : 10.10.13.20 host : host3 dataDisks : - name : rook pool : data-pool size : 512 # GiB Final cluster configuration ha.yaml hosts : - name : host1 connection : type : remote user : kubitect ip : 10.10.0.5 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host2 connection : type : remote user : kubitect ip : 10.10.0.6 ssh : keyfile : \"~/.ssh/id_rsa_ha\" - name : host3 connection : type : remote user : kubitect ip : 10.10.0.10 ssh : keyfile : \"~/.ssh/id_rsa_ha\" dataResourcePools : - name : data-pool path : /mnt/libvirt/pools/ - name : host4 connection : type : remote user : kubitect ip : 10.10.0.11 ssh : keyfile : \"~/.ssh/id_rsa_ha\" dataResourcePools : - name : data-pool path : /mnt/libvirt/pools/ - name : host5 connection : type : remote user : kubitect ip : 10.10.0.12 ssh : keyfile : \"~/.ssh/id_rsa_ha\" dataResourcePools : - name : data-pool path : /mnt/libvirt/pools/ cluster : name : kubitect-ha network : mode : bridge cidr : 10.10.0.0/20 bridge : br0 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : loadBalancer : vip : 10.10.13.200 instances : - id : 1 ip : 10.10.13.201 host : host1 - id : 2 ip : 10.10.13.202 host : host2 master : instances : - id : 1 ip : 10.10.13.10 host : host3 - id : 2 ip : 10.10.13.11 host : host4 - id : 3 ip : 10.10.13.12 host : host5 worker : instances : - id : 1 ip : 10.10.13.20 host : host3 dataDisks : - name : rook pool : data-pool size : 512 - id : 2 ip : 10.10.13.21 host : host4 dataDisks : - name : rook pool : data-pool size : 512 - id : 3 ip : 10.10.13.22 host : host5 dataDisks : - name : rook pool : data-pool size : 512 kubernetes : version : v1.23.7 kubespray : version : v2.20.0","title":"Step 4.1 (Optional): Data disks configuration"},{"location":"examples/ha-cluster/#step-5-applying-the-configuration","text":"Apply the cluster configuration. kubitect apply --config ha.yaml","title":"Step 5: Applying the configuration"},{"location":"examples/multi-master-cluster/","text":"Multi-master cluster \ud83d\udd17\ufe0e This example shows how to use Kubitect to set up a Kubernetes cluster with 3 master and 3 worker nodes . Configuring multiple master nodes provides control plane redundancy, meaning that the control plane can continue to operate normally if a certain number of master nodes fail. Since Kubitect deploys clusters with a stacked control plane (see Kubernetes.io - Stacked etcd topology for more information), this means that there is no downtime as long as (n/2)+1 master nodes are available. Note In this example we skip the explanation of some common configurations (hosts, network, node template, ...), as they are already explained in the Getting started (step-by-step) guide. Step 1: Cluster configuration \ud83d\udd17\ufe0e Each worker node stores only a single control plane IP address. Therefore, when creating clusters with multiple master nodes, we need to make sure that all of them are reachable at the same IP address, otherwise all workers would send requests to only one of the master nodes. This problem can be solved by placing a load balancer in front of the control plane, and instructing it to distribute traffic to all master nodes in the cluster, as shown in the figure below. To create such cluster, all we need to do, is specify desired node instances and one load balancer. Control plane will be accessible on the load balancer's IP address. multi-master.yaml cluster : ... nodes : loadBalancer : instances : - id : 1 ip : 192.168.113.100 master : instances : # (1)! - id : 1 ip : 192.168.113.10 - id : 2 ip : 192.168.113.11 - id : 3 ip : 192.168.113.12 worker : instances : - id : 1 ip : 192.168.113.20 - id : 2 ip : 192.168.113.21 - id : 3 ip : 192.168.113.22 Size of the control plane (number of master nodes) must be odd. Kubitect detects the load balancer instance in the configuration and installs the HAProxy load balancer on an additional virtual machine. By default, the load balancer is configured to distribute traffic received on port 6443 (Kubernetes API server port) to all specified master nodes. Final cluster configuration multi-master.yaml hosts : - name : localhost connection : type : local cluster : name : local-k8s-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : loadBalancer : instances : - id : 1 ip : 192.168.113.100 master : instances : - id : 1 ip : 192.168.113.10 - id : 2 ip : 192.168.113.11 - id : 3 ip : 192.168.113.12 worker : instances : - id : 1 ip : 192.168.113.20 - id : 2 ip : 192.168.113.21 - id : 3 ip : 192.168.113.22 kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : version : v2.20.0 Step 2: Applying the configuration \ud83d\udd17\ufe0e Apply the cluster: kubitect apply --config multi-master.yaml","title":"Multi-master cluster"},{"location":"examples/multi-master-cluster/#multi-master-cluster","text":"This example shows how to use Kubitect to set up a Kubernetes cluster with 3 master and 3 worker nodes . Configuring multiple master nodes provides control plane redundancy, meaning that the control plane can continue to operate normally if a certain number of master nodes fail. Since Kubitect deploys clusters with a stacked control plane (see Kubernetes.io - Stacked etcd topology for more information), this means that there is no downtime as long as (n/2)+1 master nodes are available. Note In this example we skip the explanation of some common configurations (hosts, network, node template, ...), as they are already explained in the Getting started (step-by-step) guide.","title":"Multi-master cluster"},{"location":"examples/multi-master-cluster/#step-1-cluster-configuration","text":"Each worker node stores only a single control plane IP address. Therefore, when creating clusters with multiple master nodes, we need to make sure that all of them are reachable at the same IP address, otherwise all workers would send requests to only one of the master nodes. This problem can be solved by placing a load balancer in front of the control plane, and instructing it to distribute traffic to all master nodes in the cluster, as shown in the figure below. To create such cluster, all we need to do, is specify desired node instances and one load balancer. Control plane will be accessible on the load balancer's IP address. multi-master.yaml cluster : ... nodes : loadBalancer : instances : - id : 1 ip : 192.168.113.100 master : instances : # (1)! - id : 1 ip : 192.168.113.10 - id : 2 ip : 192.168.113.11 - id : 3 ip : 192.168.113.12 worker : instances : - id : 1 ip : 192.168.113.20 - id : 2 ip : 192.168.113.21 - id : 3 ip : 192.168.113.22 Size of the control plane (number of master nodes) must be odd. Kubitect detects the load balancer instance in the configuration and installs the HAProxy load balancer on an additional virtual machine. By default, the load balancer is configured to distribute traffic received on port 6443 (Kubernetes API server port) to all specified master nodes. Final cluster configuration multi-master.yaml hosts : - name : localhost connection : type : local cluster : name : local-k8s-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : loadBalancer : instances : - id : 1 ip : 192.168.113.100 master : instances : - id : 1 ip : 192.168.113.10 - id : 2 ip : 192.168.113.11 - id : 3 ip : 192.168.113.12 worker : instances : - id : 1 ip : 192.168.113.20 - id : 2 ip : 192.168.113.21 - id : 3 ip : 192.168.113.22 kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : version : v2.20.0","title":"Step 1: Cluster configuration"},{"location":"examples/multi-master-cluster/#step-2-applying-the-configuration","text":"Apply the cluster: kubitect apply --config multi-master.yaml","title":"Step 2: Applying the configuration"},{"location":"examples/multi-worker-cluster/","text":"Multi-worker cluster \ud83d\udd17\ufe0e This example shows how to use Kubitect to set up a Kubernetes cluster with one master and three worker nodes . Note In this example we skip the explanation of some common configurations (hosts, network, node template, ...), as they are already explained in the Getting started (step-by-step) guide. Step 1: Cluster configuration \ud83d\udd17\ufe0e To create a cluster with multiple workers, simply specify multiple worker nodes in the configuration. In this particular case, we want to have 3 worker nodes, but there can be as many as you want. multi-worker.yaml cluster : ... nodes : master : instances : - id : 1 ip : 192.168.113.10 # (1)! worker : instances : - id : 1 ip : 192.168.113.21 - id : 7 ip : 192.168.113.27 - id : 99 Static IP address of the node. If the ip property is omitted, the DHCP lease is requested when the cluster is created. Final cluster configuration multi-worker.yaml hosts : - name : localhost connection : type : local cluster : name : local-k8s-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : master : instances : - id : 1 ip : 192.168.113.10 worker : instances : - id : 1 ip : 192.168.113.21 - id : 7 ip : 192.168.113.27 - id : 99 kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : version : v2.20.0 Step 2: Applying the configuration \ud83d\udd17\ufe0e Apply the cluster: kubitect apply --config multi-worker.yaml","title":"Multi-worker cluster"},{"location":"examples/multi-worker-cluster/#multi-worker-cluster","text":"This example shows how to use Kubitect to set up a Kubernetes cluster with one master and three worker nodes . Note In this example we skip the explanation of some common configurations (hosts, network, node template, ...), as they are already explained in the Getting started (step-by-step) guide.","title":"Multi-worker cluster"},{"location":"examples/multi-worker-cluster/#step-1-cluster-configuration","text":"To create a cluster with multiple workers, simply specify multiple worker nodes in the configuration. In this particular case, we want to have 3 worker nodes, but there can be as many as you want. multi-worker.yaml cluster : ... nodes : master : instances : - id : 1 ip : 192.168.113.10 # (1)! worker : instances : - id : 1 ip : 192.168.113.21 - id : 7 ip : 192.168.113.27 - id : 99 Static IP address of the node. If the ip property is omitted, the DHCP lease is requested when the cluster is created. Final cluster configuration multi-worker.yaml hosts : - name : localhost connection : type : local cluster : name : local-k8s-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : master : instances : - id : 1 ip : 192.168.113.10 worker : instances : - id : 1 ip : 192.168.113.21 - id : 7 ip : 192.168.113.27 - id : 99 kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : version : v2.20.0","title":"Step 1: Cluster configuration"},{"location":"examples/multi-worker-cluster/#step-2-applying-the-configuration","text":"Apply the cluster: kubitect apply --config multi-worker.yaml","title":"Step 2: Applying the configuration"},{"location":"examples/network-bridge/","text":"Network bridge \ud83d\udd17\ufe0e Bridged networks allow virtual machines to connect directly to the LAN. To use Kubitect with bridged network mode, a bridge interface must be preconfigured on the host machine. This example shows how to configure a simple bridge interface using Netplan . Step 1 - (Pre)configure the bridge on the host \ud83d\udd17\ufe0e Before the network bridge can be created, a name of the host's network interface is required. This interface will be used by the bridge. To print the available network interfaces of the host, use the following command. nmcli device | grep ethernet Similarly to the previous command, network interfaces can be printed using ifconfig or ip commands. Note that these commands output all interfaces, including virtual ones. ifconfig -a # or ip a Once you have obtained the name of the host's network interface (in our case eth0 ), you can create a bridge interface (in our case br0 ) by creating a file with the following content: /etc/netplan/bridge0.yaml network : version : 2 renderer : networkd ethernets : eth0 : {} # (1)! bridges : br0 : # (2)! interfaces : - eth0 dhcp4 : true dhcp6 : false addresses : # (3)! - 10.10.0.17 Existing host's ethernet interface to be enslaved. Custom name of the bridge interface. Optionally a static IP address can be set for the bridge interface. Tip See the official Netplan configuration examples for more advance configurations. Validate if the configuration is correctly parsed by Netplan. sudo netplan generate Apply the configuration. sudo netplan apply Step 2 - Disable netfilter on the host \ud83d\udd17\ufe0e The final step is to prevent packets traversing the bridge from being sent to iptables for processing. cat >> /etc/sysctl.conf <<EOF net.bridge.bridge-nf-call-ip6tables = 0 net.bridge.bridge-nf-call-iptables = 0 net.bridge.bridge-nf-call-arptables = 0 EOF sysctl -p /etc/sysctl.conf Tip For more information, see the libvirt documentation . Step 3 - Set up a cluster over bridged network \ud83d\udd17\ufe0e In the cluster configuration file, set the following variables: cluster.network.mode to bridge , cluster.network.cidr to the network CIDR of the LAN and cluster.network.bridge to the name of the bridge you have created ( br0 in our case) cluster : network : mode : bridge cidr : 10.10.13.0/24 bridge : br0 ...","title":"Network bridge"},{"location":"examples/network-bridge/#network-bridge","text":"Bridged networks allow virtual machines to connect directly to the LAN. To use Kubitect with bridged network mode, a bridge interface must be preconfigured on the host machine. This example shows how to configure a simple bridge interface using Netplan .","title":"Network bridge"},{"location":"examples/network-bridge/#step-1-preconfigure-the-bridge-on-the-host","text":"Before the network bridge can be created, a name of the host's network interface is required. This interface will be used by the bridge. To print the available network interfaces of the host, use the following command. nmcli device | grep ethernet Similarly to the previous command, network interfaces can be printed using ifconfig or ip commands. Note that these commands output all interfaces, including virtual ones. ifconfig -a # or ip a Once you have obtained the name of the host's network interface (in our case eth0 ), you can create a bridge interface (in our case br0 ) by creating a file with the following content: /etc/netplan/bridge0.yaml network : version : 2 renderer : networkd ethernets : eth0 : {} # (1)! bridges : br0 : # (2)! interfaces : - eth0 dhcp4 : true dhcp6 : false addresses : # (3)! - 10.10.0.17 Existing host's ethernet interface to be enslaved. Custom name of the bridge interface. Optionally a static IP address can be set for the bridge interface. Tip See the official Netplan configuration examples for more advance configurations. Validate if the configuration is correctly parsed by Netplan. sudo netplan generate Apply the configuration. sudo netplan apply","title":"Step 1 - (Pre)configure the bridge on the host"},{"location":"examples/network-bridge/#step-2-disable-netfilter-on-the-host","text":"The final step is to prevent packets traversing the bridge from being sent to iptables for processing. cat >> /etc/sysctl.conf <<EOF net.bridge.bridge-nf-call-ip6tables = 0 net.bridge.bridge-nf-call-iptables = 0 net.bridge.bridge-nf-call-arptables = 0 EOF sysctl -p /etc/sysctl.conf Tip For more information, see the libvirt documentation .","title":"Step 2 - Disable netfilter on the host"},{"location":"examples/network-bridge/#step-3-set-up-a-cluster-over-bridged-network","text":"In the cluster configuration file, set the following variables: cluster.network.mode to bridge , cluster.network.cidr to the network CIDR of the LAN and cluster.network.bridge to the name of the bridge you have created ( br0 in our case) cluster : network : mode : bridge cidr : 10.10.13.0/24 bridge : br0 ...","title":"Step 3 - Set up a cluster over bridged network"},{"location":"examples/rook-cluster/","text":"Rook cluster \ud83d\udd17\ufe0e Important Since the Rook addon is still under development, it may not work as expected. Therefore, any feedback would be greatly appreciated. This example shows how to use Kubitect to set up distributed storage with Rook . For distributed storage, we add an additional data disk to each virtual machine as shown on the figure below. Basic setup \ud83d\udd17\ufe0e Step 1: Define data resource pool \ud83d\udd17\ufe0e To configure distributed storage with Rook, the data disks must be attached to the virtual machines. By default, each data disk is created in a main resource pool. Optionally, you can configure additional resource pools and associate data disks with them later. In this example, we define an additional resource pool named 'rook-pool'. rook-sample.yaml hosts : - name : localhost connection : type : local dataResourcePools : - name : rook-pool Step 2: Attach data disks \ud83d\udd17\ufe0e After the data resource pool is configured, we are ready to allocate some data disks to the virtual machines. rook-sample.yaml cluster : nodes : worker : instances : - id : 1 dataDisks : - name : rook pool : rook-pool # (1)! size : 256 - id : 2 dataDisks : - name : rook pool : rook-pool size : 256 - id : 3 - id : 4 dataDisks : - name : rook pool : rook-pool size : 256 - name : test pool : rook-pool size : 32 To create data disks in the main resource pool, either omit the pool property or set its value to main . Step 3: Enable Rook addon \ud83d\udd17\ufe0e Once the disks are configured, you only need to activate the Rook addon. rook-sample.yaml addons : rook : enabled : true By default, Rook resources are provisioned on all worker nodes (without any constraints). This behavior can be restricted with node selectors. Final cluster configuration rook-sample.yaml hosts : - name : localhost connection : type : local dataResourcePools : - name : rook-pool cluster : name : rook-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : master : instances : - id : 1 worker : instances : - id : 1 dataDisks : - name : rook pool : rook-pool size : 256 - id : 2 dataDisks : - name : rook pool : rook-pool size : 256 - id : 3 - id : 4 dataDisks : - name : rook pool : rook-pool size : 256 - name : test pool : rook-pool size : 32 kubernetes : version : v1.23.7 kubespray : version : v2.20.0 addons : rook : enabled : true Step 4: Apply the configuration \ud83d\udd17\ufe0e kubitect apply --config rook-sample.yaml Node selector \ud83d\udd17\ufe0e The node selector is a dictionary of labels and their potential values. The node selector restricts on which nodes Rook can be deployed, by selecting only those nodes that match all the specified labels. Step 1: Set node labels \ud83d\udd17\ufe0e To use the node selector effectively, you should give your nodes custom labels. In this example, we label all worker nodes with the label rook . To ensure that scaling the cluster does not subsequently affect Rook, we set label's value to false by default. Only the nodes where Rook should be deployed are labeled rook : true , as shown in the figure below. The following configuration snippet shows how to set a default label and override it for a particular instance. rook-sample.yaml cluster : nodes : worker : default : labels : rook : false instances : - id : 1 labels : rook : true # (1)! - id : 2 labels : rook : true - id : 3 labels : rook : true - id : 4 By default, the label rook : false is set for all worker nodes. Setting the label rook : true for this particular instance overrides the default label. Step 2: Configure a node selector \ud83d\udd17\ufe0e So far we have labeled all worker nodes, but labeling is not enough to prevent Rook from being deployed on all worker nodes. To restrict on which nodes Rook resources can be deployed, we need to configure a node selector. We want to deploy Rook on the nodes labeled with the label rook : true , as shown in the figure below. The following configuration snippet shows how to configure the node selector mentioned above. rook-sample.yaml addons : rook : enabled : true nodeSelector : rook : true Final cluster configuration rook-sample.yaml hosts : - name : localhost connection : type : local dataResourcePools : - name : rook-pool cluster : name : rook-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : master : instances : - id : 1 worker : default : labels : rook : false instances : - id : 1 labels : rook : true dataDisks : - name : rook pool : rook-pool size : 256 - id : 2 labels : rook : true dataDisks : - name : rook pool : rook-pool size : 256 - id : 3 labels : rook : true - id : 4 dataDisks : - name : rook pool : rook-pool size : 256 - name : test pool : rook-pool size : 32 kubernetes : version : v1.23.7 kubespray : version : v2.20.0 addons : rook : enabled : true nodeSelector : rook : true Step 3: Apply the configuration \ud83d\udd17\ufe0e kubitect apply --config rook-sample.yaml","title":"Rook cluster"},{"location":"examples/rook-cluster/#rook-cluster","text":"Important Since the Rook addon is still under development, it may not work as expected. Therefore, any feedback would be greatly appreciated. This example shows how to use Kubitect to set up distributed storage with Rook . For distributed storage, we add an additional data disk to each virtual machine as shown on the figure below.","title":"Rook cluster"},{"location":"examples/rook-cluster/#basic-setup","text":"","title":"Basic setup"},{"location":"examples/rook-cluster/#step-1-define-data-resource-pool","text":"To configure distributed storage with Rook, the data disks must be attached to the virtual machines. By default, each data disk is created in a main resource pool. Optionally, you can configure additional resource pools and associate data disks with them later. In this example, we define an additional resource pool named 'rook-pool'. rook-sample.yaml hosts : - name : localhost connection : type : local dataResourcePools : - name : rook-pool","title":"Step 1: Define data resource pool"},{"location":"examples/rook-cluster/#step-2-attach-data-disks","text":"After the data resource pool is configured, we are ready to allocate some data disks to the virtual machines. rook-sample.yaml cluster : nodes : worker : instances : - id : 1 dataDisks : - name : rook pool : rook-pool # (1)! size : 256 - id : 2 dataDisks : - name : rook pool : rook-pool size : 256 - id : 3 - id : 4 dataDisks : - name : rook pool : rook-pool size : 256 - name : test pool : rook-pool size : 32 To create data disks in the main resource pool, either omit the pool property or set its value to main .","title":"Step 2: Attach data disks"},{"location":"examples/rook-cluster/#step-3-enable-rook-addon","text":"Once the disks are configured, you only need to activate the Rook addon. rook-sample.yaml addons : rook : enabled : true By default, Rook resources are provisioned on all worker nodes (without any constraints). This behavior can be restricted with node selectors. Final cluster configuration rook-sample.yaml hosts : - name : localhost connection : type : local dataResourcePools : - name : rook-pool cluster : name : rook-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : master : instances : - id : 1 worker : instances : - id : 1 dataDisks : - name : rook pool : rook-pool size : 256 - id : 2 dataDisks : - name : rook pool : rook-pool size : 256 - id : 3 - id : 4 dataDisks : - name : rook pool : rook-pool size : 256 - name : test pool : rook-pool size : 32 kubernetes : version : v1.23.7 kubespray : version : v2.20.0 addons : rook : enabled : true","title":"Step 3: Enable Rook addon"},{"location":"examples/rook-cluster/#step-4-apply-the-configuration","text":"kubitect apply --config rook-sample.yaml","title":"Step 4: Apply the configuration"},{"location":"examples/rook-cluster/#node-selector","text":"The node selector is a dictionary of labels and their potential values. The node selector restricts on which nodes Rook can be deployed, by selecting only those nodes that match all the specified labels.","title":"Node selector"},{"location":"examples/rook-cluster/#step-1-set-node-labels","text":"To use the node selector effectively, you should give your nodes custom labels. In this example, we label all worker nodes with the label rook . To ensure that scaling the cluster does not subsequently affect Rook, we set label's value to false by default. Only the nodes where Rook should be deployed are labeled rook : true , as shown in the figure below. The following configuration snippet shows how to set a default label and override it for a particular instance. rook-sample.yaml cluster : nodes : worker : default : labels : rook : false instances : - id : 1 labels : rook : true # (1)! - id : 2 labels : rook : true - id : 3 labels : rook : true - id : 4 By default, the label rook : false is set for all worker nodes. Setting the label rook : true for this particular instance overrides the default label.","title":"Step 1: Set node labels"},{"location":"examples/rook-cluster/#step-2-configure-a-node-selector","text":"So far we have labeled all worker nodes, but labeling is not enough to prevent Rook from being deployed on all worker nodes. To restrict on which nodes Rook resources can be deployed, we need to configure a node selector. We want to deploy Rook on the nodes labeled with the label rook : true , as shown in the figure below. The following configuration snippet shows how to configure the node selector mentioned above. rook-sample.yaml addons : rook : enabled : true nodeSelector : rook : true Final cluster configuration rook-sample.yaml hosts : - name : localhost connection : type : local dataResourcePools : - name : rook-pool cluster : name : rook-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : master : instances : - id : 1 worker : default : labels : rook : false instances : - id : 1 labels : rook : true dataDisks : - name : rook pool : rook-pool size : 256 - id : 2 labels : rook : true dataDisks : - name : rook pool : rook-pool size : 256 - id : 3 labels : rook : true - id : 4 dataDisks : - name : rook pool : rook-pool size : 256 - name : test pool : rook-pool size : 32 kubernetes : version : v1.23.7 kubespray : version : v2.20.0 addons : rook : enabled : true nodeSelector : rook : true","title":"Step 2: Configure a node selector"},{"location":"examples/rook-cluster/#step-3-apply-the-configuration","text":"kubitect apply --config rook-sample.yaml","title":"Step 3: Apply the configuration"},{"location":"examples/single-node-cluster/","text":"Single node cluster \ud83d\udd17\ufe0e This example shows how to setup a single node Kubernetes cluster using Kubitect. Note In this example we skip the explanation of some common configurations (hosts, network, node template, ...), as they are already explained in the Getting started (step-by-step) guide. Step 1: Create the configuration \ud83d\udd17\ufe0e If you want to initialize a cluster with only one node, specify a single master node in the cluster configuration file: single-node.yaml cluster : ... nodes : master : instances : - id : 1 ip : 192.168.113.10 # (1)! Static IP address of the node. If the ip property is omitted, the DHCP lease is requested when the cluster is created. Final cluster configuration single-node.yaml hosts : - name : localhost connection : type : local cluster : name : local-k8s-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : master : default : ram : 4 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.10 kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : version : v2.20.0 Step 2: Applying the configuration \ud83d\udd17\ufe0e Apply the cluster: kubitect apply --config single-node.yaml Your master node now also becomes a worker node.","title":"Single node cluster"},{"location":"examples/single-node-cluster/#single-node-cluster","text":"This example shows how to setup a single node Kubernetes cluster using Kubitect. Note In this example we skip the explanation of some common configurations (hosts, network, node template, ...), as they are already explained in the Getting started (step-by-step) guide.","title":"Single node cluster"},{"location":"examples/single-node-cluster/#step-1-create-the-configuration","text":"If you want to initialize a cluster with only one node, specify a single master node in the cluster configuration file: single-node.yaml cluster : ... nodes : master : instances : - id : 1 ip : 192.168.113.10 # (1)! Static IP address of the node. If the ip property is omitted, the DHCP lease is requested when the cluster is created. Final cluster configuration single-node.yaml hosts : - name : localhost connection : type : local cluster : name : local-k8s-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu nodes : master : default : ram : 4 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.10 kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : version : v2.20.0","title":"Step 1: Create the configuration"},{"location":"examples/single-node-cluster/#step-2-applying-the-configuration","text":"Apply the cluster: kubitect apply --config single-node.yaml Your master node now also becomes a worker node.","title":"Step 2: Applying the configuration"},{"location":"getting-started/getting-started/","text":"Getting Started \ud83d\udd17\ufe0e In this step-by-step guide, you will learn how to prepare a custom cluster configuration file from scratch and use it to create a functional Kubernetes cluster consisting of a one master and one worker node . Step 1 - Make sure all requirements are satisfied \ud83d\udd17\ufe0e For the successful installation of the Kubernetes cluster, some requirements must be met. Step 2 - Create cluster configuration file \ud83d\udd17\ufe0e In the quick start you have created a very basic Kubernetes cluster from predefined cluster configuration file. If configuration is not explicitly provided to the command-line tool using --config option, default cluster configuration file is used ( /examples/default-cluster.yaml ). Now it's time to create your own cluster topology. Before you begin create a new yaml file. touch kubitect.yaml Step 3 - Prepare hosts configuration \ud83d\udd17\ufe0e In the cluster configuration file, we will first define hosts. Hosts represent target servers. A host can be either a local or a remote machine. Localhost Remote host If the cluster is set up on the same machine where the command line tool is installed, we specify a host whose connection type is set to local . kubitect.yaml hosts : - name : localhost # (1)! connection : type : local Custom unique name of the host. When cluster is deployed on the remote machine, the IP address of the remote machine along with the SSH credentails needs to be specified for the host. kubitect.yaml hosts : - name : my-remote-host connection : type : remote user : myuser ip : 10.10.40.143 # (1)! ssh : keyfile : \"~/.ssh/id_rsa_server1\" # (2)! IP address of the remote host. Path to the password-less SSH key file required for establishing connection with the remote host. In this tutorial we will use only localhost. Step 4 - Define cluster infrastructure \ud83d\udd17\ufe0e The second part of the configuration file consists of the cluster infrastructure. In this part, all virtual machines are defined along with their properties such as operating system, CPU cores, amount of RAM and so on. For easier interpretation of the components that the final cluster will be made of, see the below image. Let's take a look at the following configuration: kubitect.yaml cluster : name : local-k8s-cluster network : ... nodeTemplate : ... nodes : ... We can see that the infrastructure configuration consists of the cluster name and 3 subsections: cluster.name is a cluster name that is used as a prefix for each resource created by Kubitect. cluster.network holds information about the network properties of the cluster. cluster.nodeTemplate contains properties that apply to all our nodes. For example, properties like operating system, SSH user, and SSH private key are the same for all our nodes. cluster.nodes subsection defines each node in our cluster. Now that we have a general idea about the infrastructure configuration, we can look at each of these subsections in more detail. Step 4.1 - Cluster network \ud83d\udd17\ufe0e The cluster network subsection defines the network that our cluster will use. Currently, two network modes are supported - NAT and bridge. The nat network mode instructs Kubitect to create a virtual network that performs network address translation. This mode allows the use of IP address ranges that do not exist within our local area network (LAN). The bridge network mode instructs Kubitect to use a predefined bridge interface. In this mode, virtual machines can connect directly to LAN. Use of this mode is mandatory when a cluster spreads over multipe hosts. To keep this tutorial as simple as possible, we will use the NAT mode, as it does not require a preconfigured bridge interface. kubitect.yaml cluster : ... network : mode : nat cidr : 192.168.113.0/24 The above configuration will instruct Kubitect to create a virtual network that uses 192.168.113.0/24 IP range. Step 4.2 - Node template \ud83d\udd17\ufe0e As mentioned earlier, the nodeTemplate subsection is used to define general properties of our nodes. Required properties are: user - the name of the user that will be created on all virtual machines and will also be used for SSH. Besides the required properties, there are some potentially useful properties: os.distro - defines the operating system for the nodes (currently ubuntu and debian are supported). By default, latest Ubuntu 22.04 release is used. ssh.addToKnownHosts - if true, all virtual machines will be added to SSH known hosts. If you later destroy the cluster, these virtual machines will also be removed from the known hosts. updateOnBoot - if true, all virtual machines are updated at the first boot. Our noteTemplate subsection now looks like this: kubitect.yaml cluster : ... nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu22 Step 4.3 - Cluster nodes \ud83d\udd17\ufe0e In the nodes subsection, we can define three types of nodes: loadBalancer nodes are internal load balancers used to expose the Kubernetes control plane at a single endpoint. master nodes are Kubernetes master nodes that also contain an etcd key-value store. Since etcd is present on these nodes, the number of master nodes must be odd. For more information, see etcd FAQ . worker nodes are the nodes on which your actual workload runs. In this tutorial, we will use only one master node, so internal load balancers are not required. The easiest way to explain this part is to look at the actual configuration: kubitect.yaml cluster : ... nodes : master : default : # (1)! ram : 4 # (2)! cpu : 2 # (3)! mainDiskSize : 32 # (4)! instances : # (5)! - id : 1 # (6)! ip : 192.168.113.10 # (7)! worker : default : ram : 8 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.21 ram : 4 # (8)! Default properties are applied to all nodes of the same type (in this case master nodes). They are especially useful, when multiple nodes of the same type are specified. Amount of RAM allocated to the master nodes (in GiB). Amount of vCPU allocated to the master nodes. Size of the virtual disk attached to each master node (in GiB). List of master node instances. Instance ID is the only required field that must be specified for each instance. Static IP address set for this particular instance. If the ip property is omitted, the DHCP lease is requested when the cluster is created. Since the amount of RAM (4 GiB) is specified for this particular instance, the default value (8 GiB) is overwritten. Step 4.4 - Kubernetes properties \ud83d\udd17\ufe0e The last part of the cluster configuration consists of the Kubernetes properties. In this section we define the Kubernetes version, the DNS plugin and so on. It is also important to check if Kubespray supports a specific Kubernetes version. If you are using a custom Kubespray, you can also specify the URL to a custom Git repository. kubitect.yaml kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : version : v2.20.0 Step 5 - Create the cluster \ud83d\udd17\ufe0e Tip If you encounter any issues during the installation, please refer to the troubleshooting page first. Our final cluster configuration looks like this: kubitect.yaml hosts : - name : localhost connection : type : local cluster : name : local-k8s-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu22 nodes : master : default : ram : 4 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.10 worker : default : ram : 8 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.21 ram : 4 kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : version : v2.20.0 Now create the cluster by applying your custom configuration using the Kubitect command line tool. Also, let's name our cluster my-first-cluster . kubitect apply --cluster my-first-cluster --config kubitect.yaml When the configuration is applied, the Terraform plan shows the changes Terraform wants to make to your infrastructure. User confirmation of the plan is required before Kubitect begins creating the cluster. Tip To skip the user confirmation step, the flag --auto-approve can be used. When the cluster is applied, it is created in Kubitect's home directory, which has the following structure. ~/.kubitect \u251c\u2500\u2500 clusters \u2502 \u251c\u2500\u2500 default \u2502 \u251c\u2500\u2500 my-first-cluster \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 bin \u2514\u2500\u2500 ... All created clusters can be listed at any time using the following command. kubitect list clusters # Clusters: # - my-first-cluster (active) Step 6 - Test the cluster \ud83d\udd17\ufe0e After successful installation of the Kubernetes cluster, Kubeconfig is created in the cluster's directory. To export the Kubeconfig to a separate file, run the following command. kubitect export kubeconfig --cluster my-first-cluster > kubeconfig.yaml Use the exported Kubeconfig to list all cluster nodes. kubectl get nodes --kubeconfig kubeconfig.yaml Congratulations, you have completed the getting started tutorial.","title":"Getting started (step-by-step)"},{"location":"getting-started/getting-started/#getting-started","text":"In this step-by-step guide, you will learn how to prepare a custom cluster configuration file from scratch and use it to create a functional Kubernetes cluster consisting of a one master and one worker node .","title":"Getting Started"},{"location":"getting-started/getting-started/#step-1-make-sure-all-requirements-are-satisfied","text":"For the successful installation of the Kubernetes cluster, some requirements must be met.","title":"Step 1 - Make sure all requirements are satisfied"},{"location":"getting-started/getting-started/#step-2-create-cluster-configuration-file","text":"In the quick start you have created a very basic Kubernetes cluster from predefined cluster configuration file. If configuration is not explicitly provided to the command-line tool using --config option, default cluster configuration file is used ( /examples/default-cluster.yaml ). Now it's time to create your own cluster topology. Before you begin create a new yaml file. touch kubitect.yaml","title":"Step 2 - Create cluster configuration file"},{"location":"getting-started/getting-started/#step-3-prepare-hosts-configuration","text":"In the cluster configuration file, we will first define hosts. Hosts represent target servers. A host can be either a local or a remote machine. Localhost Remote host If the cluster is set up on the same machine where the command line tool is installed, we specify a host whose connection type is set to local . kubitect.yaml hosts : - name : localhost # (1)! connection : type : local Custom unique name of the host. When cluster is deployed on the remote machine, the IP address of the remote machine along with the SSH credentails needs to be specified for the host. kubitect.yaml hosts : - name : my-remote-host connection : type : remote user : myuser ip : 10.10.40.143 # (1)! ssh : keyfile : \"~/.ssh/id_rsa_server1\" # (2)! IP address of the remote host. Path to the password-less SSH key file required for establishing connection with the remote host. In this tutorial we will use only localhost.","title":"Step 3 - Prepare hosts configuration"},{"location":"getting-started/getting-started/#step-4-define-cluster-infrastructure","text":"The second part of the configuration file consists of the cluster infrastructure. In this part, all virtual machines are defined along with their properties such as operating system, CPU cores, amount of RAM and so on. For easier interpretation of the components that the final cluster will be made of, see the below image. Let's take a look at the following configuration: kubitect.yaml cluster : name : local-k8s-cluster network : ... nodeTemplate : ... nodes : ... We can see that the infrastructure configuration consists of the cluster name and 3 subsections: cluster.name is a cluster name that is used as a prefix for each resource created by Kubitect. cluster.network holds information about the network properties of the cluster. cluster.nodeTemplate contains properties that apply to all our nodes. For example, properties like operating system, SSH user, and SSH private key are the same for all our nodes. cluster.nodes subsection defines each node in our cluster. Now that we have a general idea about the infrastructure configuration, we can look at each of these subsections in more detail.","title":"Step 4 - Define cluster infrastructure"},{"location":"getting-started/getting-started/#step-41-cluster-network","text":"The cluster network subsection defines the network that our cluster will use. Currently, two network modes are supported - NAT and bridge. The nat network mode instructs Kubitect to create a virtual network that performs network address translation. This mode allows the use of IP address ranges that do not exist within our local area network (LAN). The bridge network mode instructs Kubitect to use a predefined bridge interface. In this mode, virtual machines can connect directly to LAN. Use of this mode is mandatory when a cluster spreads over multipe hosts. To keep this tutorial as simple as possible, we will use the NAT mode, as it does not require a preconfigured bridge interface. kubitect.yaml cluster : ... network : mode : nat cidr : 192.168.113.0/24 The above configuration will instruct Kubitect to create a virtual network that uses 192.168.113.0/24 IP range.","title":"Step 4.1 - Cluster network"},{"location":"getting-started/getting-started/#step-42-node-template","text":"As mentioned earlier, the nodeTemplate subsection is used to define general properties of our nodes. Required properties are: user - the name of the user that will be created on all virtual machines and will also be used for SSH. Besides the required properties, there are some potentially useful properties: os.distro - defines the operating system for the nodes (currently ubuntu and debian are supported). By default, latest Ubuntu 22.04 release is used. ssh.addToKnownHosts - if true, all virtual machines will be added to SSH known hosts. If you later destroy the cluster, these virtual machines will also be removed from the known hosts. updateOnBoot - if true, all virtual machines are updated at the first boot. Our noteTemplate subsection now looks like this: kubitect.yaml cluster : ... nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu22","title":"Step 4.2 - Node template"},{"location":"getting-started/getting-started/#step-43-cluster-nodes","text":"In the nodes subsection, we can define three types of nodes: loadBalancer nodes are internal load balancers used to expose the Kubernetes control plane at a single endpoint. master nodes are Kubernetes master nodes that also contain an etcd key-value store. Since etcd is present on these nodes, the number of master nodes must be odd. For more information, see etcd FAQ . worker nodes are the nodes on which your actual workload runs. In this tutorial, we will use only one master node, so internal load balancers are not required. The easiest way to explain this part is to look at the actual configuration: kubitect.yaml cluster : ... nodes : master : default : # (1)! ram : 4 # (2)! cpu : 2 # (3)! mainDiskSize : 32 # (4)! instances : # (5)! - id : 1 # (6)! ip : 192.168.113.10 # (7)! worker : default : ram : 8 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.21 ram : 4 # (8)! Default properties are applied to all nodes of the same type (in this case master nodes). They are especially useful, when multiple nodes of the same type are specified. Amount of RAM allocated to the master nodes (in GiB). Amount of vCPU allocated to the master nodes. Size of the virtual disk attached to each master node (in GiB). List of master node instances. Instance ID is the only required field that must be specified for each instance. Static IP address set for this particular instance. If the ip property is omitted, the DHCP lease is requested when the cluster is created. Since the amount of RAM (4 GiB) is specified for this particular instance, the default value (8 GiB) is overwritten.","title":"Step 4.3 - Cluster nodes"},{"location":"getting-started/getting-started/#step-44-kubernetes-properties","text":"The last part of the cluster configuration consists of the Kubernetes properties. In this section we define the Kubernetes version, the DNS plugin and so on. It is also important to check if Kubespray supports a specific Kubernetes version. If you are using a custom Kubespray, you can also specify the URL to a custom Git repository. kubitect.yaml kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : version : v2.20.0","title":"Step 4.4 - Kubernetes properties"},{"location":"getting-started/getting-started/#step-5-create-the-cluster","text":"Tip If you encounter any issues during the installation, please refer to the troubleshooting page first. Our final cluster configuration looks like this: kubitect.yaml hosts : - name : localhost connection : type : local cluster : name : local-k8s-cluster network : mode : nat cidr : 192.168.113.0/24 nodeTemplate : user : k8s updateOnBoot : true ssh : addToKnownHosts : true os : distro : ubuntu22 nodes : master : default : ram : 4 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.10 worker : default : ram : 8 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.21 ram : 4 kubernetes : version : v1.23.7 networkPlugin : calico dnsMode : coredns kubespray : version : v2.20.0 Now create the cluster by applying your custom configuration using the Kubitect command line tool. Also, let's name our cluster my-first-cluster . kubitect apply --cluster my-first-cluster --config kubitect.yaml When the configuration is applied, the Terraform plan shows the changes Terraform wants to make to your infrastructure. User confirmation of the plan is required before Kubitect begins creating the cluster. Tip To skip the user confirmation step, the flag --auto-approve can be used. When the cluster is applied, it is created in Kubitect's home directory, which has the following structure. ~/.kubitect \u251c\u2500\u2500 clusters \u2502 \u251c\u2500\u2500 default \u2502 \u251c\u2500\u2500 my-first-cluster \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 bin \u2514\u2500\u2500 ... All created clusters can be listed at any time using the following command. kubitect list clusters # Clusters: # - my-first-cluster (active)","title":"Step 5 - Create the cluster"},{"location":"getting-started/getting-started/#step-6-test-the-cluster","text":"After successful installation of the Kubernetes cluster, Kubeconfig is created in the cluster's directory. To export the Kubeconfig to a separate file, run the following command. kubitect export kubeconfig --cluster my-first-cluster > kubeconfig.yaml Use the exported Kubeconfig to list all cluster nodes. kubectl get nodes --kubeconfig kubeconfig.yaml Congratulations, you have completed the getting started tutorial.","title":"Step 6 - Test the cluster"},{"location":"getting-started/installation/","text":"Installation \ud83d\udd17\ufe0e Before starting with installation, make sure you meet all the requirements . Install Kubitect CLI tool \ud83d\udd17\ufe0e After all requirements are met, download the Kubitect command line tool. curl -o kubitect.tar.gz -L https://dl.kubitect.io/linux/amd64/latest Note The download URL is a combination of the operating system type, system architecture and version of Kubitect ( https://dl.kubitect.io/<os>/<arch>/<version> ). All releases can be found on GitHub release page . Unpack tar.gz file. tar -xzf kubitect.tar.gz Install Kubitect command line tool by placing the Kubitect binary file in /usr/local/bin directory. sudo mv kubitect /usr/local/bin/ Verify the installation by checking the Kubitect version. kubitect --version # kubitect version v2.3.0 Tip If you are using Kubitect for the first time, we strongly recommend you to take a look at the getting started tutorial. Enable shell autocomplete \ud83d\udd17\ufe0e To load completions in your current shell session ( bash ): source < ( kubitect completion bash ) To load completions for every new session, execute once: kubitect completion bash > /etc/bash_completion.d/kubitect Tip For all supported shells run: kubitect completion -h For shell specific instructions run: kubitect completion shell -h","title":"Installation"},{"location":"getting-started/installation/#installation","text":"Before starting with installation, make sure you meet all the requirements .","title":"Installation"},{"location":"getting-started/installation/#install-kubitect-cli-tool","text":"After all requirements are met, download the Kubitect command line tool. curl -o kubitect.tar.gz -L https://dl.kubitect.io/linux/amd64/latest Note The download URL is a combination of the operating system type, system architecture and version of Kubitect ( https://dl.kubitect.io/<os>/<arch>/<version> ). All releases can be found on GitHub release page . Unpack tar.gz file. tar -xzf kubitect.tar.gz Install Kubitect command line tool by placing the Kubitect binary file in /usr/local/bin directory. sudo mv kubitect /usr/local/bin/ Verify the installation by checking the Kubitect version. kubitect --version # kubitect version v2.3.0 Tip If you are using Kubitect for the first time, we strongly recommend you to take a look at the getting started tutorial.","title":"Install Kubitect CLI tool"},{"location":"getting-started/installation/#enable-shell-autocomplete","text":"To load completions in your current shell session ( bash ): source < ( kubitect completion bash ) To load completions for every new session, execute once: kubitect completion bash > /etc/bash_completion.d/kubitect Tip For all supported shells run: kubitect completion -h For shell specific instructions run: kubitect completion shell -h","title":"Enable shell autocomplete"},{"location":"getting-started/introduction/","text":"","title":"Introduction"},{"location":"getting-started/quick-start/","text":"Quick start \ud83d\udd17\ufe0e Step 1 - Create the cluster \ud83d\udd17\ufe0e Run the following command to apply the default cluster configuration, which creates a cluster with one master and one worker node . Generated cluster configuration files will be stored in ~/.kubitect/clusters/default/ directory. kubitect apply Step 2 - Export kubeconfig \ud83d\udd17\ufe0e After successful installation of the Kubernetes cluster, Kubeconfig will be created within cluster's directory. To export the Kubeconfig into custom file run the following command. kubitect export kubeconfig > kubeconfig.yaml Step 3 - Test the cluster \ud83d\udd17\ufe0e Test if the cluster works by displaying all cluster nodes. kubectl get nodes --kubeconfig kubeconfig.yaml","title":"Quick start"},{"location":"getting-started/quick-start/#quick-start","text":"","title":"Quick start"},{"location":"getting-started/quick-start/#step-1-create-the-cluster","text":"Run the following command to apply the default cluster configuration, which creates a cluster with one master and one worker node . Generated cluster configuration files will be stored in ~/.kubitect/clusters/default/ directory. kubitect apply","title":"Step 1 - Create the cluster"},{"location":"getting-started/quick-start/#step-2-export-kubeconfig","text":"After successful installation of the Kubernetes cluster, Kubeconfig will be created within cluster's directory. To export the Kubeconfig into custom file run the following command. kubitect export kubeconfig > kubeconfig.yaml","title":"Step 2 - Export kubeconfig"},{"location":"getting-started/quick-start/#step-3-test-the-cluster","text":"Test if the cluster works by displaying all cluster nodes. kubectl get nodes --kubeconfig kubeconfig.yaml","title":"Step 3 - Test the cluster"},{"location":"getting-started/requirements/","text":"Requirements \ud83d\udd17\ufe0e Local machine \ud83d\udd17\ufe0e On the machine where the command-line tool (kubitect) is installed, the following requirements must be met: Git Python >= 3.0 Python virtualenv Hosts \ud83d\udd17\ufe0e A host is a physical server that can be either a local or remote machine. Each host must have: installed libvirt virtualization API and installed hypervisor that is supported by libvirt If the host is a remote server: password-less SSH key to connect to the remote server Example - Install KVM \ud83d\udd17\ufe0e For example, to install the KVM (Kernel-based Virtual Machine) hypervisor and libvirt, use yum or apt to install the following packages: qemu-kvm libvirt-clients libvirt-daemon libvirt-daemon-system After installation, add user to the kvm and libvirt groups. sudo adduser $USER kvm sudo adduser $USER libvirt","title":"Requirements"},{"location":"getting-started/requirements/#requirements","text":"","title":"Requirements"},{"location":"getting-started/requirements/#local-machine","text":"On the machine where the command-line tool (kubitect) is installed, the following requirements must be met: Git Python >= 3.0 Python virtualenv","title":"Local machine"},{"location":"getting-started/requirements/#hosts","text":"A host is a physical server that can be either a local or remote machine. Each host must have: installed libvirt virtualization API and installed hypervisor that is supported by libvirt If the host is a remote server: password-less SSH key to connect to the remote server","title":"Hosts"},{"location":"getting-started/requirements/#example-install-kvm","text":"For example, to install the KVM (Kernel-based Virtual Machine) hypervisor and libvirt, use yum or apt to install the following packages: qemu-kvm libvirt-clients libvirt-daemon libvirt-daemon-system After installation, add user to the kvm and libvirt groups. sudo adduser $USER kvm sudo adduser $USER libvirt","title":"Example - Install KVM"},{"location":"getting-started/other/local-development/","text":"Local development \ud83d\udd17\ufe0e This document shows how to build a CLI tool manually and how to use the project without creating any files outside the project's directory. Prerequisites \ud83d\udd17\ufe0e Go 1.18 or greater installed Git client installed Step 1: Clone the project \ud83d\udd17\ufe0e First, you have to clone the project. git clone https://github.com/MusicDin/kubitect Afterwards, move into the cloned project. cd kubitect Step 2: Install Kubitect CLI tool \ud83d\udd17\ufe0e Kubitect CLI tool is implemented in Go using cobra library. The tool can either be installed from already built versions available on GitHub or you can build it manually. To manually build the CLI tool, first change to the cli directory. cd cli Now, using build the tool using go. go build . This will create a cli binary file, which can be moved into /usr/local/bin/ directory to use it globally. sudo mv cli /usr/bin/local/kubitect Step 3: Local development \ud83d\udd17\ufe0e By default, Kubitect creates and manages clusters located in the Kubitect home directory ( ~/.kubitect ). Although this approach is very useful for everyday use, it can be inconvenient if you are actively making changes to the project, as each change must be committed to the Git repository. For this very reason, the Kubitect CLI tool has the --local option, which replaces the project's home directory with the path of the current directory. This way, the source code from the current directory is used to create a cluster and all cluster-related files are created in the current directory. This option can be used with most actions, such as apply or destroy . kubitect apply --local","title":"Local development"},{"location":"getting-started/other/local-development/#local-development","text":"This document shows how to build a CLI tool manually and how to use the project without creating any files outside the project's directory.","title":"Local development"},{"location":"getting-started/other/local-development/#prerequisites","text":"Go 1.18 or greater installed Git client installed","title":"Prerequisites"},{"location":"getting-started/other/local-development/#step-1-clone-the-project","text":"First, you have to clone the project. git clone https://github.com/MusicDin/kubitect Afterwards, move into the cloned project. cd kubitect","title":"Step 1: Clone the project"},{"location":"getting-started/other/local-development/#step-2-install-kubitect-cli-tool","text":"Kubitect CLI tool is implemented in Go using cobra library. The tool can either be installed from already built versions available on GitHub or you can build it manually. To manually build the CLI tool, first change to the cli directory. cd cli Now, using build the tool using go. go build . This will create a cli binary file, which can be moved into /usr/local/bin/ directory to use it globally. sudo mv cli /usr/bin/local/kubitect","title":"Step 2: Install Kubitect CLI tool"},{"location":"getting-started/other/local-development/#step-3-local-development","text":"By default, Kubitect creates and manages clusters located in the Kubitect home directory ( ~/.kubitect ). Although this approach is very useful for everyday use, it can be inconvenient if you are actively making changes to the project, as each change must be committed to the Git repository. For this very reason, the Kubitect CLI tool has the --local option, which replaces the project's home directory with the path of the current directory. This way, the source code from the current directory is used to create a cluster and all cluster-related files are created in the current directory. This option can be used with most actions, such as apply or destroy . kubitect apply --local","title":"Step 3: Local development"},{"location":"getting-started/other/troubleshooting/","text":"Troubleshooting \ud83d\udd17\ufe0e Is your issue not listed here? If the troubleshooting page is missing an error you encountered, please report it on GitHub by opening an issue . By doing so, you will help improve the project and help others find the solution to the same problem faster. General errors \ud83d\udd17\ufe0e Virtualenv not found \ud83d\udd17\ufe0e Error Explanation Solution Error Output: /bin/sh: 1: virtualenv: not found /bin/sh: 2: ansible-playbook: not found Explanation The error indicates that the virtualenv is not installed. Solution There are many ways to install virtualenv . For all installation options you can refere to their official documentation - Virtualenv installation . For example, virtualenv can be installed using pip . First install pip. sudo apt install python3-pip Then install virtualenv using pip3. pip3 install virtualenv KVM/Libvirt errors \ud83d\udd17\ufe0e Failed to connect socket (No such file or directory) \ud83d\udd17\ufe0e Error Explanation Solution Error Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such file or directory') Explanation The problem may occur when libvirt is not started. Solution Make sure that the libvirt service is running: sudo systemctl status libvirtd If the libvirt service is not running, start it: sudo systemctl start libvirtd Optional: Start the libvirt service automatically at boot time: sudo systemctl enable libvirtd Failed to connect socket (Permission denied) \ud83d\udd17\ufe0e Error Explanation Solution Error Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied') Explanation The error indicates that either the libvirtd service is not running or the current user is not in the libvirt (or kvm ) group. Solution If the libvirtd service is not running, start it: sudo systemctl start libvirtd Add the current user to the libvirt and kvm groups if needed: # Add current user to groups sudo adduser $USER libvirt sudo adduser $USER kvm # Verify groups are added id -nG # Reload user session Error creating libvirt domain \ud83d\udd17\ufe0e Error Explanation Solution Error Error: Error creating libvirt domain: \u2026 Could not open '/tmp/terraform_libvirt_provider_images/image.qcow2': Permission denied') Explanation The error indicates that the file cannot be created in the specified location due to missing permissions. Make sure the directory exists. Make sure the directory of the file that is being denied has appropriate user permissions. Optionally qemu security driver can be disabled. Solution Make sure the security_driver in /etc/libvirt/qemu.conf is set to none instead of selinux . This line is commented out by default, so you should uncomment it if needed: # /etc/libvirt/qemu.conf ... security_driver = \"none\" ... Do not forget to restart the libvirt service after making the changes: sudo systemctl restart libvirtd Libvirt domain already exists \ud83d\udd17\ufe0e Error Explanation Solution Error Error: Error defining libvirt domain: virError(Code=9, Domain=20, Message='operation failed: domain ' your-domain ' already exists with uuid '...') Explanation The error indicates that the libvirt domain (virtual machine) already exists. Solution The resource you are trying to create already exists. Make sure you destroy the resource: virsh destroy your-domain virsh undefine your-domain You can verify that the domain was successfully removed: virsh dominfo --domain your-domain If the domain was successfully removed, the output should look something like this: error: failed to get domain ' your-domain ' Libvirt volume already exists \ud83d\udd17\ufe0e Error Explanation Solution Error Error: Error creating libvirt volume: virError(Code=90, Domain=18, Message='storage volume ' your-volume .qcow2' exists already') and / or Error:Error creating libvirt volume for cloudinit device cloud-init .iso: virError(Code=90, Domain=18, Message='storage volume ' cloud-init .iso' exists already') Explanation The error indicates that the specified volume already exists. Solution Volumes created by Libvirt are still attached to the images, which prevents a new volume from being created with the same name. Therefore, these volumes must be removed: virsh vol-delete cloud-init .iso --pool your_resource_pool and / or virsh vol-delete your-volume .qcow2 --pool your_resource_pool Libvirt storage pool already exists \ud83d\udd17\ufe0e Error Explanation Solution Error Error: Error storage pool ' your-pool ' already exists Explanation The error indicates that the libvirt storage pool already exists. Solution Remove the existing libvirt storage pool. virsh pool-destroy your-pool && virsh pool-undefine your-pool Failed to apply firewall rules \ud83d\udd17\ufe0e Error Explanation Solution Error Error: internal error: Failed to apply firewall rules /sbin/iptables -w --table filter --insert LIBVIRT_INP --in-interface virbr2 --protocol tcp --destination-port 67 --jump ACCEPT: iptables: No chain/target/match by that name. Explanation Libvirt was already running when firewall (usually FirewallD) was started/installed. Therefore, libvirtd service must be restarted to detect the changes. Solution Restart the libvirtd service: sudo systemctl restart libvirtd Failed to remove storage pool \ud83d\udd17\ufe0e Error Explanation Solution Error Error: error deleting storage pool: failed to remove pool '/var/lib/libvirt/images/local-k8s-cluster-main-resource-pool': Directory not empty Explanation The pool cannot be deleted because there are still some volumes in the pool. Therefore, the volumes should be removed before the pool can be deleted. Solution Make sure the pool is running. virsh pool-start --pool local-k8s-cluster-main-resource-pool List volumes in the pool. virsh vol-list --pool local-k8s-cluster-main-resource-pool # Name Path # ------------------------------------------------------------------------------------- # base_volume /var/lib/libvirt/images/local-k8s-cluster-main-resource-pool/base_volume Delete listed volumes from the pool. virsh vol-delete --pool local-k8s-cluster-main-resource-pool --vol base_volume Destroy and undefine the pool. virsh pool-destroy --pool local-k8s-cluster-main-resource-pool virsh pool-undefine --pool local-k8s-cluster-main-resource-pool HAProxy load balancer errors \ud83d\udd17\ufe0e Random HAProxy (503) bad gateway \ud83d\udd17\ufe0e Error Explanation Solution Error HAProxy returns a random HTTP 503 (Bad gateway) error. Explanation More than one HAProxy processes are listening on the same port. Solution 1 For example, if an error is thrown when accessing port 80 , check which processes are listening on port 80 on the load balancer VM: netstat -lnput | grep 80 # Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name # tcp 0 0 192.168.113.200:80 0.0.0.0:* LISTEN 1976/haproxy # tcp 0 0 192.168.113.200:80 0.0.0.0:* LISTEN 1897/haproxy If you see more than one process, kill the unnecessary process: kill 1976 Note: You can kill all HAProxy processes and only one will be automatically recreated. Solution 2 Check the HAProxy configuration file ( config/haproxy/haproxy.cfg ) that it does not contain 2 frontends bound to the same port.","title":"Troubleshooting"},{"location":"getting-started/other/troubleshooting/#troubleshooting","text":"Is your issue not listed here? If the troubleshooting page is missing an error you encountered, please report it on GitHub by opening an issue . By doing so, you will help improve the project and help others find the solution to the same problem faster.","title":"Troubleshooting"},{"location":"getting-started/other/troubleshooting/#general-errors","text":"","title":"General errors"},{"location":"getting-started/other/troubleshooting/#virtualenv-not-found","text":"Error Explanation Solution Error Output: /bin/sh: 1: virtualenv: not found /bin/sh: 2: ansible-playbook: not found Explanation The error indicates that the virtualenv is not installed. Solution There are many ways to install virtualenv . For all installation options you can refere to their official documentation - Virtualenv installation . For example, virtualenv can be installed using pip . First install pip. sudo apt install python3-pip Then install virtualenv using pip3. pip3 install virtualenv","title":"Virtualenv not found"},{"location":"getting-started/other/troubleshooting/#kvmlibvirt-errors","text":"","title":"KVM/Libvirt errors"},{"location":"getting-started/other/troubleshooting/#failed-to-connect-socket-no-such-file-or-directory","text":"Error Explanation Solution Error Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such file or directory') Explanation The problem may occur when libvirt is not started. Solution Make sure that the libvirt service is running: sudo systemctl status libvirtd If the libvirt service is not running, start it: sudo systemctl start libvirtd Optional: Start the libvirt service automatically at boot time: sudo systemctl enable libvirtd","title":"Failed to connect socket (No such file or directory)"},{"location":"getting-started/other/troubleshooting/#failed-to-connect-socket-permission-denied","text":"Error Explanation Solution Error Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied') Explanation The error indicates that either the libvirtd service is not running or the current user is not in the libvirt (or kvm ) group. Solution If the libvirtd service is not running, start it: sudo systemctl start libvirtd Add the current user to the libvirt and kvm groups if needed: # Add current user to groups sudo adduser $USER libvirt sudo adduser $USER kvm # Verify groups are added id -nG # Reload user session","title":"Failed to connect socket (Permission denied)"},{"location":"getting-started/other/troubleshooting/#error-creating-libvirt-domain","text":"Error Explanation Solution Error Error: Error creating libvirt domain: \u2026 Could not open '/tmp/terraform_libvirt_provider_images/image.qcow2': Permission denied') Explanation The error indicates that the file cannot be created in the specified location due to missing permissions. Make sure the directory exists. Make sure the directory of the file that is being denied has appropriate user permissions. Optionally qemu security driver can be disabled. Solution Make sure the security_driver in /etc/libvirt/qemu.conf is set to none instead of selinux . This line is commented out by default, so you should uncomment it if needed: # /etc/libvirt/qemu.conf ... security_driver = \"none\" ... Do not forget to restart the libvirt service after making the changes: sudo systemctl restart libvirtd","title":"Error creating libvirt domain"},{"location":"getting-started/other/troubleshooting/#libvirt-domain-already-exists","text":"Error Explanation Solution Error Error: Error defining libvirt domain: virError(Code=9, Domain=20, Message='operation failed: domain ' your-domain ' already exists with uuid '...') Explanation The error indicates that the libvirt domain (virtual machine) already exists. Solution The resource you are trying to create already exists. Make sure you destroy the resource: virsh destroy your-domain virsh undefine your-domain You can verify that the domain was successfully removed: virsh dominfo --domain your-domain If the domain was successfully removed, the output should look something like this: error: failed to get domain ' your-domain '","title":"Libvirt domain already exists"},{"location":"getting-started/other/troubleshooting/#libvirt-volume-already-exists","text":"Error Explanation Solution Error Error: Error creating libvirt volume: virError(Code=90, Domain=18, Message='storage volume ' your-volume .qcow2' exists already') and / or Error:Error creating libvirt volume for cloudinit device cloud-init .iso: virError(Code=90, Domain=18, Message='storage volume ' cloud-init .iso' exists already') Explanation The error indicates that the specified volume already exists. Solution Volumes created by Libvirt are still attached to the images, which prevents a new volume from being created with the same name. Therefore, these volumes must be removed: virsh vol-delete cloud-init .iso --pool your_resource_pool and / or virsh vol-delete your-volume .qcow2 --pool your_resource_pool","title":"Libvirt volume already exists"},{"location":"getting-started/other/troubleshooting/#libvirt-storage-pool-already-exists","text":"Error Explanation Solution Error Error: Error storage pool ' your-pool ' already exists Explanation The error indicates that the libvirt storage pool already exists. Solution Remove the existing libvirt storage pool. virsh pool-destroy your-pool && virsh pool-undefine your-pool","title":"Libvirt storage pool already exists"},{"location":"getting-started/other/troubleshooting/#failed-to-apply-firewall-rules","text":"Error Explanation Solution Error Error: internal error: Failed to apply firewall rules /sbin/iptables -w --table filter --insert LIBVIRT_INP --in-interface virbr2 --protocol tcp --destination-port 67 --jump ACCEPT: iptables: No chain/target/match by that name. Explanation Libvirt was already running when firewall (usually FirewallD) was started/installed. Therefore, libvirtd service must be restarted to detect the changes. Solution Restart the libvirtd service: sudo systemctl restart libvirtd","title":"Failed to apply firewall rules"},{"location":"getting-started/other/troubleshooting/#failed-to-remove-storage-pool","text":"Error Explanation Solution Error Error: error deleting storage pool: failed to remove pool '/var/lib/libvirt/images/local-k8s-cluster-main-resource-pool': Directory not empty Explanation The pool cannot be deleted because there are still some volumes in the pool. Therefore, the volumes should be removed before the pool can be deleted. Solution Make sure the pool is running. virsh pool-start --pool local-k8s-cluster-main-resource-pool List volumes in the pool. virsh vol-list --pool local-k8s-cluster-main-resource-pool # Name Path # ------------------------------------------------------------------------------------- # base_volume /var/lib/libvirt/images/local-k8s-cluster-main-resource-pool/base_volume Delete listed volumes from the pool. virsh vol-delete --pool local-k8s-cluster-main-resource-pool --vol base_volume Destroy and undefine the pool. virsh pool-destroy --pool local-k8s-cluster-main-resource-pool virsh pool-undefine --pool local-k8s-cluster-main-resource-pool","title":"Failed to remove storage pool"},{"location":"getting-started/other/troubleshooting/#haproxy-load-balancer-errors","text":"","title":"HAProxy load balancer errors"},{"location":"getting-started/other/troubleshooting/#random-haproxy-503-bad-gateway","text":"Error Explanation Solution Error HAProxy returns a random HTTP 503 (Bad gateway) error. Explanation More than one HAProxy processes are listening on the same port. Solution 1 For example, if an error is thrown when accessing port 80 , check which processes are listening on port 80 on the load balancer VM: netstat -lnput | grep 80 # Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name # tcp 0 0 192.168.113.200:80 0.0.0.0:* LISTEN 1976/haproxy # tcp 0 0 192.168.113.200:80 0.0.0.0:* LISTEN 1897/haproxy If you see more than one process, kill the unnecessary process: kill 1976 Note: You can kill all HAProxy processes and only one will be automatically recreated. Solution 2 Check the HAProxy configuration file ( config/haproxy/haproxy.cfg ) that it does not contain 2 frontends bound to the same port.","title":"Random HAProxy (503) bad gateway"},{"location":"user-guide/before-you-begin/","text":"Before you begin \ud83d\udd17\ufe0e The user guide is divided into three subsections: Configuration , Cluster Management , and Reference . The Configuration subsection contains explanations of the configurable Kubitect properties. The Cluster Management subsection introduces the operations that can be performed over the cluster. Finally, the Reference subsection contains a configuration and CLI reference. The following symbol conventions are used throughout the user guide: - Indicates the Kubitect version in which the property was either added or last modified. - Indicates that the property is required in every valid configuration. - Indicates the default value of the property. - Indicates that the feature or property is experimental (not yet stable). This means that its implementation may change drastically over time and that its activation may lead to unexpected behavior.","title":"Before you begin"},{"location":"user-guide/before-you-begin/#before-you-begin","text":"The user guide is divided into three subsections: Configuration , Cluster Management , and Reference . The Configuration subsection contains explanations of the configurable Kubitect properties. The Cluster Management subsection introduces the operations that can be performed over the cluster. Finally, the Reference subsection contains a configuration and CLI reference. The following symbol conventions are used throughout the user guide: - Indicates the Kubitect version in which the property was either added or last modified. - Indicates that the property is required in every valid configuration. - Indicates the default value of the property. - Indicates that the feature or property is experimental (not yet stable). This means that its implementation may change drastically over time and that its activation may lead to unexpected behavior.","title":"Before you begin"},{"location":"user-guide/configuration/addons/","text":"Addons \ud83d\udd17\ufe0e Configuration \ud83d\udd17\ufe0e Kubespray addons \ud83d\udd17\ufe0e v2.1.0 Kubespray offers many useful configurable addons, such as the Ingress-NGINX controller , MetalLB , and so on. Kubespray addons can be configured in Kubitect under the addons.kubespray property. The configuration of Kubespray addons is exactly the same as the default configuration of Kubespray addons, since Kubitect simply copies the provided configuration into Kubespray's group variables when the cluster is created. All available Kubespray addons can be found in the Kubespray addons sample , while most of them are documented in the official Kubespray documentation . addons : kubespray : # Nginx ingress controller deployment ingress_nginx_enabled : true ingress_nginx_namespace : \"ingress-nginx\" ingress_nginx_insecure_port : 80 ingress_nginx_secure_port : 443 # MetalLB deployment metallb_enabled : true metallb_speaker_enabled : true metallb_ip_range : - \"10.10.9.201-10.10.9.254\" metallb_pool_name : \"default\" metallb_auto_assign : true metallb_version : v0.12.1 metallb_protocol : \"layer2\" Rook addon \ud83d\udd17\ufe0e v2.2.0 \u2002 Experimental Rook is an orchestration tool that allows Ceph , a reliable and scalable storage, to run within a Kubernetes cluster. In Kubitect, Rook can be enabled by simply setting addons.rook.enabled to true. addons : rook : enabled : true Rook is deployed only on worker nodes. When a cluster is created without worker nodes, Kubitect attempts to install Rook on the master node. In addition to enabling the Rook addon, at least one data disk must be attached to a node suitable for Rook deployment. If Kubitect determines that no data disks are available for Rook, it will simply skip installing Rook. By default, Rook uses all available data disks attached to worker nodes and converts them to distributed storage. Similarly, all worker nodes are used for Rook deployment. To restrict on which nodes Rook resources can be deployed, the node selector can be used. Node selector \ud83d\udd17\ufe0e The node selector is a dictionary of node labels used to determine which nodes are eligible for Rook deployment. If a node does not match all of the specified node labels, Rook resources cannot be deployed on that node and disks attached to that node are not used for distributed storage. addons : rook : nodeSelector : rook : true Version \ud83d\udd17\ufe0e By default, the latest ( master ) Rook version is used. To use a specific version of Rook, set the addons.rook.version property to the desired version. addons : rook : version : v1.9.9","title":"Addons"},{"location":"user-guide/configuration/addons/#addons","text":"","title":"Addons"},{"location":"user-guide/configuration/addons/#configuration","text":"","title":"Configuration"},{"location":"user-guide/configuration/addons/#kubespray-addons","text":"v2.1.0 Kubespray offers many useful configurable addons, such as the Ingress-NGINX controller , MetalLB , and so on. Kubespray addons can be configured in Kubitect under the addons.kubespray property. The configuration of Kubespray addons is exactly the same as the default configuration of Kubespray addons, since Kubitect simply copies the provided configuration into Kubespray's group variables when the cluster is created. All available Kubespray addons can be found in the Kubespray addons sample , while most of them are documented in the official Kubespray documentation . addons : kubespray : # Nginx ingress controller deployment ingress_nginx_enabled : true ingress_nginx_namespace : \"ingress-nginx\" ingress_nginx_insecure_port : 80 ingress_nginx_secure_port : 443 # MetalLB deployment metallb_enabled : true metallb_speaker_enabled : true metallb_ip_range : - \"10.10.9.201-10.10.9.254\" metallb_pool_name : \"default\" metallb_auto_assign : true metallb_version : v0.12.1 metallb_protocol : \"layer2\"","title":"Kubespray addons"},{"location":"user-guide/configuration/addons/#rook-addon","text":"v2.2.0 \u2002 Experimental Rook is an orchestration tool that allows Ceph , a reliable and scalable storage, to run within a Kubernetes cluster. In Kubitect, Rook can be enabled by simply setting addons.rook.enabled to true. addons : rook : enabled : true Rook is deployed only on worker nodes. When a cluster is created without worker nodes, Kubitect attempts to install Rook on the master node. In addition to enabling the Rook addon, at least one data disk must be attached to a node suitable for Rook deployment. If Kubitect determines that no data disks are available for Rook, it will simply skip installing Rook. By default, Rook uses all available data disks attached to worker nodes and converts them to distributed storage. Similarly, all worker nodes are used for Rook deployment. To restrict on which nodes Rook resources can be deployed, the node selector can be used.","title":"Rook addon"},{"location":"user-guide/configuration/addons/#node-selector","text":"The node selector is a dictionary of node labels used to determine which nodes are eligible for Rook deployment. If a node does not match all of the specified node labels, Rook resources cannot be deployed on that node and disks attached to that node are not used for distributed storage. addons : rook : nodeSelector : rook : true","title":"Node selector"},{"location":"user-guide/configuration/addons/#version","text":"By default, the latest ( master ) Rook version is used. To use a specific version of Rook, set the addons.rook.version property to the desired version. addons : rook : version : v1.9.9","title":"Version"},{"location":"user-guide/configuration/cluster-name/","text":"Cluster metadata \ud83d\udd17\ufe0e Configuration \ud83d\udd17\ufe0e Cluster name \ud83d\udd17\ufe0e v2.0.0 \u2002 Required The cluster name must be defined as part of the Kubitect configuration. It will be used as a prefix for all resources created by Kubitect as part of this cluster. cluster : name : my-cluster For example, the name of each virtual machine is generated as <cluster.name>-<node.type>-<node.instance.id> . This way, the master node with ID 1 would result in my-cluster-master-1 .","title":"Cluster name"},{"location":"user-guide/configuration/cluster-name/#cluster-metadata","text":"","title":"Cluster metadata"},{"location":"user-guide/configuration/cluster-name/#configuration","text":"","title":"Configuration"},{"location":"user-guide/configuration/cluster-name/#cluster-name","text":"v2.0.0 \u2002 Required The cluster name must be defined as part of the Kubitect configuration. It will be used as a prefix for all resources created by Kubitect as part of this cluster. cluster : name : my-cluster For example, the name of each virtual machine is generated as <cluster.name>-<node.type>-<node.instance.id> . This way, the master node with ID 1 would result in my-cluster-master-1 .","title":"Cluster name"},{"location":"user-guide/configuration/cluster-network/","text":"Cluster network \ud83d\udd17\ufe0e This document describes how to define the cluster network in the Kubitect configuration. It defines either the properties of the network to be created or the network to which the cluster nodes are to be assigned. Configuration \ud83d\udd17\ufe0e Network mode \ud83d\udd17\ufe0e v2.0.0 \u2002 Required Kubitect supports two network modes. The first is the nat mode and the other is the bridge mode. cluster : network : mode : nat NAT mode \ud83d\udd17\ufe0e In NAT (Network Address Translation) mode, the libvirt virtual network is created for the cluster. This reduces the need for manual configurations, but is limited to one host (a single physical server). Bridge mode \ud83d\udd17\ufe0e In bridge mode, a real host network device is shared with the virtual machines. Therefore, each virtual machine can bind to any available IP address on the local network, just like a physical computer. This approach makes the virtual machine visible on the network, which enables the creation of clusters across multiple physical servers. The only requirement for using bridged networks is the preconfigured bridge interface on each target host. Preconfiguring bridge interfaces is necessary because every environment is different. For example, someone might use link aggregation (also known as link bonding or teaming), which cannot be detected automatically and therefore requires manual configuration. The Network bridge example describes how to create a bridge interface with netplan and configure Kubitect to use it. How to automate bridge interface creation? If you have an idea or suggestion on how to automate the creation of bridge interfaces, feel free to open an issue on GitHub. Network CIDR \ud83d\udd17\ufe0e v2.0.0 \u2002 Required Network CIDR (Classless Inter-Domain Routing) defines the network in a form of <network_ip>/<network_prefix_bits> . All IP addresses specified in the cluster section of the configuration must be in this network range ( this includes a network gateway, node instances, floating IP of the load balancer, etc. ). When using NAT network mode, the network CIDR defines an unused private network that is created. In bridge mode, the network CIDR should specify the network to which the cluster belongs. cluster : network : cidr : 192.168.113.0/24 # (1)! In nat mode - Any unused private network within a local network. In bridge mode - A network to which the cluster belongs. Network gateway \ud83d\udd17\ufe0e v2.0.0 The network gateway (or default gateway) defines the IP address of the router. By default, it does not need to be specified because the first client IP in the network range is used as the gateway address. If the gateway IP differs from this, it must be specified manually. Also note that the gateway IP address must be within the specified network range. cluster : network : cidr : 10.10.0.0/20 gateway : 10.10.0.230 # (1)! If this option is omitted, 10.10.0.1 is used as the gateway IP (first client IP in the network range). Network bridge \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: virbr0 The network bridge defines the bridge interface that virtual machines connect to. When the NAT network mode is used, a virtual network bridge interface is created on the host. Virtual bridges are usually prefixed with vir (example: virbr44 ). If this option is omitted, the virtual bridge name is automatically determined by libvirt. Otherwise, the specified name is used for the virtual bridge. In the case of bridge network mode, the network bridge should be the name of the preconfigured bridge interface (example: br0 ). cluster : network : bridge : br0 Example usage \ud83d\udd17\ufe0e Virtual NAT network \ud83d\udd17\ufe0e If the cluster is created on a single host, the NAT network mode can be used. In this case, only the CIDR of the new network needs to be specified in addition to the network mode. cluster : network : mode : nat cidr : 192.168.113.0/24 Bridged network \ud83d\udd17\ufe0e To make the cluster nodes visible on the local network as physical machines or to create the cluster across multiple hosts, bridge network mode must be used. Also, the network CIDR of an existing network must be specified along with the preconfigured host bridge interface. cluster : network : mode : bridge cidr : 10.10.64.0/24 bridge : br0","title":"Cluster network"},{"location":"user-guide/configuration/cluster-network/#cluster-network","text":"This document describes how to define the cluster network in the Kubitect configuration. It defines either the properties of the network to be created or the network to which the cluster nodes are to be assigned.","title":"Cluster network"},{"location":"user-guide/configuration/cluster-network/#configuration","text":"","title":"Configuration"},{"location":"user-guide/configuration/cluster-network/#network-mode","text":"v2.0.0 \u2002 Required Kubitect supports two network modes. The first is the nat mode and the other is the bridge mode. cluster : network : mode : nat","title":"Network mode"},{"location":"user-guide/configuration/cluster-network/#nat-mode","text":"In NAT (Network Address Translation) mode, the libvirt virtual network is created for the cluster. This reduces the need for manual configurations, but is limited to one host (a single physical server).","title":"NAT mode"},{"location":"user-guide/configuration/cluster-network/#bridge-mode","text":"In bridge mode, a real host network device is shared with the virtual machines. Therefore, each virtual machine can bind to any available IP address on the local network, just like a physical computer. This approach makes the virtual machine visible on the network, which enables the creation of clusters across multiple physical servers. The only requirement for using bridged networks is the preconfigured bridge interface on each target host. Preconfiguring bridge interfaces is necessary because every environment is different. For example, someone might use link aggregation (also known as link bonding or teaming), which cannot be detected automatically and therefore requires manual configuration. The Network bridge example describes how to create a bridge interface with netplan and configure Kubitect to use it. How to automate bridge interface creation? If you have an idea or suggestion on how to automate the creation of bridge interfaces, feel free to open an issue on GitHub.","title":"Bridge mode"},{"location":"user-guide/configuration/cluster-network/#network-cidr","text":"v2.0.0 \u2002 Required Network CIDR (Classless Inter-Domain Routing) defines the network in a form of <network_ip>/<network_prefix_bits> . All IP addresses specified in the cluster section of the configuration must be in this network range ( this includes a network gateway, node instances, floating IP of the load balancer, etc. ). When using NAT network mode, the network CIDR defines an unused private network that is created. In bridge mode, the network CIDR should specify the network to which the cluster belongs. cluster : network : cidr : 192.168.113.0/24 # (1)! In nat mode - Any unused private network within a local network. In bridge mode - A network to which the cluster belongs.","title":"Network CIDR"},{"location":"user-guide/configuration/cluster-network/#network-gateway","text":"v2.0.0 The network gateway (or default gateway) defines the IP address of the router. By default, it does not need to be specified because the first client IP in the network range is used as the gateway address. If the gateway IP differs from this, it must be specified manually. Also note that the gateway IP address must be within the specified network range. cluster : network : cidr : 10.10.0.0/20 gateway : 10.10.0.230 # (1)! If this option is omitted, 10.10.0.1 is used as the gateway IP (first client IP in the network range).","title":"Network gateway"},{"location":"user-guide/configuration/cluster-network/#network-bridge","text":"v2.0.0 \u2002 Default: virbr0 The network bridge defines the bridge interface that virtual machines connect to. When the NAT network mode is used, a virtual network bridge interface is created on the host. Virtual bridges are usually prefixed with vir (example: virbr44 ). If this option is omitted, the virtual bridge name is automatically determined by libvirt. Otherwise, the specified name is used for the virtual bridge. In the case of bridge network mode, the network bridge should be the name of the preconfigured bridge interface (example: br0 ). cluster : network : bridge : br0","title":"Network bridge"},{"location":"user-guide/configuration/cluster-network/#example-usage","text":"","title":"Example usage"},{"location":"user-guide/configuration/cluster-network/#virtual-nat-network","text":"If the cluster is created on a single host, the NAT network mode can be used. In this case, only the CIDR of the new network needs to be specified in addition to the network mode. cluster : network : mode : nat cidr : 192.168.113.0/24","title":"Virtual NAT network"},{"location":"user-guide/configuration/cluster-network/#bridged-network","text":"To make the cluster nodes visible on the local network as physical machines or to create the cluster across multiple hosts, bridge network mode must be used. Also, the network CIDR of an existing network must be specified along with the preconfigured host bridge interface. cluster : network : mode : bridge cidr : 10.10.64.0/24 bridge : br0","title":"Bridged network"},{"location":"user-guide/configuration/cluster-node-template/","text":"Cluster node template \ud83d\udd17\ufe0e The note template in the cluster section of the configuration defines the properties of all nodes in the cluster. This includes the properties of the operating system (OS), DNS, and virtual machine user. Configuration \ud83d\udd17\ufe0e Virtual machine user \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: k8s The user property defines the name of the passwordless user created on each virtual machine. It is used to access the virtual machines during cluster configuration. If the user property is omitted, a user named k8s is created on all virtual machines. This user can also be used later to access each virtual machine via SSH. cluster : nodeTemplate : user : kubitect Operating system (OS) \ud83d\udd17\ufe0e OS distribution \ud83d\udd17\ufe0e v2.1.0 \u2002 Default: ubuntu The operating system for virtual machines can be specified in the node template. Currently, either Ubuntu or Debian can be configured. By default, the Ubuntu distribution is installed on all virtual machines. To use Debian instead, set os.distro property to Debian. cluster : nodeTemplate : os : distro : debian # (1)! By default, ubuntu is used. Available OS distribution presets are the following: ubuntu - Latest Ubuntu 22.04 release. ( default ) ubuntu22 - Ubuntu 22.04 release 2022-07-12 . ubuntu20 - Ubuntu 20.04 release 2022-07-11 . debian - Latest Debian 11 release. debian11 - Debian 11 release 2022-07-11 . Ubuntu images are downloaded from the Ubuntu cloud image repository and Debian images are downloaded from the Debian cloud image repository . Custom OS source \ud83d\udd17\ufe0e v2.1.0 If the presets do not meet your needs, you can also use a custom Ubuntu or Debian image by simply specifying the image source. The source of an image can be either a local path on a system or an URL pointing to the image download. cluster : nodeTemplate : os : distro : ubuntu source : https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img Primary OS network interface \ud83d\udd17\ufe0e v2.1.0 When a virtual machine is created, the network interface names are evaluated deterministically. Therefore, Kubitect should use the correct network interface names for all available presets. However, if you want to instruct Kubitect to use a specific network interface as primary, set its name as the value of the os.networkInterface property. cluster : nodeTemplate : os : networkInterface : ens3 Custom DNS list \ud83d\udd17\ufe0e v2.1.0 The list of Domain Name Servers (DNS) can be configured in the node template. These servers are used by all virtual machines for DNS resolution. By default, a DNS list contains only the network gateway. cluster : nodeTemplate : dns : # (1)! - 1.1.1.1 - 1.0.0.1 IP addresses 1.1.1.1 and 1.0.0.1 represent CloudFlare's primary and secondary public DNS resolvers, respectively. CPU mode \ud83d\udd17\ufe0e v2.2.0 \u2002 Default: custom The CPU mode property can be used to simplify the configuration of a guest CPU to be as close as possible to the host CPU. Consult the libvirt documentation to learn about all available CPU modes: custom (default) host-model host-passthrough maximum cluster : nodeTemplate : cpuMode : host-passthrough SSH options \ud83d\udd17\ufe0e Custom SSH certificate \ud83d\udd17\ufe0e v2.0.0 Kubitect ensures that SSH certificates are automatically generated before the cluster is deployed. The generated certificates are located in the config/.ssh/ directory inside a cluster directory. You can use a custom SSH certificate by specifying a local path to the private key. Note that the public key must be located in the same directory with the .pub suffix. cluster : nodeTemplate : ssh : privateKeyPath : \"~/.ssh/id_rsa_test\" Warning SSH certificates must be passwordless, otherwise Kubespray will fail to configure the cluster. Adding nodes to the known hosts \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: false In addition, Kubitect allows you to add all created virtual machines to SSH known hosts on the local machine. To enable this behavior, set the addToKnownHosts property to true. cluster : nodeTemplate : ssh : addToKnownHosts : true","title":"Cluster node template"},{"location":"user-guide/configuration/cluster-node-template/#cluster-node-template","text":"The note template in the cluster section of the configuration defines the properties of all nodes in the cluster. This includes the properties of the operating system (OS), DNS, and virtual machine user.","title":"Cluster node template"},{"location":"user-guide/configuration/cluster-node-template/#configuration","text":"","title":"Configuration"},{"location":"user-guide/configuration/cluster-node-template/#virtual-machine-user","text":"v2.0.0 \u2002 Default: k8s The user property defines the name of the passwordless user created on each virtual machine. It is used to access the virtual machines during cluster configuration. If the user property is omitted, a user named k8s is created on all virtual machines. This user can also be used later to access each virtual machine via SSH. cluster : nodeTemplate : user : kubitect","title":"Virtual machine user"},{"location":"user-guide/configuration/cluster-node-template/#operating-system-os","text":"","title":"Operating system (OS)"},{"location":"user-guide/configuration/cluster-node-template/#os-distribution","text":"v2.1.0 \u2002 Default: ubuntu The operating system for virtual machines can be specified in the node template. Currently, either Ubuntu or Debian can be configured. By default, the Ubuntu distribution is installed on all virtual machines. To use Debian instead, set os.distro property to Debian. cluster : nodeTemplate : os : distro : debian # (1)! By default, ubuntu is used. Available OS distribution presets are the following: ubuntu - Latest Ubuntu 22.04 release. ( default ) ubuntu22 - Ubuntu 22.04 release 2022-07-12 . ubuntu20 - Ubuntu 20.04 release 2022-07-11 . debian - Latest Debian 11 release. debian11 - Debian 11 release 2022-07-11 . Ubuntu images are downloaded from the Ubuntu cloud image repository and Debian images are downloaded from the Debian cloud image repository .","title":"OS distribution"},{"location":"user-guide/configuration/cluster-node-template/#custom-os-source","text":"v2.1.0 If the presets do not meet your needs, you can also use a custom Ubuntu or Debian image by simply specifying the image source. The source of an image can be either a local path on a system or an URL pointing to the image download. cluster : nodeTemplate : os : distro : ubuntu source : https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img","title":"Custom OS source"},{"location":"user-guide/configuration/cluster-node-template/#primary-os-network-interface","text":"v2.1.0 When a virtual machine is created, the network interface names are evaluated deterministically. Therefore, Kubitect should use the correct network interface names for all available presets. However, if you want to instruct Kubitect to use a specific network interface as primary, set its name as the value of the os.networkInterface property. cluster : nodeTemplate : os : networkInterface : ens3","title":"Primary OS network interface"},{"location":"user-guide/configuration/cluster-node-template/#custom-dns-list","text":"v2.1.0 The list of Domain Name Servers (DNS) can be configured in the node template. These servers are used by all virtual machines for DNS resolution. By default, a DNS list contains only the network gateway. cluster : nodeTemplate : dns : # (1)! - 1.1.1.1 - 1.0.0.1 IP addresses 1.1.1.1 and 1.0.0.1 represent CloudFlare's primary and secondary public DNS resolvers, respectively.","title":"Custom DNS list"},{"location":"user-guide/configuration/cluster-node-template/#cpu-mode","text":"v2.2.0 \u2002 Default: custom The CPU mode property can be used to simplify the configuration of a guest CPU to be as close as possible to the host CPU. Consult the libvirt documentation to learn about all available CPU modes: custom (default) host-model host-passthrough maximum cluster : nodeTemplate : cpuMode : host-passthrough","title":"CPU mode"},{"location":"user-guide/configuration/cluster-node-template/#ssh-options","text":"","title":"SSH options"},{"location":"user-guide/configuration/cluster-node-template/#custom-ssh-certificate","text":"v2.0.0 Kubitect ensures that SSH certificates are automatically generated before the cluster is deployed. The generated certificates are located in the config/.ssh/ directory inside a cluster directory. You can use a custom SSH certificate by specifying a local path to the private key. Note that the public key must be located in the same directory with the .pub suffix. cluster : nodeTemplate : ssh : privateKeyPath : \"~/.ssh/id_rsa_test\" Warning SSH certificates must be passwordless, otherwise Kubespray will fail to configure the cluster.","title":"Custom SSH certificate"},{"location":"user-guide/configuration/cluster-node-template/#adding-nodes-to-the-known-hosts","text":"v2.0.0 \u2002 Default: false In addition, Kubitect allows you to add all created virtual machines to SSH known hosts on the local machine. To enable this behavior, set the addToKnownHosts property to true. cluster : nodeTemplate : ssh : addToKnownHosts : true","title":"Adding nodes to the known hosts"},{"location":"user-guide/configuration/cluster-nodes/","text":"Cluster nodes \ud83d\udd17\ufe0e Nodes configuration structure \ud83d\udd17\ufe0e Cluster's nodes configuration consists of three node types : Master nodes (control plane) Worker nodes Load balancers For any cluster deployment, at least one master node needs to be configured . Configuring only one master node produces a single-node cluster. In most cases, a multi-node cluster is desired and therefore worker nodes should be configured as well. If the control plane of the cluster contains multiple nodes, at least one load balancer must be configured. Such topology allows the cluster to continue operating normally if any control plane node fails. In addition, configuring multiple load balancers provides failover in case the primary load balancer fails. Kubitect currently supports only stacked control plane, which means that etcd key-value stores are deployed on control plane nodes. Since an etcd cluster requires a majority \"(n/2) + 1\" of nodes to agree to a change in the cluster, an odd number of nodes (1, 3, 5, ...) provides the best fault tolerance. For example, in control planes with 3 nodes, 2 nodes represent the majority, giving a fault tolerance of 1 node. In control planes with 4 nodes, the majority is 3 nodes, which provides the same fault tolerance. For this reason, Kubitect prevents deployment of the cluster whose control plane contains an even number of nodes. The nodes configuration structure is the following: cluster : nodes : masters : ... workers : ... loadBalancers : ... Each node type has two subsections, default and instances . Instances represent an array of actual nodes, while defaults provide the configuration that is applied to all instances of a certain node type. Each default value can also be overwritten by setting the same property for a specific instance. cluster : nodes : <node-type> : default : ... instances : ... Configuration \ud83d\udd17\ufe0e Common node properties \ud83d\udd17\ufe0e For each instance there is a set of predefined properties that can be set. Some properties apply for all node types, while some properties are specific for a certain node type. Properties that apply for all node types, are referred to as common properties . Instance ID \ud83d\udd17\ufe0e v2.3.0 \u2002 Required Each instance in the cluster must have an ID that must be unique among all instances of the same node type. The instance ID is used as a suffix for the name of each node. cluster : nodes : <node-type> : instances : - id : 1 - id : compute-1 - id : 77 CPU \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: 2 vCPU The cpu property defines an amount of vCPU cores assigned to the virtual machine. It can be set for a specific instance or as a default value for all instances. cluster : nodes : <node-type> : default : cpu : 2 instances : - id : 1 # (1)! - id : 2 cpu : 4 # (2)! Since the cpu property is not set for this instance, the default value is used (2). This instance has the cpu property set, and therefore the set value (4) overrides the default value (2). If the property is not set at the instance level or as a default value, Kubitect uses its own default value (2 vCPU). cluster : nodes : <node-type> : instances : - id : 1 # (1)! Since the 'cpu' property is not set at instance level or as a default value, Kubitect sets the value of the 'cpu' property to 2 vCPU . RAM \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: 4 GiB The ram property defines an amount of RAM assigned to the virtual machine (in GiB). It can be set for a specific instance or as a default value for all instances. cluster : nodes : <node-type> : default : ram : 8 instances : - id : 1 # (1)! - id : 2 ram : 16 # (2)! Since the ram property is not set for this instance, the default value is used (8 GiB). This instance has the ram property set, and therefore the set value (16 GiB) overrides the default value (8 GiB). If the property is not set at the instance level or as a default value, Kubitect uses its own default value (4 GiB). cluster : nodes : <node-type> : instances : - id : 1 # (1)! Since the ram property is not set at instance level or as a default value, Kubitect sets the value of the ram property to 4 GiB . Main disk size \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: 32 GiB The mainDiskSize property defines an amount of space assigned to the virtual machine (in GiB). It can be set for a specific instance or as a default value for all instances. cluster : nodes : <node-type> : default : mainDiskSize : 128 instances : - id : 1 # (1)! - id : 2 mainDiskSize : 256 # (2)! Since the mainDiskSize property is not set for this instance, the default value is used (128 GiB). This instance has the mainDiskSize property set, so therefore the set value (256 GiB) overrides the default value (128 GiB). If the property is not set at the instance level or as a default value, Kubitect uses its own default value (32 GiB). cluster : nodes : <node-type> : instances : - id : 1 # (1)! Since the mainDiskSize property is not set at instance level or as a default value, Kubitect sets the value of the mainDiskSize property to 32 GiB . IP address \ud83d\udd17\ufe0e v2.0.0 For each node a static IP address can be set. f no IP address is set for a particular node, a DHCP lease is requested. Kubitect also checks whether all set IP addresses are within the defined network range (see Network CIDR ). cluster : network : mode : nat cidr : 192.168.113.0/24 nodes : <node-type> : instances : - id : 1 ip : 192.168.113.5 # (1)! - id : 2 # (2)! A static IP ( 192.168.113.5 ) is set for this instance. Since no IP address is defined for this instance, a DHCP lease is requested. MAC address \ud83d\udd17\ufe0e v2.0.0 By default, MAC addresses are generated for each virtual machine created, but a custom MAC address can also be set. cluster : nodes : <node-type> : instances : - id : 1 mac : \"52:54:00:00:13:10\" # (1)! - id : 2 # (2)! A custom MAC address ( 52:54:00:00:13:10 ) is set for this instance. Since no MAC address is defined for this instance, the MAC address is generated during cluster creation. Host affinity \ud83d\udd17\ufe0e v2.0.0 By default, all instances are deployed on the default host . Kubitect can be instructed to deploy the instance on a specific host by specifying the name of the host in the instance configuration. hosts : - name : host1 ... - name : host2 default : true ... cluster : nodes : <node-type> : instances : - id : 1 host : host1 # (1)! - id : 2 # (2)! The instance is deployed on host1 . Since no host is specified, the instance is deployed on the default host ( host2 ). Control plane and worker node properties \ud83d\udd17\ufe0e The following properties can be configured only for control plane or worker nodes. Data disks \ud83d\udd17\ufe0e v2.2.0 By default, only a main disk (volume) is attached to each provisioned virtual machine. Since the main disk already contains an operating system, so it may not be suitable for storing data. Therefore, additional disks might be required. For example, a Rook can be easily configured to use all the empty disks attached to the virtual machine to form a storage cluster. A name and size (in GiB) must be configured for each data disk. By default, data disks are created in the main resource pool. To create a data disk in a custom data resource pool , the pool property can be set to the name of the desired data resource pool. Also note that the data disk name must be unique among all data disks for a given instance. cluster : nodes : <node-type> : instances : - id : 1 dataDisks : - name : data-volume pool : main # (1)! size : 256 - name : rook-volume pool : rook-pool # (2)! size : 512 When pool property is omitted or set to main , the data disk is created in the main resource pool. Custom data resource pool must be configured in the hosts section. Note Default data disks are currently not supported. Node labels \ud83d\udd17\ufe0e v2.1.0 Node labels are configured as a dictionary of key-value pairs. They are used to label actual Kubernetes nodes, and therefore can only be applied to control plane (master) and worker nodes. They can be set for a specific instance or as a default value for all instances. Labels set for a specific instance are merged with the default labels. However, labels set at the instance level take precedence over default labels. cluster : nodes : <node-type> : # (1)! default : labels : label-key-1 : def-label-value-1 label-key-2 : def-label-value-2 instances : - id : 1 labels : # (2)! label-key-3 : instance-label-value-3 - id : 2 labels : # (3)! label-key-1 : new-label-value-1 Node labels can only be applied to control plane (master) and worker nodes. Labels defined at the instance level are merged with default labels. As a result, the following labels are applied to this instance: label-key-1 : def-label-value-1 label-key-2 : def-label-value-2 label-key-3 : instance-label-value-3 Labels defined at the instance level take precedence over default labels. As a result, the following labels are applied to this instance: label-key-1 : new-label-value-1 label-key-2 : def-label-value-2 Node taints \ud83d\udd17\ufe0e v2.2.0 Node taints are configured as a list of strings in the format key=value:effect . Similar to node labels, taints can only be applied to control plane (master) and worker nodes. Taints can be set for a specific instance or as a default value for all instances. Taints set for a particular instance are merged with the default taints and duplicate entries are removed. cluster : nodes : <node-type> : # (1)! default : taints : - \"key1=value1:NoSchedule\" instances : - id : 1 taints : - \"key2=value2:NoExecute\" Node taints can only be applied to control plane (master) and worker nodes. Load balancer properties \ud83d\udd17\ufe0e The following properties can be configured only for load balancers. Virtual IP address (VIP) \ud83d\udd17\ufe0e v2.0.0 Load balancers distribute traffic directed to the control plane across all master nodes. Nevertheless, a load balancer can fail and make the control plane unreachable. To avoid such a situation, multiple load balancers can be configured. They work on the failover principle, i.e. one of them is primary and actively serves incoming traffic, while others are secondary and take over the primary position only if the primary load balancer fails. If one of the secondary load balancers becomes primary, it should still be reachable via the same IP. This IP is usually referred to as a virtual or floating IP (VIP). VIP must be specified if multiple load balancers are configured. It must also be an unused host IP address within the configured network. cluster : nodes : loadBalancer : vip : 168.192.113.200 Virtual router ID \ud83d\udd17\ufe0e v2.1.0 \u2002 Default: 51 When a cluster is created with a virtual IP (VIP) set, Kubitect configures the virtual router redundancy protocol (VRRP), which provides failover for load balancers. A virtual router ID (VRID) identifies the group of VRRP routers. Each group has its own ID. Since there can be only one master in each group, two groups cannot have the same ID. The virtual router ID can be any number between 0 and 255. By default, Kubitect sets the virtual router ID to 51 . If you set up multiple clusters that use VIP, you must ensure that the virtual router ID is different for each cluster. cluster : nodes : loadBalancer : vip : 168.192.113.200 virtualRouterId : 30 # (1)! If the virtual IP (VIP) is not set, the virtual router ID is ignored. Priority \ud83d\udd17\ufe0e v2.1.0 \u2002 Default: 10 Each load balancer has a priority that is used to select a primary load balancer. The one with the highest priority becomes the primary and all others become secondary. If the primary load balancer fails, the next one with the highest priority takes over. If two load balancers have the same priority, the one with the higher sum of IP address digits is selected. The priority can be any number between 0 and 255. The default priority is 10. cluster : nodes : loadBalancer : instances : - id : 1 # (1)! - id : 2 priority : 200 # (2)! Since the load balancer priority for this instance is not specified, it is set to 10. Since this load balancer instance has the highest priority (200 > 10), it becomes the primary load balancer. Port forwarding \ud83d\udd17\ufe0e v2.1.0 By default, all configured load balancers distribute incoming traffic on port 6443 across all control plane nodes. Kubitect allows additional user-defined ports to be configured. The following properties can be configured for each port: name - Name is a unique port identifier. port - Incoming port is a port on which the load balancer listens for incoming traffic. targetPort - Target port is a port where traffic is forwarded by the load balancer. target - Target is a group of nodes to which traffic is forwarded. Possible targets are: masters - control plane nodes workers - worker nodes all - control plane and worker nodes. A unique name and a unique incoming port must be configured for each port. The configuration of target and target port is optional. If target port is not configured, it is set to the same value as the incoming port. If target is not configured, incoming traffic is distributed across worker nodes by default. cluster : nodes : loadBalancer : forwardPorts : - name : https port : 443 # (1)! targetPort : 31200 # (2)! target : all # (3)! Incoming port is the port on which a load balancer listens for incoming traffic. It can be any number between 1 and 65353, excluding ports 6443 (Kubernetes API server) and 22 (SSH). Target port is the port on which the traffic is forwarded. By default, it is set to the same value as the incoming port. Target represents a group of nodes to which incoming traffic is forwarded. Possible values are: masters workers all If the target is not configured, it defaults to the workers . Example usage \ud83d\udd17\ufe0e Set a role to all worker nodes \ud83d\udd17\ufe0e By default worker nodes have no roles ( <none> ). For example, to set node role to all worker nodes in the cluster, set default label with key node-role.kubernetes.io/node . cluster : nodes : worker : default : labels : node-role.kubernetes.io/node : # (1)! instances : ... If the label value is omitted, null is set as the label value. Node roles can be seen by listing cluster nodes with kubectl . NAME STATUS ROLES AGE VERSION local-k8s-cluster-master-1 Ready control-plane,master 19m v1.22.6 local-k8s-cluster-worker-1 Ready node 19m v1.22.6 local-k8s-cluster-worker-2 Ready node 19m v1.22.6 Load balance HTTP requests \ud83d\udd17\ufe0e Kubitect allows users to define custom port forwarding on load balancers. For example, to distribute HTTP and HTTPS requests across all worker nodes, at least one load balancer has to be specified and port forwarding must be configured, as shown in the sample configuration below. cluster : nodes : loadBalancer : forwardPorts : - name : http port : 80 - name : https port : 443 target : all # (1)! instances : - id : 1 When the target is set to all , load balancers distribute traffic across all nodes (master and worker nodes).","title":"Cluster nodes"},{"location":"user-guide/configuration/cluster-nodes/#cluster-nodes","text":"","title":"Cluster nodes"},{"location":"user-guide/configuration/cluster-nodes/#nodes-configuration-structure","text":"Cluster's nodes configuration consists of three node types : Master nodes (control plane) Worker nodes Load balancers For any cluster deployment, at least one master node needs to be configured . Configuring only one master node produces a single-node cluster. In most cases, a multi-node cluster is desired and therefore worker nodes should be configured as well. If the control plane of the cluster contains multiple nodes, at least one load balancer must be configured. Such topology allows the cluster to continue operating normally if any control plane node fails. In addition, configuring multiple load balancers provides failover in case the primary load balancer fails. Kubitect currently supports only stacked control plane, which means that etcd key-value stores are deployed on control plane nodes. Since an etcd cluster requires a majority \"(n/2) + 1\" of nodes to agree to a change in the cluster, an odd number of nodes (1, 3, 5, ...) provides the best fault tolerance. For example, in control planes with 3 nodes, 2 nodes represent the majority, giving a fault tolerance of 1 node. In control planes with 4 nodes, the majority is 3 nodes, which provides the same fault tolerance. For this reason, Kubitect prevents deployment of the cluster whose control plane contains an even number of nodes. The nodes configuration structure is the following: cluster : nodes : masters : ... workers : ... loadBalancers : ... Each node type has two subsections, default and instances . Instances represent an array of actual nodes, while defaults provide the configuration that is applied to all instances of a certain node type. Each default value can also be overwritten by setting the same property for a specific instance. cluster : nodes : <node-type> : default : ... instances : ...","title":"Nodes configuration structure"},{"location":"user-guide/configuration/cluster-nodes/#configuration","text":"","title":"Configuration"},{"location":"user-guide/configuration/cluster-nodes/#common-node-properties","text":"For each instance there is a set of predefined properties that can be set. Some properties apply for all node types, while some properties are specific for a certain node type. Properties that apply for all node types, are referred to as common properties .","title":"Common node properties"},{"location":"user-guide/configuration/cluster-nodes/#instance-id","text":"v2.3.0 \u2002 Required Each instance in the cluster must have an ID that must be unique among all instances of the same node type. The instance ID is used as a suffix for the name of each node. cluster : nodes : <node-type> : instances : - id : 1 - id : compute-1 - id : 77","title":"Instance ID"},{"location":"user-guide/configuration/cluster-nodes/#cpu","text":"v2.0.0 \u2002 Default: 2 vCPU The cpu property defines an amount of vCPU cores assigned to the virtual machine. It can be set for a specific instance or as a default value for all instances. cluster : nodes : <node-type> : default : cpu : 2 instances : - id : 1 # (1)! - id : 2 cpu : 4 # (2)! Since the cpu property is not set for this instance, the default value is used (2). This instance has the cpu property set, and therefore the set value (4) overrides the default value (2). If the property is not set at the instance level or as a default value, Kubitect uses its own default value (2 vCPU). cluster : nodes : <node-type> : instances : - id : 1 # (1)! Since the 'cpu' property is not set at instance level or as a default value, Kubitect sets the value of the 'cpu' property to 2 vCPU .","title":"CPU"},{"location":"user-guide/configuration/cluster-nodes/#ram","text":"v2.0.0 \u2002 Default: 4 GiB The ram property defines an amount of RAM assigned to the virtual machine (in GiB). It can be set for a specific instance or as a default value for all instances. cluster : nodes : <node-type> : default : ram : 8 instances : - id : 1 # (1)! - id : 2 ram : 16 # (2)! Since the ram property is not set for this instance, the default value is used (8 GiB). This instance has the ram property set, and therefore the set value (16 GiB) overrides the default value (8 GiB). If the property is not set at the instance level or as a default value, Kubitect uses its own default value (4 GiB). cluster : nodes : <node-type> : instances : - id : 1 # (1)! Since the ram property is not set at instance level or as a default value, Kubitect sets the value of the ram property to 4 GiB .","title":"RAM"},{"location":"user-guide/configuration/cluster-nodes/#main-disk-size","text":"v2.0.0 \u2002 Default: 32 GiB The mainDiskSize property defines an amount of space assigned to the virtual machine (in GiB). It can be set for a specific instance or as a default value for all instances. cluster : nodes : <node-type> : default : mainDiskSize : 128 instances : - id : 1 # (1)! - id : 2 mainDiskSize : 256 # (2)! Since the mainDiskSize property is not set for this instance, the default value is used (128 GiB). This instance has the mainDiskSize property set, so therefore the set value (256 GiB) overrides the default value (128 GiB). If the property is not set at the instance level or as a default value, Kubitect uses its own default value (32 GiB). cluster : nodes : <node-type> : instances : - id : 1 # (1)! Since the mainDiskSize property is not set at instance level or as a default value, Kubitect sets the value of the mainDiskSize property to 32 GiB .","title":"Main disk size"},{"location":"user-guide/configuration/cluster-nodes/#ip-address","text":"v2.0.0 For each node a static IP address can be set. f no IP address is set for a particular node, a DHCP lease is requested. Kubitect also checks whether all set IP addresses are within the defined network range (see Network CIDR ). cluster : network : mode : nat cidr : 192.168.113.0/24 nodes : <node-type> : instances : - id : 1 ip : 192.168.113.5 # (1)! - id : 2 # (2)! A static IP ( 192.168.113.5 ) is set for this instance. Since no IP address is defined for this instance, a DHCP lease is requested.","title":"IP address"},{"location":"user-guide/configuration/cluster-nodes/#mac-address","text":"v2.0.0 By default, MAC addresses are generated for each virtual machine created, but a custom MAC address can also be set. cluster : nodes : <node-type> : instances : - id : 1 mac : \"52:54:00:00:13:10\" # (1)! - id : 2 # (2)! A custom MAC address ( 52:54:00:00:13:10 ) is set for this instance. Since no MAC address is defined for this instance, the MAC address is generated during cluster creation.","title":"MAC address"},{"location":"user-guide/configuration/cluster-nodes/#host-affinity","text":"v2.0.0 By default, all instances are deployed on the default host . Kubitect can be instructed to deploy the instance on a specific host by specifying the name of the host in the instance configuration. hosts : - name : host1 ... - name : host2 default : true ... cluster : nodes : <node-type> : instances : - id : 1 host : host1 # (1)! - id : 2 # (2)! The instance is deployed on host1 . Since no host is specified, the instance is deployed on the default host ( host2 ).","title":"Host affinity"},{"location":"user-guide/configuration/cluster-nodes/#control-plane-and-worker-node-properties","text":"The following properties can be configured only for control plane or worker nodes.","title":"Control plane and worker node properties"},{"location":"user-guide/configuration/cluster-nodes/#data-disks","text":"v2.2.0 By default, only a main disk (volume) is attached to each provisioned virtual machine. Since the main disk already contains an operating system, so it may not be suitable for storing data. Therefore, additional disks might be required. For example, a Rook can be easily configured to use all the empty disks attached to the virtual machine to form a storage cluster. A name and size (in GiB) must be configured for each data disk. By default, data disks are created in the main resource pool. To create a data disk in a custom data resource pool , the pool property can be set to the name of the desired data resource pool. Also note that the data disk name must be unique among all data disks for a given instance. cluster : nodes : <node-type> : instances : - id : 1 dataDisks : - name : data-volume pool : main # (1)! size : 256 - name : rook-volume pool : rook-pool # (2)! size : 512 When pool property is omitted or set to main , the data disk is created in the main resource pool. Custom data resource pool must be configured in the hosts section. Note Default data disks are currently not supported.","title":"Data disks"},{"location":"user-guide/configuration/cluster-nodes/#node-labels","text":"v2.1.0 Node labels are configured as a dictionary of key-value pairs. They are used to label actual Kubernetes nodes, and therefore can only be applied to control plane (master) and worker nodes. They can be set for a specific instance or as a default value for all instances. Labels set for a specific instance are merged with the default labels. However, labels set at the instance level take precedence over default labels. cluster : nodes : <node-type> : # (1)! default : labels : label-key-1 : def-label-value-1 label-key-2 : def-label-value-2 instances : - id : 1 labels : # (2)! label-key-3 : instance-label-value-3 - id : 2 labels : # (3)! label-key-1 : new-label-value-1 Node labels can only be applied to control plane (master) and worker nodes. Labels defined at the instance level are merged with default labels. As a result, the following labels are applied to this instance: label-key-1 : def-label-value-1 label-key-2 : def-label-value-2 label-key-3 : instance-label-value-3 Labels defined at the instance level take precedence over default labels. As a result, the following labels are applied to this instance: label-key-1 : new-label-value-1 label-key-2 : def-label-value-2","title":"Node labels"},{"location":"user-guide/configuration/cluster-nodes/#node-taints","text":"v2.2.0 Node taints are configured as a list of strings in the format key=value:effect . Similar to node labels, taints can only be applied to control plane (master) and worker nodes. Taints can be set for a specific instance or as a default value for all instances. Taints set for a particular instance are merged with the default taints and duplicate entries are removed. cluster : nodes : <node-type> : # (1)! default : taints : - \"key1=value1:NoSchedule\" instances : - id : 1 taints : - \"key2=value2:NoExecute\" Node taints can only be applied to control plane (master) and worker nodes.","title":"Node taints"},{"location":"user-guide/configuration/cluster-nodes/#load-balancer-properties","text":"The following properties can be configured only for load balancers.","title":"Load balancer properties"},{"location":"user-guide/configuration/cluster-nodes/#virtual-ip-address-vip","text":"v2.0.0 Load balancers distribute traffic directed to the control plane across all master nodes. Nevertheless, a load balancer can fail and make the control plane unreachable. To avoid such a situation, multiple load balancers can be configured. They work on the failover principle, i.e. one of them is primary and actively serves incoming traffic, while others are secondary and take over the primary position only if the primary load balancer fails. If one of the secondary load balancers becomes primary, it should still be reachable via the same IP. This IP is usually referred to as a virtual or floating IP (VIP). VIP must be specified if multiple load balancers are configured. It must also be an unused host IP address within the configured network. cluster : nodes : loadBalancer : vip : 168.192.113.200","title":"Virtual IP address (VIP)"},{"location":"user-guide/configuration/cluster-nodes/#virtual-router-id","text":"v2.1.0 \u2002 Default: 51 When a cluster is created with a virtual IP (VIP) set, Kubitect configures the virtual router redundancy protocol (VRRP), which provides failover for load balancers. A virtual router ID (VRID) identifies the group of VRRP routers. Each group has its own ID. Since there can be only one master in each group, two groups cannot have the same ID. The virtual router ID can be any number between 0 and 255. By default, Kubitect sets the virtual router ID to 51 . If you set up multiple clusters that use VIP, you must ensure that the virtual router ID is different for each cluster. cluster : nodes : loadBalancer : vip : 168.192.113.200 virtualRouterId : 30 # (1)! If the virtual IP (VIP) is not set, the virtual router ID is ignored.","title":"Virtual router ID"},{"location":"user-guide/configuration/cluster-nodes/#priority","text":"v2.1.0 \u2002 Default: 10 Each load balancer has a priority that is used to select a primary load balancer. The one with the highest priority becomes the primary and all others become secondary. If the primary load balancer fails, the next one with the highest priority takes over. If two load balancers have the same priority, the one with the higher sum of IP address digits is selected. The priority can be any number between 0 and 255. The default priority is 10. cluster : nodes : loadBalancer : instances : - id : 1 # (1)! - id : 2 priority : 200 # (2)! Since the load balancer priority for this instance is not specified, it is set to 10. Since this load balancer instance has the highest priority (200 > 10), it becomes the primary load balancer.","title":"Priority"},{"location":"user-guide/configuration/cluster-nodes/#port-forwarding","text":"v2.1.0 By default, all configured load balancers distribute incoming traffic on port 6443 across all control plane nodes. Kubitect allows additional user-defined ports to be configured. The following properties can be configured for each port: name - Name is a unique port identifier. port - Incoming port is a port on which the load balancer listens for incoming traffic. targetPort - Target port is a port where traffic is forwarded by the load balancer. target - Target is a group of nodes to which traffic is forwarded. Possible targets are: masters - control plane nodes workers - worker nodes all - control plane and worker nodes. A unique name and a unique incoming port must be configured for each port. The configuration of target and target port is optional. If target port is not configured, it is set to the same value as the incoming port. If target is not configured, incoming traffic is distributed across worker nodes by default. cluster : nodes : loadBalancer : forwardPorts : - name : https port : 443 # (1)! targetPort : 31200 # (2)! target : all # (3)! Incoming port is the port on which a load balancer listens for incoming traffic. It can be any number between 1 and 65353, excluding ports 6443 (Kubernetes API server) and 22 (SSH). Target port is the port on which the traffic is forwarded. By default, it is set to the same value as the incoming port. Target represents a group of nodes to which incoming traffic is forwarded. Possible values are: masters workers all If the target is not configured, it defaults to the workers .","title":"Port forwarding"},{"location":"user-guide/configuration/cluster-nodes/#example-usage","text":"","title":"Example usage"},{"location":"user-guide/configuration/cluster-nodes/#set-a-role-to-all-worker-nodes","text":"By default worker nodes have no roles ( <none> ). For example, to set node role to all worker nodes in the cluster, set default label with key node-role.kubernetes.io/node . cluster : nodes : worker : default : labels : node-role.kubernetes.io/node : # (1)! instances : ... If the label value is omitted, null is set as the label value. Node roles can be seen by listing cluster nodes with kubectl . NAME STATUS ROLES AGE VERSION local-k8s-cluster-master-1 Ready control-plane,master 19m v1.22.6 local-k8s-cluster-worker-1 Ready node 19m v1.22.6 local-k8s-cluster-worker-2 Ready node 19m v1.22.6","title":"Set a role to all worker nodes"},{"location":"user-guide/configuration/cluster-nodes/#load-balance-http-requests","text":"Kubitect allows users to define custom port forwarding on load balancers. For example, to distribute HTTP and HTTPS requests across all worker nodes, at least one load balancer has to be specified and port forwarding must be configured, as shown in the sample configuration below. cluster : nodes : loadBalancer : forwardPorts : - name : http port : 80 - name : https port : 443 target : all # (1)! instances : - id : 1 When the target is set to all , load balancers distribute traffic across all nodes (master and worker nodes).","title":"Load balance HTTP requests"},{"location":"user-guide/configuration/hosts/","text":"Hosts configuration \ud83d\udd17\ufe0e Defining Kubitect hosts is esential. Hosts represent the target servers where the cluster will be deployed. Every valid configuration must contain at least one host, but there can be as many hosts as needed. The host can be either a local or remote server. Configuration \ud83d\udd17\ufe0e Localhost \ud83d\udd17\ufe0e v2.0.0 When cluster is deployed on the server where the Kubitect command line tool is installed, a host whose connection type is set to local needs to be specified. Such host is also refered to as localhost. hosts : - name : localhost # (1)! connection : type : local Custom unique name of the host. Remote hosts \ud83d\udd17\ufe0e v2.0.0 When cluster is deployed on the remote host, the IP address of the remote host along with the SSH credentails needs to be specified for the host. hosts : - name : my-remote-host connection : type : remote user : myuser ip : 10.10.40.143 # (1)! ssh : keyfile : \"~/.ssh/id_rsa_server1\" # (2)! IP address of the remote host. Path to the password-less SSH key file required for establishing connection with the remote host. Default is ~/.ssh/id_rsa . Host's SSH port \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: 22 By default, port 22 is used for SSH. If host is running SSH client on a different port, it is possible to change it for each host separately. hosts : - name : remote-host connection : type : remote ssh : port : 1234 Host verification (known SSH hosts) \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: false By default remote hosts are not verified in the known SSH hosts. If for any reason host verification is desired, you can enable it for each host separately. hosts : - name : remote-host connection : type : remote ssh : verify : true Default host \ud83d\udd17\ufe0e v2.0.0 If the host is specified as the default, all instances that do not point to a specific host are deployed to the default host. If the default host is not specified, these instances are deployed on the first host in the list. hosts : - name : localhost connection : type : local - name : default-host default : true ... Main resource pool \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: /var/lib/libvirt/images/ The main resource pool path defines the location on the host where main disks (volumes) are created for each node provisioned on that particular host. Because the main resource pool contains volumes on which the node's operating system and all required packages are installed, it is recommended that the main resource pool is created on fast storage devices such as SSD disks. By default, main disk pool path is set to /var/lib/libvirt/images/ . hosts : - name : host1 # (1)! - name : host2 mainResourcePoolPath : /mnt/ssd/kubitect/ # (2)! Because the main resource pool path for this host is not set, the default path ( /var/lib/libvirt/images/ ) is used. The main resource pool path is set for this host, so the node's main disks are created in this location. Data resource pools \ud83d\udd17\ufe0e v2.0.0 Data resource pools define additional resource pools ( besides the required main resource pool ). For example, main disks contain the OS image and should be created on fast storage devices, while data resource pools can be used to attach additional virtual disks that can be created on slower storage devices such as HDDs. Multiple data resource pools can be defined on each host. Each configured pool must have a unique name on a particular host. The data resource pool name is used to associate the virtual disks defined in the node configuration with the actual data resource pool. The path of the data resources is set to /var/lib/libvirt/images by default, but can be easily configured with the path property. hosts : - name : host1 dataResourcePools : - name : rook-pool path : /mnt/hdd/kubitect/pools/ - name : data-pool # (1)! If the path of the resource pool is not specified, it is created under the path /var/lib/libvirt/images/ . Example usage \ud83d\udd17\ufe0e Multiple hosts \ud83d\udd17\ufe0e With Kubitect the cluster can be deployed on multiple hosts. All hosts need to be specified in the configuration file. hosts : - name : localhost connection : type : local - name : remote-host-1 connection : type : remote user : myuser ip : 10.10.40.143 ssh : port : 123 keyfile : \"~/.ssh/id_rsa_server1\" - name : remote-host-2 default : true connection : type : remote user : myuser ip : 10.10.40.145 ssh : keyfile : \"~/.ssh/id_rsa_server2\" ...","title":"Hosts"},{"location":"user-guide/configuration/hosts/#hosts-configuration","text":"Defining Kubitect hosts is esential. Hosts represent the target servers where the cluster will be deployed. Every valid configuration must contain at least one host, but there can be as many hosts as needed. The host can be either a local or remote server.","title":"Hosts configuration"},{"location":"user-guide/configuration/hosts/#configuration","text":"","title":"Configuration"},{"location":"user-guide/configuration/hosts/#localhost","text":"v2.0.0 When cluster is deployed on the server where the Kubitect command line tool is installed, a host whose connection type is set to local needs to be specified. Such host is also refered to as localhost. hosts : - name : localhost # (1)! connection : type : local Custom unique name of the host.","title":"Localhost"},{"location":"user-guide/configuration/hosts/#remote-hosts","text":"v2.0.0 When cluster is deployed on the remote host, the IP address of the remote host along with the SSH credentails needs to be specified for the host. hosts : - name : my-remote-host connection : type : remote user : myuser ip : 10.10.40.143 # (1)! ssh : keyfile : \"~/.ssh/id_rsa_server1\" # (2)! IP address of the remote host. Path to the password-less SSH key file required for establishing connection with the remote host. Default is ~/.ssh/id_rsa .","title":"Remote hosts"},{"location":"user-guide/configuration/hosts/#hosts-ssh-port","text":"v2.0.0 \u2002 Default: 22 By default, port 22 is used for SSH. If host is running SSH client on a different port, it is possible to change it for each host separately. hosts : - name : remote-host connection : type : remote ssh : port : 1234","title":"Host's SSH port"},{"location":"user-guide/configuration/hosts/#host-verification-known-ssh-hosts","text":"v2.0.0 \u2002 Default: false By default remote hosts are not verified in the known SSH hosts. If for any reason host verification is desired, you can enable it for each host separately. hosts : - name : remote-host connection : type : remote ssh : verify : true","title":"Host verification (known SSH hosts)"},{"location":"user-guide/configuration/hosts/#default-host","text":"v2.0.0 If the host is specified as the default, all instances that do not point to a specific host are deployed to the default host. If the default host is not specified, these instances are deployed on the first host in the list. hosts : - name : localhost connection : type : local - name : default-host default : true ...","title":"Default host"},{"location":"user-guide/configuration/hosts/#main-resource-pool","text":"v2.0.0 \u2002 Default: /var/lib/libvirt/images/ The main resource pool path defines the location on the host where main disks (volumes) are created for each node provisioned on that particular host. Because the main resource pool contains volumes on which the node's operating system and all required packages are installed, it is recommended that the main resource pool is created on fast storage devices such as SSD disks. By default, main disk pool path is set to /var/lib/libvirt/images/ . hosts : - name : host1 # (1)! - name : host2 mainResourcePoolPath : /mnt/ssd/kubitect/ # (2)! Because the main resource pool path for this host is not set, the default path ( /var/lib/libvirt/images/ ) is used. The main resource pool path is set for this host, so the node's main disks are created in this location.","title":"Main resource pool"},{"location":"user-guide/configuration/hosts/#data-resource-pools","text":"v2.0.0 Data resource pools define additional resource pools ( besides the required main resource pool ). For example, main disks contain the OS image and should be created on fast storage devices, while data resource pools can be used to attach additional virtual disks that can be created on slower storage devices such as HDDs. Multiple data resource pools can be defined on each host. Each configured pool must have a unique name on a particular host. The data resource pool name is used to associate the virtual disks defined in the node configuration with the actual data resource pool. The path of the data resources is set to /var/lib/libvirt/images by default, but can be easily configured with the path property. hosts : - name : host1 dataResourcePools : - name : rook-pool path : /mnt/hdd/kubitect/pools/ - name : data-pool # (1)! If the path of the resource pool is not specified, it is created under the path /var/lib/libvirt/images/ .","title":"Data resource pools"},{"location":"user-guide/configuration/hosts/#example-usage","text":"","title":"Example usage"},{"location":"user-guide/configuration/hosts/#multiple-hosts","text":"With Kubitect the cluster can be deployed on multiple hosts. All hosts need to be specified in the configuration file. hosts : - name : localhost connection : type : local - name : remote-host-1 connection : type : remote user : myuser ip : 10.10.40.143 ssh : port : 123 keyfile : \"~/.ssh/id_rsa_server1\" - name : remote-host-2 default : true connection : type : remote user : myuser ip : 10.10.40.145 ssh : keyfile : \"~/.ssh/id_rsa_server2\" ...","title":"Multiple hosts"},{"location":"user-guide/configuration/kubernetes/","text":"Kubernetes configuration \ud83d\udd17\ufe0e The Kubernetes section of the configuration file contains properties that are closely related to Kubernetes, such as Kubernetes version, network plugin, and DNS mode. In addition, the Kubespray project version and URL can also be specified in this section of the Kubitect configuration. Configuration \ud83d\udd17\ufe0e Kubespray version \ud83d\udd17\ufe0e v2.0.0 \u2002 Required As Kubitect relays on the Kubespray for deploying a Kubernetes cluster, a Kubespray project is cloned during a cluster creation. This property defines the version of the Kubespray to be cloned. All Kubespray versions can be found on on their GitHub release page . kubernetes : kubespray : version : v2.20.0 Kubespray URL \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: https://github.com/kubernetes-sigs/kubespray By default, Kubespray is cloned from the official GitHub repository. If there is a need to use a custom forked version of the project, the url to the repository can be specified with this property. kubernetes : kubespray : url : https://github.com/kubernetes-sigs/kubespray Kubernetes version \ud83d\udd17\ufe0e v2.0.0 \u2002 Required The Kubernetes version must be defined in the Kubitect configuration. It must be ensured that Kubespray supports the provided Kubernetes version, otherwise the cluster setup will fail. In addition, Kubernetes version must be prefixed with the v . kubernetes : version : v1.23.6 Kubernetes network plugin \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: calico Kubitect supports multiple Kubernetes network plugins. Currently, the following network plugins are supported: calico canal cilium flannel kube-router weave If the network plugin is not set in the Kubitect configuration file, calico is used by default. kubernetes : networkPlugin : calico Kubernetes DNS mode \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: coredns Two DNS servers are supproted, coredns and kubedns . It is highly recommended to use CoreDNS, which has replaced the kube-dns. If this property is omitted, CoreDNS is used. kubernetes : dnsMode : coredns Copy kubeconfig \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: false Kubitect provides option to automatically copy the Kubeconfig file to ~/.kube/config path. By default, this option is disabled, as it can overwrite an existing file. kubernetes : other : copyKubeconfig : true Auto renew control plane certificates \ud83d\udd17\ufe0e v2.2.0 \u2002 Default: false Control plane certificates are valid for 1 year and are renewed each time the cluster is upgraded. In some rare cases, this can cause clusters that are not upgraded frequently to stop working properly. Therefore, the control plane certificates can be renewed automatically on the first Monday of each month by setting the autoRenewCertificates property to true. kubernetes : other : autoRenewCertificates : true Example usage \ud83d\udd17\ufe0e Minimal configuration \ud83d\udd17\ufe0e The minimalistic Kubernetes configuration encompasses setting Kubernetes and Kubesrpay versions. kuberentes : version : v1.23.7 kubespray : version : v2.20.0","title":"Kubernetes"},{"location":"user-guide/configuration/kubernetes/#kubernetes-configuration","text":"The Kubernetes section of the configuration file contains properties that are closely related to Kubernetes, such as Kubernetes version, network plugin, and DNS mode. In addition, the Kubespray project version and URL can also be specified in this section of the Kubitect configuration.","title":"Kubernetes configuration"},{"location":"user-guide/configuration/kubernetes/#configuration","text":"","title":"Configuration"},{"location":"user-guide/configuration/kubernetes/#kubespray-version","text":"v2.0.0 \u2002 Required As Kubitect relays on the Kubespray for deploying a Kubernetes cluster, a Kubespray project is cloned during a cluster creation. This property defines the version of the Kubespray to be cloned. All Kubespray versions can be found on on their GitHub release page . kubernetes : kubespray : version : v2.20.0","title":"Kubespray version"},{"location":"user-guide/configuration/kubernetes/#kubespray-url","text":"v2.0.0 \u2002 Default: https://github.com/kubernetes-sigs/kubespray By default, Kubespray is cloned from the official GitHub repository. If there is a need to use a custom forked version of the project, the url to the repository can be specified with this property. kubernetes : kubespray : url : https://github.com/kubernetes-sigs/kubespray","title":"Kubespray URL"},{"location":"user-guide/configuration/kubernetes/#kubernetes-version","text":"v2.0.0 \u2002 Required The Kubernetes version must be defined in the Kubitect configuration. It must be ensured that Kubespray supports the provided Kubernetes version, otherwise the cluster setup will fail. In addition, Kubernetes version must be prefixed with the v . kubernetes : version : v1.23.6","title":"Kubernetes version"},{"location":"user-guide/configuration/kubernetes/#kubernetes-network-plugin","text":"v2.0.0 \u2002 Default: calico Kubitect supports multiple Kubernetes network plugins. Currently, the following network plugins are supported: calico canal cilium flannel kube-router weave If the network plugin is not set in the Kubitect configuration file, calico is used by default. kubernetes : networkPlugin : calico","title":"Kubernetes network plugin"},{"location":"user-guide/configuration/kubernetes/#kubernetes-dns-mode","text":"v2.0.0 \u2002 Default: coredns Two DNS servers are supproted, coredns and kubedns . It is highly recommended to use CoreDNS, which has replaced the kube-dns. If this property is omitted, CoreDNS is used. kubernetes : dnsMode : coredns","title":"Kubernetes DNS mode"},{"location":"user-guide/configuration/kubernetes/#copy-kubeconfig","text":"v2.0.0 \u2002 Default: false Kubitect provides option to automatically copy the Kubeconfig file to ~/.kube/config path. By default, this option is disabled, as it can overwrite an existing file. kubernetes : other : copyKubeconfig : true","title":"Copy kubeconfig"},{"location":"user-guide/configuration/kubernetes/#auto-renew-control-plane-certificates","text":"v2.2.0 \u2002 Default: false Control plane certificates are valid for 1 year and are renewed each time the cluster is upgraded. In some rare cases, this can cause clusters that are not upgraded frequently to stop working properly. Therefore, the control plane certificates can be renewed automatically on the first Monday of each month by setting the autoRenewCertificates property to true. kubernetes : other : autoRenewCertificates : true","title":"Auto renew control plane certificates"},{"location":"user-guide/configuration/kubernetes/#example-usage","text":"","title":"Example usage"},{"location":"user-guide/configuration/kubernetes/#minimal-configuration","text":"The minimalistic Kubernetes configuration encompasses setting Kubernetes and Kubesrpay versions. kuberentes : version : v1.23.7 kubespray : version : v2.20.0","title":"Minimal configuration"},{"location":"user-guide/configuration/project-metadata/","text":"Project metadata \ud83d\udd17\ufe0e The project metadata contains properties that are closely related to the project itself. These properties are rather fine-grained settings, as the default values should cover most use cases. Configuration \ud83d\udd17\ufe0e Project URL \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: https://github.com/MusicDin/kubitect By default, Kubitect clones the source code of the project from the official GitHub repository . To use a custom repository instead, set the kubitect.url property in the configuration file to the desired url. kubitect : url : \"https://github.com/user/repo\" Project version \ud83d\udd17\ufe0e v2.0.0 \u2002 Default: CLI tool version If project version is not specified in the configuration file, version of the CLI tool will be used by default ( recommended ). To set the specific project version, set kubitect.version property in the configuration file. kubitect : version : v2.2.0 # (1)! Version can be either a tag ( v2.2.0 ) or a branch name ( master ). All Kubitect versions can be found on the release page . Warning A mismatched CLI and project version can lead to unexpected behavior.","title":"Project metadata"},{"location":"user-guide/configuration/project-metadata/#project-metadata","text":"The project metadata contains properties that are closely related to the project itself. These properties are rather fine-grained settings, as the default values should cover most use cases.","title":"Project metadata"},{"location":"user-guide/configuration/project-metadata/#configuration","text":"","title":"Configuration"},{"location":"user-guide/configuration/project-metadata/#project-url","text":"v2.0.0 \u2002 Default: https://github.com/MusicDin/kubitect By default, Kubitect clones the source code of the project from the official GitHub repository . To use a custom repository instead, set the kubitect.url property in the configuration file to the desired url. kubitect : url : \"https://github.com/user/repo\"","title":"Project URL"},{"location":"user-guide/configuration/project-metadata/#project-version","text":"v2.0.0 \u2002 Default: CLI tool version If project version is not specified in the configuration file, version of the CLI tool will be used by default ( recommended ). To set the specific project version, set kubitect.version property in the configuration file. kubitect : version : v2.2.0 # (1)! Version can be either a tag ( v2.2.0 ) or a branch name ( master ). All Kubitect versions can be found on the release page . Warning A mismatched CLI and project version can lead to unexpected behavior.","title":"Project version"},{"location":"user-guide/management/creating/","text":"Creating the cluster \ud83d\udd17\ufe0e With Kubitect, clusters are created by applying the cluster configuration to the Kubitect CLI tool. If no cluster configuration is not specified, the default configuration is applied, as described in the Quick start guide. Applying default configuration \ud83d\udd17\ufe0e To create a cluster with the default configuration, simply run the following command. kubitect apply Applying custom configuration \ud83d\udd17\ufe0e To create a cluster with the custom configuration file, run the apply command with the --config flag. kubitect apply --flag <PathToClusterConfig> Specifying cluster directory name \ud83d\udd17\ufe0e The configuration files for each cluster created with Kubitect are generated under the path ~/.kubitect/clusters/<ClusterName> . If no cluster name is specified, cluster name default is used. The name of the cluster can be specified with the flag --cluster . kubitect apply --cluster <ClusterName>","title":"Creating the cluster"},{"location":"user-guide/management/creating/#creating-the-cluster","text":"With Kubitect, clusters are created by applying the cluster configuration to the Kubitect CLI tool. If no cluster configuration is not specified, the default configuration is applied, as described in the Quick start guide.","title":"Creating the cluster"},{"location":"user-guide/management/creating/#applying-default-configuration","text":"To create a cluster with the default configuration, simply run the following command. kubitect apply","title":"Applying default configuration"},{"location":"user-guide/management/creating/#applying-custom-configuration","text":"To create a cluster with the custom configuration file, run the apply command with the --config flag. kubitect apply --flag <PathToClusterConfig>","title":"Applying custom configuration"},{"location":"user-guide/management/creating/#specifying-cluster-directory-name","text":"The configuration files for each cluster created with Kubitect are generated under the path ~/.kubitect/clusters/<ClusterName> . If no cluster name is specified, cluster name default is used. The name of the cluster can be specified with the flag --cluster . kubitect apply --cluster <ClusterName>","title":"Specifying cluster directory name"},{"location":"user-guide/management/destroying/","text":"Destroying the cluster \ud83d\udd17\ufe0e The destruction of the cluster consists of 2 parts. The first part destroys all active cluster components, while the second part deletes all configuration files for a given cluster. Destroy the cluster \ud83d\udd17\ufe0e To destroy a specific cluster, simply run the destroy command, specifying the name of the cluster to be destroyed. kubitect destroy --cluster my-cluster This command destroys all active cluster components, including virtual machines, virtual networks, and resource pools, while preserving important configuration files, such as the Kubitect configuration. Therefore, the cluster configuration file can be retrieved even after the cluster is destroyed. Purge the cluster \ud83d\udd17\ufe0e A purge command deletes all files associated with the cluster, including initialized virtual environments and configuration files. It can be executed only if the cluster is already destroyed. kubitect purge --cluster my-cluster","title":"Destroying the cluster"},{"location":"user-guide/management/destroying/#destroying-the-cluster","text":"The destruction of the cluster consists of 2 parts. The first part destroys all active cluster components, while the second part deletes all configuration files for a given cluster.","title":"Destroying the cluster"},{"location":"user-guide/management/destroying/#destroy-the-cluster","text":"To destroy a specific cluster, simply run the destroy command, specifying the name of the cluster to be destroyed. kubitect destroy --cluster my-cluster This command destroys all active cluster components, including virtual machines, virtual networks, and resource pools, while preserving important configuration files, such as the Kubitect configuration. Therefore, the cluster configuration file can be retrieved even after the cluster is destroyed.","title":"Destroy the cluster"},{"location":"user-guide/management/destroying/#purge-the-cluster","text":"A purge command deletes all files associated with the cluster, including initialized virtual environments and configuration files. It can be executed only if the cluster is already destroyed. kubitect purge --cluster my-cluster","title":"Purge the cluster"},{"location":"user-guide/management/scaling/","text":"Scaling the cluster \ud83d\udd17\ufe0e Any cluster created with Kubitect can be subsequently scaled. To do so, simply change the configuration and reapply it using the scale action. Export the cluster configuration \ud83d\udd17\ufe0e Exporting the current cluster configuration is optional, but strongly recommended to ensure that changes are made to the latest version of the configuration. The cluster configuration file can be exported using the export command. kubitect export config --cluster my-cluster > my-cluster.yaml Scale the cluster \ud83d\udd17\ufe0e Warning Currently, only worker nodes can scale seamlessly. The ultimate goal is to be able to scale every node type in the cluster with a single command. It is planned to address this issue in one of the following releases. Kubitect supports the simultaneous addition and removal of multiple worker nodes. In the configuration file, add new workers or remove/comment workers from the cluster.nodes.worker.instances list. my-cluster.yaml cluster : ... nodes : ... worker : instances : - id : 1 #- id: 2 # Worker node to be removed - id : 3 # New worker node - id : 4 # New worker node Apply the modified configuration with action set to scale : kubitect apply --config my-cluster.yaml --cluster my-cluster --action scale As a result, the worker node with ID 2 is removed and the worker nodes with IDs 3 and 4 are added to the cluster.","title":"Scaling the cluster"},{"location":"user-guide/management/scaling/#scaling-the-cluster","text":"Any cluster created with Kubitect can be subsequently scaled. To do so, simply change the configuration and reapply it using the scale action.","title":"Scaling the cluster"},{"location":"user-guide/management/scaling/#export-the-cluster-configuration","text":"Exporting the current cluster configuration is optional, but strongly recommended to ensure that changes are made to the latest version of the configuration. The cluster configuration file can be exported using the export command. kubitect export config --cluster my-cluster > my-cluster.yaml","title":"Export the cluster configuration"},{"location":"user-guide/management/scaling/#scale-the-cluster","text":"Warning Currently, only worker nodes can scale seamlessly. The ultimate goal is to be able to scale every node type in the cluster with a single command. It is planned to address this issue in one of the following releases. Kubitect supports the simultaneous addition and removal of multiple worker nodes. In the configuration file, add new workers or remove/comment workers from the cluster.nodes.worker.instances list. my-cluster.yaml cluster : ... nodes : ... worker : instances : - id : 1 #- id: 2 # Worker node to be removed - id : 3 # New worker node - id : 4 # New worker node Apply the modified configuration with action set to scale : kubitect apply --config my-cluster.yaml --cluster my-cluster --action scale As a result, the worker node with ID 2 is removed and the worker nodes with IDs 3 and 4 are added to the cluster.","title":"Scale the cluster"},{"location":"user-guide/management/upgrading/","text":"Upgrading the cluster \ud83d\udd17\ufe0e A running Kubernetes cluster can be upgraded to a higher version by increasing the Kubernetes version in the cluster's configuration file and reapplying it using the upgrade action. Export the cluster configuration \ud83d\udd17\ufe0e Exporting the current cluster configuration is optional, but strongly recommended to ensure that changes are made to the latest version of the configuration. The cluster configuration file can be exported using the export command. kubitect export config --cluster my-cluster > my-cluster.yaml Upgrade the cluster \ud83d\udd17\ufe0e Important Do not skip releases when upgrading--upgrade by one tag at a time. For more information read Kubespray upgrades . Before upgrading the cluster, ensure that Kubespray supports a specific Kubernetes version. In the cluster configuration file, set the desired Kubernetes version and adjust the Kubespray version if necessary. Example: cluster.yaml kubernetes : version : v1.22.5 # Old value: v1.21.6 ... kubespray : version : v2.18.0 # Old value: v2.17.1 ... Apply the modified configuration using upgrade action. kubitect apply --config cluster.yaml --action upgrade The cluster is upgraded using the in-place strategy, i.e., the nodes are upgraded one after the other, making each node unavailable for the duration of its upgrade.","title":"Upgrading the cluster"},{"location":"user-guide/management/upgrading/#upgrading-the-cluster","text":"A running Kubernetes cluster can be upgraded to a higher version by increasing the Kubernetes version in the cluster's configuration file and reapplying it using the upgrade action.","title":"Upgrading the cluster"},{"location":"user-guide/management/upgrading/#export-the-cluster-configuration","text":"Exporting the current cluster configuration is optional, but strongly recommended to ensure that changes are made to the latest version of the configuration. The cluster configuration file can be exported using the export command. kubitect export config --cluster my-cluster > my-cluster.yaml","title":"Export the cluster configuration"},{"location":"user-guide/management/upgrading/#upgrade-the-cluster","text":"Important Do not skip releases when upgrading--upgrade by one tag at a time. For more information read Kubespray upgrades . Before upgrading the cluster, ensure that Kubespray supports a specific Kubernetes version. In the cluster configuration file, set the desired Kubernetes version and adjust the Kubespray version if necessary. Example: cluster.yaml kubernetes : version : v1.22.5 # Old value: v1.21.6 ... kubespray : version : v2.18.0 # Old value: v2.17.1 ... Apply the modified configuration using upgrade action. kubitect apply --config cluster.yaml --action upgrade The cluster is upgraded using the in-place strategy, i.e., the nodes are upgraded one after the other, making each node unavailable for the duration of its upgrade.","title":"Upgrade the cluster"},{"location":"user-guide/reference/cli/","text":"CLI reference \ud83d\udd17\ufe0e This document contains a reference of the Kubitect CLI tool. It documents each command along with its flags. Tip All available commands can be displayed by running kubitect --help or simply kubitect -h . To see the help for a particular command, run kubitect command -h . Kubitect commands \ud83d\udd17\ufe0e kubitect apply \ud83d\udd17\ufe0e Apply the cluster configuration. Usage kubitect apply [ flags ] Flags -a , --action <string> \u2003 cluster action: create | scale | upgrade (default: create ) --auto-approve \u2003 automatically approve any user permission requests --cluster <string> \u2003 name of the cluster to be used (default: default ) -c , --config <string> \u2003 path to the cluster config file -l , --local \u2003 use a current directory as the cluster path kubitect destroy \ud83d\udd17\ufe0e Destroy the cluster. Configuration files such as cluster configuration are left in the cluster directory. Usage kubitect destroy [ flags ] Flags --auto-approve \u2003 automatically approve any user permission requests --cluster <string> \u2003 name of the cluster to be used (default: default ) -l , --local \u2003 use a current directory as the cluster path kubitect export config \ud83d\udd17\ufe0e Print cluster's configuration file to the standard output. Usage kubitect export config [ flags ] Flags --cluster <string> \u2003 name of the cluster to be used (default: default ) -l , --local \u2003 use a current directory as the cluster path kubitect export kubeconfig \ud83d\udd17\ufe0e Print cluster's kubeconfig to the standard output. Usage kubitect export kubeconfig [ flags ] Flags --cluster <string> \u2003 name of the cluster to be used (default: default ) -l , --local \u2003 use a current directory as the cluster path kubitect list clusters \ud83d\udd17\ufe0e List Kubitect clusters. Usage kubitect list clusters kubitect purge \ud83d\udd17\ufe0e Purge the cluster directory. Purge commands removes all files generated during cluster creation. Note that only destroyed clusters can be purged. Usage kubitect purge [ flags ] Flags --auto-approve \u2003 automatically approve any user permission requests --cluster <string> \u2003 name of the cluster to be used (default: default ) Autogenerated commands \ud83d\udd17\ufe0e kubitect completion \ud83d\udd17\ufe0e Generate the autocompletion script for Kubitect for the specified shell. Usage kubitect completion [ command ] Commands bash \u2003 Generate the autocompletion script for bash. fish \u2003 Generate the autocompletion script for fish. zsh \u2003 Generate the autocompletion script for zsh. Tip Run kubitect completion shell -h for instructions how to add autocompletion for a specific shell. kubitect help \ud83d\udd17\ufe0e Help provides help for any command in the application. Simply type kubitect help [path to command] for full details. Usage kubitect help [ command ] or kubitect [ command ] -h Other \ud83d\udd17\ufe0e Version flag \ud83d\udd17\ufe0e Print Kubitect CLI tool version. Usage kubitect -v or kubitect --version Debug flag \ud83d\udd17\ufe0e Enable debug messages. This can be especially handy with the apply command. Usage kubitect [ command ] --debug","title":"CLI tool reference"},{"location":"user-guide/reference/cli/#cli-reference","text":"This document contains a reference of the Kubitect CLI tool. It documents each command along with its flags. Tip All available commands can be displayed by running kubitect --help or simply kubitect -h . To see the help for a particular command, run kubitect command -h .","title":"CLI reference"},{"location":"user-guide/reference/cli/#kubitect-commands","text":"","title":"Kubitect commands"},{"location":"user-guide/reference/cli/#kubitect-apply","text":"Apply the cluster configuration. Usage kubitect apply [ flags ] Flags -a , --action <string> \u2003 cluster action: create | scale | upgrade (default: create ) --auto-approve \u2003 automatically approve any user permission requests --cluster <string> \u2003 name of the cluster to be used (default: default ) -c , --config <string> \u2003 path to the cluster config file -l , --local \u2003 use a current directory as the cluster path","title":"kubitect apply"},{"location":"user-guide/reference/cli/#kubitect-destroy","text":"Destroy the cluster. Configuration files such as cluster configuration are left in the cluster directory. Usage kubitect destroy [ flags ] Flags --auto-approve \u2003 automatically approve any user permission requests --cluster <string> \u2003 name of the cluster to be used (default: default ) -l , --local \u2003 use a current directory as the cluster path","title":"kubitect destroy"},{"location":"user-guide/reference/cli/#kubitect-export-config","text":"Print cluster's configuration file to the standard output. Usage kubitect export config [ flags ] Flags --cluster <string> \u2003 name of the cluster to be used (default: default ) -l , --local \u2003 use a current directory as the cluster path","title":"kubitect export config"},{"location":"user-guide/reference/cli/#kubitect-export-kubeconfig","text":"Print cluster's kubeconfig to the standard output. Usage kubitect export kubeconfig [ flags ] Flags --cluster <string> \u2003 name of the cluster to be used (default: default ) -l , --local \u2003 use a current directory as the cluster path","title":"kubitect export kubeconfig"},{"location":"user-guide/reference/cli/#kubitect-list-clusters","text":"List Kubitect clusters. Usage kubitect list clusters","title":"kubitect list clusters"},{"location":"user-guide/reference/cli/#kubitect-purge","text":"Purge the cluster directory. Purge commands removes all files generated during cluster creation. Note that only destroyed clusters can be purged. Usage kubitect purge [ flags ] Flags --auto-approve \u2003 automatically approve any user permission requests --cluster <string> \u2003 name of the cluster to be used (default: default )","title":"kubitect purge"},{"location":"user-guide/reference/cli/#autogenerated-commands","text":"","title":"Autogenerated commands"},{"location":"user-guide/reference/cli/#kubitect-completion","text":"Generate the autocompletion script for Kubitect for the specified shell. Usage kubitect completion [ command ] Commands bash \u2003 Generate the autocompletion script for bash. fish \u2003 Generate the autocompletion script for fish. zsh \u2003 Generate the autocompletion script for zsh. Tip Run kubitect completion shell -h for instructions how to add autocompletion for a specific shell.","title":"kubitect completion"},{"location":"user-guide/reference/cli/#kubitect-help","text":"Help provides help for any command in the application. Simply type kubitect help [path to command] for full details. Usage kubitect help [ command ] or kubitect [ command ] -h","title":"kubitect help"},{"location":"user-guide/reference/cli/#other","text":"","title":"Other"},{"location":"user-guide/reference/cli/#version-flag","text":"Print Kubitect CLI tool version. Usage kubitect -v or kubitect --version","title":"Version flag"},{"location":"user-guide/reference/cli/#debug-flag","text":"Enable debug messages. This can be especially handy with the apply command. Usage kubitect [ command ] --debug","title":"Debug flag"},{"location":"user-guide/reference/configuration/","text":"Configuration reference \ud83d\udd17\ufe0e This document contains a reference of the Kubitect configuration file and documents all possible configuration properties. The configuration sections are as follows: kubitect - Project metadata. hosts - A list of physical hosts (local or remote). cluster - Configuration of the cluster infrastructure. Virtual machine properties, node types to install, and the host on which to install the nodes. kubernetes - Kubernetes and Kubespray configuration. addons - Configurable addons and applications. Each configuration property is documented with 5 columns: Property name, description, type, default value and is the property required. Note [*] annotates an array. Kubitect section \ud83d\udd17\ufe0e Name Type Default value Required? Description kubitect.url string https://github.com/MusicDin/kubitect No URL of the project's git repository. kubitect.version string CLI tool version No Version of the git repository. Can be a branch or a tag. Hosts section \ud83d\udd17\ufe0e Name Type Default value Required? Description hosts[*].connection.ip string Yes, if connection.type is set to remote IP address is used to SSH into the remote machine. hosts[*].connection.ssh.keyfile string ~/.ssh/id_rsa Path to the keyfile that is used to SSH into the remote machine hosts[*].connection.ssh.port number 22 The port number of SSH protocol for remote machine. hosts[*].connection.ssh.verify boolean false If true, the SSH host is verified, which means that the host must be present in the known SSH hosts. hosts[*].connection.type string Yes Possible values are: local or localhost remote hosts[*].connection.user string Yes, if connection.type is set to remote Username is used to SSH into the remote machine. hosts[*].dataResourcePools[*].name string Name of the data resource pool. Must be unique within the same host. It is used to link virtual machine volumes to the specific resource pool. hosts[*].dataResourcePools[*].path string /var/lib/libvirt/images/ Host path to the location where data resource pool is created. hosts[*].default boolean false Nodes where host is not specified will be installed on default host. The first host in the list is used as a default host if none is marked as a default. hosts[*].name string Yes Custom server name used to link nodes with physical hosts. hosts[*].mainResourcePoolPath string /var/lib/libvirt/images/ Path to the resource pool used for main virtual machine volumes. Cluster section \ud83d\udd17\ufe0e Name Type Default value Required? Description cluster.name string Yes Custom cluster name that is used as a prefix for various cluster components. cluster.network.bridge string virbr0 By default virbr0 is set as a name of virtual bridge. In case network mode is set to bridge, name of the preconfigured bridge needs to be set here. cluster.network.cidr string Yes Network cidr that contains network IP with network mask bits (IPv4/mask_bits). cluster.network.gateway string First client IP in network. By default first client IP is taken as a gateway. If network cidr is set to 10.0.0.0/24 then gateway would be 10.0.0.1. Set gateway if it differs from default value. cluster.network.mode string Yes Network mode. Possible values are: nat - Creates virtual local network. bridge - Uses preconfigured bridge interface on the machine (Only bridge mode supports multiple hosts). route - Creates virtual local network, but does not apply NAT. cluster.nodes.loadBalancer.default.cpu number 2 Default number of vCPU allocated to a load balancer instance. cluster.nodes.loadBalancer.default.mainDiskSize number 32 Size of the main disk (in GiB) that is attached to a load balancer instance. cluster.nodes.loadBalancer.default.ram number 4 Default amount of RAM (in GiB) allocated to a load balancer instance. cluster.nodes.loadBalancer.forwardPorts[*].name string Yes, if port is configured Unique name of the forwarded port. cluster.nodes.loadBalancer.forwardPorts[*].port number Yes, if port is configured Incoming port is the port on which a load balancer listens for the incoming traffic. cluster.nodes.loadBalancer.forwardPorts[*].targetPort number Incoming port value Target port is the port on which a load balancer forwards traffic. cluster.nodes.loadBalancer.forwardPorts[*].target string workers Target is a group of nodes on which a load balancer forwards traffic. Possible targets are: masters workers all cluster.nodes.loadBalancer.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.loadBalancer.instances[*].host string Name of the host on which the instance is deployed. If the name is not specified, the instance is deployed on the default host. cluster.nodes.loadBalancer.instances[*].id string Yes Unique identifier of a load balancer instance. cluster.nodes.loadBalancer.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.loadBalancer.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.loadBalancer.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.loadBalancer.instances[*].priority number 10 Keepalived priority of the load balancer. A load balancer with the highest priority becomes the leader (active). The priority can be set to any number between 0 and 255. cluster.nodes.loadBalancer.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.loadBalancer.vip string Yes, if more then one instance of load balancer is specified. Virtual IP (floating IP) is the static IP used by load balancers to provide a fail-over. Each load balancer still has its own IP beside the shared one. cluster.nodes.loadBalancer.virtualRouterId number 51 Virtual router ID identifies the group of VRRP routers. It can be any number between 0 and 255 and should be unique among different clusters. cluster.nodes.master.default.cpu number 2 Default number of vCPU allocated to a master node. cluster.nodes.master.default.labels dictionary Array of default node labels that are applied to all master nodes. cluster.nodes.master.default.mainDiskSize number 32 Size of the main disk (in GiB) that is attached to a master node. cluster.nodes.master.default.ram number 4 Default amount of RAM (in GiB) allocated to a master node. cluster.nodes.master.default.taints list List of default node taints that are applied to all master nodes. cluster.nodes.master.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.master.instances[*].dataDisks[*].name string Name of the additional data disk that is attached to the master node. cluster.nodes.master.instances[*].dataDisks[*].pool string main Name of the data resource pool where the additional data disk is created. Referenced resource pool must be configure on the same host. cluster.nodes.master.instances[*].dataDisks[*].size string Size of the additional data disk (in GiB) that is attached to the master node. cluster.nodes.master.instances[*].host string Name of the host on which the instance is deployed. If the name is not specified, the instance is deployed on the default host. cluster.nodes.master.instances[*].id string Yes Unique identifier of a master node. cluster.nodes.master.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.master.instances[*].labels dictionary Array of node labels that are applied to this specific master node. cluster.nodes.master.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.master.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.master.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.master.instances[*].taints list List of node taints that are applied to this specific master node. cluster.nodes.worker.default.cpu number 2 Default number of vCPU allocated to a worker node. cluster.nodes.worker.default.labels dictionary Array of default node labels that are applied to all worker nodes. cluster.nodes.worker.default.mainDiskSize number 32 Size of the main disk (in GiB) that is attached to a worker node. cluster.nodes.worker.default.ram number 4 Default amount of RAM (in GiB) allocated to a worker node. cluster.nodes.worker.default.taints list List of default node taints that are applied to all worker nodes. cluster.nodes.worker.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.worker.instances[*].dataDisks[*].name string Name of the additional data disk that is attached to the worker node. cluster.nodes.worker.instances[*].dataDisks[*].pool string main Name of the data resource pool where the additional data disk is created. Referenced resource pool must be configure on the same host. cluster.nodes.worker.instances[*].dataDisks[*].size string Size of the additional data disk (in GiB) that is attached to the worker node. cluster.nodes.worker.instances[*].host string Name of the host on which the instance is deployed. If the name is not specified, the instance is deployed on the default host. cluster.nodes.worker.instances[*].id string Yes Unique identifier of a worker node. cluster.nodes.worker.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.worker.instances[*].labels dictionary Array of node labels that are applied to this specific worker node. cluster.nodes.worker.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.worker.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.worker.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.worker.instances[*].taints list List of node taints that are applied to this specific worker node. cluster.nodeTemplate.cpuMode string custom Guest virtual machine CPU mode. cluster.nodeTemplate.dns list Value of network.gateway Custom DNS list used by all created virtual machines. If none is provided, network gateway is used. cluster.nodeTemplate.os.distro string ubuntu Set OS distribution. Possible values are: ubuntu debian custom - For all other distros (for development only) cluster.nodeTemplate.os.networkInterface string Depends on os.distro Network interface used by virtual machines to connect to the network. Network interface is preconfigured for each OS image (usually ens3 or eth0). By default, the value from distro preset ( /terraform/defaults.yaml ) is set, but can be overwritten if needed. cluster.nodeTemplate.os.source string Depends on os.distro Source of an OS image. It can be either path on a local file system or an URL of the image. By default, the value from distro preset ( /terraform/defaults.yaml )isset, but can be overwritten if needed. cluster.nodeTemplate.ssh.addToKnownHosts boolean true If set to true, each virtual machine will be added to the known hosts on the machine where the project is being run. Note that all machines will also be removed from known hosts when destroying the cluster. cluster.nodeTemplate.ssh.privateKeyPath string Path to private key that is later used to SSH into each virtual machine. On the same path with .pub prefix needs to be present public key. If this value is not set, SSH key will be generated in ./config/.ssh/ directory. cluster.nodeTemplate.updateOnBoot boolean true If set to true, the operating system will be updated when it boots. cluster.nodeTemplate.user string k8s User created on each virtual machine. Kubernetes section \ud83d\udd17\ufe0e Name Type Default value Required? Description kubernetes.dnsMode string coredns DNS server used within a Kubernetes cluster. Possible values are: coredns kubedns kubernetes.kubespray.url string https://github.com/kubernetes-sigs/kubespray.git URL to the Kubespray project. For example, it can be changed so that it targets your fork of a project. kubernetes.kubespray.version string Yes Kubespray version. Version is used to checkout into appropriate branch. kubernetes.networkPlugin string calico Network plugin used within a Kubernetes cluster. Possible values are: flannel weave calico cilium canal kube-router kubernetes.other.autoRenewCertificates boolean false When this property is set to true, control plane certificates are renewed first Monday of each month. kubernetes.other.copyKubeconfig boolean false When this property is set to true, the kubeconfig of a new cluster is copied to the ~/.kube/config . Please note that setting this property to true may cause the existing file at the destination to be overwritten. kubernetes.version string Yes Kubernetes version that will be installed. Addons section \ud83d\udd17\ufe0e Name Type Default value Required? Description addons.kubespray dictionary Kubespray addons configuration. addons.rook.enabled boolean false Enable Rook addon. addons.rook.nodeSelector dictionary Dictionary containing node labels (\"key: value\"). Rook is deployed on the nodes that match all the given labels. addons.rook.version string Rook version. By default, the latest release version is used.","title":"Configuration reference"},{"location":"user-guide/reference/configuration/#configuration-reference","text":"This document contains a reference of the Kubitect configuration file and documents all possible configuration properties. The configuration sections are as follows: kubitect - Project metadata. hosts - A list of physical hosts (local or remote). cluster - Configuration of the cluster infrastructure. Virtual machine properties, node types to install, and the host on which to install the nodes. kubernetes - Kubernetes and Kubespray configuration. addons - Configurable addons and applications. Each configuration property is documented with 5 columns: Property name, description, type, default value and is the property required. Note [*] annotates an array.","title":"Configuration reference"},{"location":"user-guide/reference/configuration/#kubitect-section","text":"Name Type Default value Required? Description kubitect.url string https://github.com/MusicDin/kubitect No URL of the project's git repository. kubitect.version string CLI tool version No Version of the git repository. Can be a branch or a tag.","title":"Kubitect section"},{"location":"user-guide/reference/configuration/#hosts-section","text":"Name Type Default value Required? Description hosts[*].connection.ip string Yes, if connection.type is set to remote IP address is used to SSH into the remote machine. hosts[*].connection.ssh.keyfile string ~/.ssh/id_rsa Path to the keyfile that is used to SSH into the remote machine hosts[*].connection.ssh.port number 22 The port number of SSH protocol for remote machine. hosts[*].connection.ssh.verify boolean false If true, the SSH host is verified, which means that the host must be present in the known SSH hosts. hosts[*].connection.type string Yes Possible values are: local or localhost remote hosts[*].connection.user string Yes, if connection.type is set to remote Username is used to SSH into the remote machine. hosts[*].dataResourcePools[*].name string Name of the data resource pool. Must be unique within the same host. It is used to link virtual machine volumes to the specific resource pool. hosts[*].dataResourcePools[*].path string /var/lib/libvirt/images/ Host path to the location where data resource pool is created. hosts[*].default boolean false Nodes where host is not specified will be installed on default host. The first host in the list is used as a default host if none is marked as a default. hosts[*].name string Yes Custom server name used to link nodes with physical hosts. hosts[*].mainResourcePoolPath string /var/lib/libvirt/images/ Path to the resource pool used for main virtual machine volumes.","title":"Hosts section"},{"location":"user-guide/reference/configuration/#cluster-section","text":"Name Type Default value Required? Description cluster.name string Yes Custom cluster name that is used as a prefix for various cluster components. cluster.network.bridge string virbr0 By default virbr0 is set as a name of virtual bridge. In case network mode is set to bridge, name of the preconfigured bridge needs to be set here. cluster.network.cidr string Yes Network cidr that contains network IP with network mask bits (IPv4/mask_bits). cluster.network.gateway string First client IP in network. By default first client IP is taken as a gateway. If network cidr is set to 10.0.0.0/24 then gateway would be 10.0.0.1. Set gateway if it differs from default value. cluster.network.mode string Yes Network mode. Possible values are: nat - Creates virtual local network. bridge - Uses preconfigured bridge interface on the machine (Only bridge mode supports multiple hosts). route - Creates virtual local network, but does not apply NAT. cluster.nodes.loadBalancer.default.cpu number 2 Default number of vCPU allocated to a load balancer instance. cluster.nodes.loadBalancer.default.mainDiskSize number 32 Size of the main disk (in GiB) that is attached to a load balancer instance. cluster.nodes.loadBalancer.default.ram number 4 Default amount of RAM (in GiB) allocated to a load balancer instance. cluster.nodes.loadBalancer.forwardPorts[*].name string Yes, if port is configured Unique name of the forwarded port. cluster.nodes.loadBalancer.forwardPorts[*].port number Yes, if port is configured Incoming port is the port on which a load balancer listens for the incoming traffic. cluster.nodes.loadBalancer.forwardPorts[*].targetPort number Incoming port value Target port is the port on which a load balancer forwards traffic. cluster.nodes.loadBalancer.forwardPorts[*].target string workers Target is a group of nodes on which a load balancer forwards traffic. Possible targets are: masters workers all cluster.nodes.loadBalancer.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.loadBalancer.instances[*].host string Name of the host on which the instance is deployed. If the name is not specified, the instance is deployed on the default host. cluster.nodes.loadBalancer.instances[*].id string Yes Unique identifier of a load balancer instance. cluster.nodes.loadBalancer.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.loadBalancer.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.loadBalancer.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.loadBalancer.instances[*].priority number 10 Keepalived priority of the load balancer. A load balancer with the highest priority becomes the leader (active). The priority can be set to any number between 0 and 255. cluster.nodes.loadBalancer.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.loadBalancer.vip string Yes, if more then one instance of load balancer is specified. Virtual IP (floating IP) is the static IP used by load balancers to provide a fail-over. Each load balancer still has its own IP beside the shared one. cluster.nodes.loadBalancer.virtualRouterId number 51 Virtual router ID identifies the group of VRRP routers. It can be any number between 0 and 255 and should be unique among different clusters. cluster.nodes.master.default.cpu number 2 Default number of vCPU allocated to a master node. cluster.nodes.master.default.labels dictionary Array of default node labels that are applied to all master nodes. cluster.nodes.master.default.mainDiskSize number 32 Size of the main disk (in GiB) that is attached to a master node. cluster.nodes.master.default.ram number 4 Default amount of RAM (in GiB) allocated to a master node. cluster.nodes.master.default.taints list List of default node taints that are applied to all master nodes. cluster.nodes.master.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.master.instances[*].dataDisks[*].name string Name of the additional data disk that is attached to the master node. cluster.nodes.master.instances[*].dataDisks[*].pool string main Name of the data resource pool where the additional data disk is created. Referenced resource pool must be configure on the same host. cluster.nodes.master.instances[*].dataDisks[*].size string Size of the additional data disk (in GiB) that is attached to the master node. cluster.nodes.master.instances[*].host string Name of the host on which the instance is deployed. If the name is not specified, the instance is deployed on the default host. cluster.nodes.master.instances[*].id string Yes Unique identifier of a master node. cluster.nodes.master.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.master.instances[*].labels dictionary Array of node labels that are applied to this specific master node. cluster.nodes.master.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.master.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.master.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.master.instances[*].taints list List of node taints that are applied to this specific master node. cluster.nodes.worker.default.cpu number 2 Default number of vCPU allocated to a worker node. cluster.nodes.worker.default.labels dictionary Array of default node labels that are applied to all worker nodes. cluster.nodes.worker.default.mainDiskSize number 32 Size of the main disk (in GiB) that is attached to a worker node. cluster.nodes.worker.default.ram number 4 Default amount of RAM (in GiB) allocated to a worker node. cluster.nodes.worker.default.taints list List of default node taints that are applied to all worker nodes. cluster.nodes.worker.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.worker.instances[*].dataDisks[*].name string Name of the additional data disk that is attached to the worker node. cluster.nodes.worker.instances[*].dataDisks[*].pool string main Name of the data resource pool where the additional data disk is created. Referenced resource pool must be configure on the same host. cluster.nodes.worker.instances[*].dataDisks[*].size string Size of the additional data disk (in GiB) that is attached to the worker node. cluster.nodes.worker.instances[*].host string Name of the host on which the instance is deployed. If the name is not specified, the instance is deployed on the default host. cluster.nodes.worker.instances[*].id string Yes Unique identifier of a worker node. cluster.nodes.worker.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.worker.instances[*].labels dictionary Array of node labels that are applied to this specific worker node. cluster.nodes.worker.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.worker.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.worker.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.worker.instances[*].taints list List of node taints that are applied to this specific worker node. cluster.nodeTemplate.cpuMode string custom Guest virtual machine CPU mode. cluster.nodeTemplate.dns list Value of network.gateway Custom DNS list used by all created virtual machines. If none is provided, network gateway is used. cluster.nodeTemplate.os.distro string ubuntu Set OS distribution. Possible values are: ubuntu debian custom - For all other distros (for development only) cluster.nodeTemplate.os.networkInterface string Depends on os.distro Network interface used by virtual machines to connect to the network. Network interface is preconfigured for each OS image (usually ens3 or eth0). By default, the value from distro preset ( /terraform/defaults.yaml ) is set, but can be overwritten if needed. cluster.nodeTemplate.os.source string Depends on os.distro Source of an OS image. It can be either path on a local file system or an URL of the image. By default, the value from distro preset ( /terraform/defaults.yaml )isset, but can be overwritten if needed. cluster.nodeTemplate.ssh.addToKnownHosts boolean true If set to true, each virtual machine will be added to the known hosts on the machine where the project is being run. Note that all machines will also be removed from known hosts when destroying the cluster. cluster.nodeTemplate.ssh.privateKeyPath string Path to private key that is later used to SSH into each virtual machine. On the same path with .pub prefix needs to be present public key. If this value is not set, SSH key will be generated in ./config/.ssh/ directory. cluster.nodeTemplate.updateOnBoot boolean true If set to true, the operating system will be updated when it boots. cluster.nodeTemplate.user string k8s User created on each virtual machine.","title":"Cluster section"},{"location":"user-guide/reference/configuration/#kubernetes-section","text":"Name Type Default value Required? Description kubernetes.dnsMode string coredns DNS server used within a Kubernetes cluster. Possible values are: coredns kubedns kubernetes.kubespray.url string https://github.com/kubernetes-sigs/kubespray.git URL to the Kubespray project. For example, it can be changed so that it targets your fork of a project. kubernetes.kubespray.version string Yes Kubespray version. Version is used to checkout into appropriate branch. kubernetes.networkPlugin string calico Network plugin used within a Kubernetes cluster. Possible values are: flannel weave calico cilium canal kube-router kubernetes.other.autoRenewCertificates boolean false When this property is set to true, control plane certificates are renewed first Monday of each month. kubernetes.other.copyKubeconfig boolean false When this property is set to true, the kubeconfig of a new cluster is copied to the ~/.kube/config . Please note that setting this property to true may cause the existing file at the destination to be overwritten. kubernetes.version string Yes Kubernetes version that will be installed.","title":"Kubernetes section"},{"location":"user-guide/reference/configuration/#addons-section","text":"Name Type Default value Required? Description addons.kubespray dictionary Kubespray addons configuration. addons.rook.enabled boolean false Enable Rook addon. addons.rook.nodeSelector dictionary Dictionary containing node labels (\"key: value\"). Rook is deployed on the nodes that match all the given labels. addons.rook.version string Rook version. By default, the latest release version is used.","title":"Addons section"}]}